{"date": "18 февраля в 15:32", "article": "\n       \r\n  \r\nСреди всех событий прошлого года больше всего нам запомнилось открытие нового — самого большого в сети Selectel — дата-центра в Санкт-Петербурге. Это открытие стало значительной вехой не только в истории компании, но и города. Церемонию открытия дата-центра 17 декабря 2015 года посетил губернатор Петербурга Георгий Полтавченко. \r\n \r\n \r\n \r\n \r\nДата-центр «Цветочная 2» расположен в деловой черте города, недалеко от станции метро Московские ворота. Рядом находится головной офис Selectel и один из наших первых  .  \r\n  \r\n«Цветочная 2» — один из самых современных в России. По уровню надежности и отказоустойчивости он соответствует требованиям стандарта Tier III по классификации Uptime Institute и стандарта TIA-942. Дата-центр имеет сертификат PCI DSS. На площади 12 тысяч м2 будет обеспечена бесперебойная работа 1000 серверных стоек. \r\n  \r\n  \r\n \r\nЧтобы рассказать эту историю с самого начала, нам придется вернуться на три года назад — в 2013-й. Именно тогда стало понятно, что вскоре Selectel потребуются новые мощности. Этому способствовали планы развития собственных хостинговых проектов и формирующийся на рынке тренд на аутсорсинг инфраструктуры. \r\n  \r\nУдачную локацию для дата-центра долго искать не пришлось. Подходящее здание расположилось прямо напротив офиса компании. По адресу Цветочная, 21 с 70-х по 90-е годы располагался завод светочувствительных материалов «Позитив». Когда производство было остановлено, помещения завода стали сдаваться в аренду небольшим фирмам. \r\n  \r\nSelectel выкупил промышленное здание для его перестройки под дата-центр. К концу 2013 года уже была готова общая архитектурная концепция «Цветочной 2» и заложены ее основные характеристики. Проект предполагал сохранение общих форм здания, но полную переделку внутренних помещений. \r\n  \r\nКомплекс «Цветочной 2» включает в себя два здания: главное, четырехэтажное и примыкающее к нему одноэтажное. Его площадь составляет 12 тысяч м2. В основу проекта легла идея создания независимых друг от друга дата-центров на каждом этаже основного здания. Один этаж — один ЦОД мощностью 2,5 МВт. Примыкающее здание решено было использовать под размещение дизель-генераторных установок, трансформаторных подстанций и распределительных устройств.  \r\n  \r\nВ начале 2014 года стартовали работы по перестройке здания. Через год — в ноябре 2014-го — в опытную эксплуатацию была введена первая очередь дата-центра, а к декабрю 2015-го завершились работы во всем здании «Цветочной 2». Вопреки экономической нестабильности, нам удалось выдержать все запланированные рамки и сроки строительства. \r\n  \r\n \r\n  \r\n  \r\n \r\n«Цветочная 2» станет первым на Северо-Западе дата-центром полностью сертифицированным Uptime Institute по стандарту Tier III. Сейчас мы завершаем первый этап сертификации — Design. \r\n  \r\nСоответствие требованиям Tier III, в частности, означает, что все инженерные системы дата-центра многократно зарезервированы: у каждого активного канала электропитания и охлаждения есть несколько дублирующих каналов. Уровень доступности услуг для Tier III дата-центра должен составлять 99,982%. Чем выше этот уровень, тем ниже допустимое время простоя (даунтайма) дата-центра. Для Tier III допустимый даунтайм — всего 95 минут в год. \r\n  \r\n«Цветочная 2» связана с дата-центром «Цветочная 1». Последний является важной городской точкой обмена трафиком и связан оптическими линиями с другими узлами связи и пиринговыми центрами. Благодаря этому новый дата-центр имеет возможности подключения практически ко всем работающим в Петербурге операторам связи, которых насчитывается более ста. Помимо этого, для расширения пиринговых отношений мы соединили «Цветочную 2» волоконно-оптическими трассами со всеми важнейшими точками обмена трафиком в городе: «Боровая, 57», «Большая Морская, 18» и дата-центром «Радуга-2». \r\n  \r\nЕсть на «Цветочной-2» и собственный узел связи, уже готовый к размещению операторов. В ближайшем будущем это позволит нашему новому дата-центру стать крупной точкой обмена трафиком. \r\n  \r\n  \r\n \r\n \r\n \r\nСтандарт PCI DSS требует повышенного внимания к безопасности. Вооружённая охрана тщательно контролирует доступ в дата-центр. Вход осуществляется строго по пропускам. У каждого сотрудника Selectel есть идентефикационная карточка с фотографией — она же и является пропуском, который нужно прикладывать к считывателю на входе. Пропуски особого образца есть также у клиентов и у подрядчиков. Пропустить по своей карте кого-то другого невозможно: если кто-то из наших сотрудников даст постороннему войти в дата-центр, то последний войдёт, но выйти уже не сможет. Разблокировать дверь по той же карте не получится. Такая система называется Anti PassBack. Благодаря её использованию вероятность нерегламентированного проникновения в помещения дата-центра практически равна нулю.  \r\n  \r\nДопуск в серверные помещения вообще строго ограничен: туда могут попасть только системные инженеры. Все остальные сотрудники в серверных «Цветочной 2» бывают только один раз — на экскурсии в честь окончания испытательного срока. \r\n  \r\n  \r\n \r\n \r\n \r\nЗа постом охраны находится диспетчерская. На мониторах в режиме реального времени отражается происходящее в серверных помещениях. Сюда же поступает актуальная информация о работе основных инженерных систем дата-центра. \r\n  \r\n  \r\n \r\n  \r\nНа каждом этаже дата-центра располагается несколько серверных помещений разных размеров. Большие серверные вмещают до 140 стоек. Для резервирования оптоволокно к стойкам подводится по нескольким маршрутам: под фальшполом и по потолку. \r\n  \r\nНа «Цветочной 2» мы предоставляем в аренду и изолированные пространства для размещения клиентского оборудования. В зону, где расположено оборудование, имеют доступ только инженеры клиента. Пространство полностью автономно и имеет собственную систему наблюдения. Клиенты, которые размещают оборудование или арендуют пространство, могут дополнительно арендовать помещение для сотрудников в офисной зоне. \r\n  \r\nДоставить оборудование даже до верхних этажей «Цветочной 2» не составляет проблемы. В дата-центре четыре лифта — один пассажирский и три грузовых. Максимальная грузоподъёмность лифта 2500 кг, такая вместимость позволяет транспортировать уже собранное оборудование в вертикальном положении.  \r\n  \r\nКстати, чтобы попасть в любую серверную дата-центра, нужно надеть «сменку». Грязь с уличной обуви может стать причиной преждевременного выхода из строя серверов и охлаждающих их кондиционеров. Но инженерам клиентов не обязательно нести с собой вторую — чистую — пару обуви. У входа в серверные залы установлены машины для автоматического надевания бахилл, в обиходе называемые просто «бахилляторами». \r\n  \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n  \r\n  \r\n \r\nИнженерные системы «Цветочной 2» обеспечивают бесперебойную работу дата-центра 24 часа 7 дней в неделю 365 дней в году. Для этого мы зарезeрвировали все элементы, каждый из них имеет дубль, принимающий на себя нагрузку в случае выхода из строя основного. \r\n  \r\n  \r\n \r\nТемпературу воздуха в серверных помещениях поддерживают прецизионные кондиционеры. Система кондиционирования построена по схеме N+1. К необходимому количеству элементов добавляется еще один, на который периодически переводится нагрузка. В целом система кондиционирования на «Цветочной 2» соответствует решениям, доказавшим свою эффективность в других дата-центрах Selectel. Однако в новом ЦОД использован принцип непрямого фрикулинга. Для охлаждения используется потенциал температуры наружного воздуха. За счет этого эффект от применяемой технологии достигает своего максимума. Естественное охлаждение снижает нагрузку на чиллеры и другое оборудование, что позволяет достигать высоких показателей энергоэффективности. \r\n  \r\n  \r\n \r\nCхема электроснабжения дата-центра предусматривает подключение технологической площадки к собственной двухсекционной распределительной подстанции напряжением 10 кВ. Эта подстанция, в свою очередь, подключена двумя независимыми высоковольтными линиями к городской системе электроснабжения. Каждый этаж «Цветочной 2» обеспечивает электричеством индивидуальная понижающая трансформаторная подстанция напряжением 10/0,4 кВ и мощностью 2*2500 кВА. \r\n  \r\nСистема гарантированного электроснабжения (СГЭ) запускается автоматически при остановке подачи тока из городской сети. Для снабжения энергией ключевого оборудования предусмотрены резерные аккумуляторные емокости. Мощность каждой — 6*400 кВт, блоки зарезервированы по схеме 2N. \r\n  \r\n \r\n \r\nЕсли ликвидация аварии затягивается и возможностей ИБП оказывается недостаточно, включаются дизель-генераторные установки (ДГУ). Кроме того, система электроснабжения в целом зарезервирована по схеме 2N. При исчезновении напряжения от одного или двух вводов электропитания на главном распределительном устройстве автоматически подаётся команда на запуск дизель-генераторных установок. \r\n  \r\n \r\n \r\n \r\n \r\nУстановки спроектированы для работы в качестве основного источника электроснабжения дата-центра и имеют схему резервирования N+1. Они способны обеспечить полноценную работу дата-центра даже в случае продолжительной аварии в масштабе региона. На территории находится запас топлива для поддержания работы ДГУ в течение 48 часов.На случай длительных перебоев с электроэнергией у нас заключен договор с топливной компанией, в котором предусмотрены особые условия гарантированного подвоза топлива. \r\n  \r\n  \r\n \r\nВ помещениях и технологических системах дата-центра установлена противопожарная сигнализация, соединенная с центральным диспетчерским пультом. Все датчики имеют адресное исполнение, поэтому инциденты возможно локализовать в течение пары минут. \r\n  \r\nНа каждом этаже есть автономные центральные станции газового пожаротушения с использованием газа хладон. Они обеспечивают защиту всех серверных и технологических помещений. Для противопожарной системы использовано оборудование отечественной компании «Болид». \r\n  \r\n \r\n \r\n  \r\nВ заключение  хотели бы поблагодарить коллегу    за экскурсию и рассказ об устройстве дата-центра. \r\n \r\n \n\n      \n       \n    ", "hub": "Хостинг / Интересные публикации / Хабрахабр", "title": "Экскурсия по дата-центру «Цветочная 2»"}
{"date": "18 февраля в 22:01", "article": "\n      Как-то, работая, заметил, что мой личный ноутбук на i5-ом, с 8 гигабайтами ОЗУ на Linux уступает служебной, менее мощной, «лошадке». \r\n \r\nБыло решено сделать «ход конём»: вместо старого SATA HDD был приобретен новый SSD диск. Админы по сути своей существа ленивые, и я не стал исключением. Вспоминать все, что было сделано заново, выстраивая свою рабочую среду, не хотелось от слова совсем, и перенос операционной системы был наименее беспроблемным вариантом в моем случае. Итак, приступим. \r\n \r\nВ ноутбук, через переходник-контроллер в слот DVD-привода, был подключен вторым новый SSD-диск на 60 Гб, определившийся в системе как /dev/sdb. \r\n \r\nНа старом 320 Гб HDD-диске /dev/sda было три раздела: /dev/sda1 (swap — предварительно отключен), /dev/sda2 (/корневой раздел), /dev/sda3 (/home).  \r\n \r\nЗагрузился в режим LiveUSB, на Ubuntu MATE 14.04 с usb-флешки. Gparted-ом создал раздел sdb1 на SSD-диске. Флаги не ставил. Входим в привилегированный режим root: \r\n \r\n \r\nПроверяем есть ли поддержка TRIM: \r\n \r\n \r\n* Data Set Management TRIM supported (limit 1 block). \r\n \r\nВывод листинга означает, что поддержка TRIM есть и она активна. \r\n \r\nПроверяем выравнивание разделов: \r\n \r\n \r\nЕсли 1 выровнено или 1 aligned, то все в порядке. \r\n \r\nСоздаем каталоги точек монтирования: \r\n \r\n \r\nМонтируем нужные разделы: \r\n \r\n \r\nПереносим, синхронизируя данные с корневого раздела: \r\n \r\n \r\nгде: \r\n-q — уменьшить уровень подробностей \r\n-a — архивный режим \r\n-H — сохранять жесткие ссылки \r\n-E — сохранить исполняемость файлов \r\n-A — сохранить списки ACL \r\n-X — сохранить расширенные атрибуты \r\n-h — выходные числа в легко воспринимаемом формате \r\n \r\nМонтируем раздел с каталогом пользователя и также переносим с него данные: \r\n \r\n \r\nДля большей сохранности от случайных ошибок отмонтируем разделы исходного HDD-диска: \r\n \r\n \r\nМонтируем файловые системы нативной ОС, в которую перенесли данные: \r\n \r\n \r\nОперацией chroot изменяем рабочий корневой каталог на тот, в который переносим данные: \r\n \r\n \r\nБекапим один из конфигурационных файлов со сведениями о файловых системах: \r\n \r\n \r\nДобавляем актуальную метку UUID раздела sdb1 в fstab: \r\n \r\n \r\nРедактируем fstab: \r\n \r\n \r\nИ приводим его к примерному виду: \r\n \r\n# / was on /dev/sdb1 during installation \r\nUUID=c45939b4-3a58-4873-aa6e-247746hgftb5 / ext4 errors=remount-ro 0 1 \r\n \r\nгде UUID=значение sdb1 \r\n \r\nОбновляем конфигурационный файл загрузчика Linux: \r\n \r\n \r\nПроверяем, правильно ли выставлены значения UUID раздела в конфигурационном файле grub.cfg: \r\n \r\n \r\nгде вместо «uuid sda2» подставляйте свое значение uuid-раздела. У меня остались старые значения, поэтому: \r\n \r\nВыходим из chroot: \r\n \r\n \r\nОтмонтируем файловые системы: \r\n \r\n \r\nМеняем в grub.cfg все старые значения uuid раздела sda2, на новые значения uuid раздела sdb1. Мне проще было сделать так. Вы можете сделать это по своему. \r\n \r\nСтавим сервер mysql, в котором есть утилита replace (пароль любой, одинаковый — не пригодится): \r\n \r\n \r\nМеняем UUID-ы: \r\n \r\n \r\nгде old_uuid и new_uuid, старое и новое значения соответственно. \r\n \r\nПроверяем поменялись ли значения в grub.cfg: \r\n \r\n \r\nЗаново монтируем файловые системы и chroot-имся: \r\n \r\n \r\nУстанавливаем сам загрузчик на sdb: \r\n \r\n \r\nВыходим из chroot: \r\n \r\n \r\nРазмонтируем всё смонтированное: \r\n \r\n \r\nВыходим из привилегированного режима root-а: \r\n \r\n \r\nПерегружаемся в новую систему. При перезагрузке в BIOS не забываем выбрать и установить загрузку с нового устройства.\n       \n    ", "hub": "*nix / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 19:17", "article": "\n        Пост расcчитан на начинающих, на людей незнакомых с технологией Apache Lucene. В нем нет материала о том, как устроен Apache Lucene внутри, какие алгоритмы, структуры данных и методы использовались для создания фреймворка. Пост является обучающим материалом-тизером, написанным для того, чтобы показать, как организовать простейший нечёткий поиск по тексту.  \r\n \r\nВ качестве материала для обучения предоставлен код на github, сам пост в качестве документации и немного данных для тестирования поисковых запросов. \r\n \r\n \r\nПодробно о библиотеке Apache Lucene написано   и  . В статье будут встречаться такие термины как: запрос, индексация, анализатор, нечеткие совпадения, токены, документы. Советую сначала прочитать  . В ней эти термины описывают в контексте фреймворка Elasticsearch, который базируется на библиотеках Apache Lucene. Поэтому базовая терминология и определения совпадают. \r\n \r\n \r\nВ статье описывается использование Apache Lucene 5.4.1. Исходный код доступен на  , в репозитории есть небольшой   для тестирования. По сути статья является подробной документацией к коду в репозитории. Начать «играть» с проектом можно с запуска тестов в классе BasicSearchExamplesTest. \r\n \r\n \r\nПроиндексировать документы можно с помощью класса  . В нём есть метод  : \r\n \r\n \r\n \r\nОн принимает на вход переменную   и  . Переменная   отвечает за поведение индексатора. Если она равна true, то индексатор будет создавать новый индекс даже если индекс уже существовал. Если false, то индекс будет обновляться. \r\nПеременная   это список объектов Document. Document это объект индексации и поиска. Он представляет собой набор полей, каждое поле имеет имя и текстовое значение. Для того чтобы получить список документов создан класс  . Его задача создавать Document используя два строковых поля: body и title. \r\n \r\n \r\n \r\nОбратите внимание что метод   по умолчанию использует  , доступный в библиотеке lucene-analyzers-common. \r\n \r\nДля того чтобы поиграть с созданием индекса перейдите к классу  . \r\n \r\n \r\nДля демонстрации базовых возможностей поиска создан класс  . В нём реализованы два метода поиска: простой поиск по токенам и нечеткий поиск. За простой поиск отвечают методы   и  , за нечеткий поиск метод  . \r\n \r\nВ Lucene существует много способов создать запрос, но для простоты методы обычного поиска реализованы только с помощью классов QueryParser и TermQuery. Методы нечеткого поиска используют FuzzyQuery, которая зависит от одного важного параметра:  . Этот параметр отвечает за нечеткость поиска, подробности  . Грубо говоря, чем он больше, тем более расплывчатым/нечетким будет поиск. Погрузиться в многообразие способов сделать запрос можно  . \r\n \r\nДля того чтобы поиграть с поиском перейдите к классу  \r\n \r\n \r\nЧтобы играть с проектом было не скучно попробуйте выполнить несколько заданий: \r\n \r\n \r\n \r\n \r\nПреимущество Apache Lucene в его простоте, высокой скорости работы и низких требованиях к ресурсам. Недостаток в отсутствии хорошей документации, особенно на русском языке. Проект очень быстро развивается, поэтому книги, туториалы и Q/A, которыми забит интернет, давно потеряли актуальность. К примеру, у меня ушло 4-5 дней только на то, чтобы понять, как вытащить векторную модель TF-IDF из индексов Lucene. Надеюсь что этот пост привлечет внимание специалистов к этой проблеме недостатка информации. \r\n \r\nДля тех же кто хочет погрузиться в мир Apache Lucene советую взглянуть на документацию Elasticsearch. Многие вещи там очень хорошо описаны, со ссылками на авторитетные источники и с примерами. \r\n \r\n \r\nЭто мой первый более или менее серьезный пост. Поэтому прошу высказывать критику, отзывы и предложения. Я мог бы написать еще несколько статей, так как сейчас вплотную работаю с Apache Lucene.\n\n      \t \n\n       \n    ", "hub": "JAVA / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 04:43", "article": "\n      Всем доброго времени суток. \r\nЕсли вы задаетесь одним из следующих вопросов: \r\n \r\nТогда вам под кат… \r\nИ да… в конце, как всегда, ссылка на код ;) \r\n \r\n \r\nMVP (Model-View-Presenter) — это один из самых распространенных шаблонов проектирования UI. \r\n \r\nСуть его заключается в следующем: \r\n \r\nСразу же в глаза бросается ключевое отличие MVP от MVC: MVP, в отличии от MVC, имеет двустороннюю связь с View. \r\nЗапомним и пойдем дальше… \r\n \r\n \r\nMVVM (Model-View-ViewModel) — это улучшеная форма MVP, при чем грань между ними так тонка, что иногда думаешь: «О, небо! За что ты так со мной?» \r\n \r\nСейчас объясню что я имею ввиду. \r\n \r\nСуть MVVM заключается в следующем: \r\n \r\n \r\n \r\n \r\nЕсли отбросить формальности, то так оно и есть. Поэтому я и сказал, что грань между MVP и MVVM очень тонка. \r\n \r\nГлобально, разница лишь в том, что MVVM реализует более гибкий слушатель событий от View. \r\nПри чем реализует таким образом, что становится доступным так называемое декларативное динамическое связывание данных. \r\n \r\n \r\nЭто такой механизм, при котором, изменив значение модели с любой стороны(со стороны View или Model), это изменение моментально вступит в силу. То есть, изменив значение в Model (в MVVM — ViewModel частично берет на себя функцию модели), оно сразу отобразится во View и наоборот. \r\n \r\nВы можете спросить: «Если в MVP есть двусторонняя связь между View и Presenter, то почему мы не можем реализовать динамическое связывание данных на MVP?». \r\n \r\nОтвет очень прост — можем! \r\nПо сути, MVP уже подразумевает динамическое связывание данных в той или иной степени. \r\nИ, если MVP это чисто императивный подход к связыванию данных, то MVVM — декларативный. \r\n \r\nВот и вся разница. \r\nНо суть у них одна и та же! \r\n \r\n \r\nТеперь рассмотрим вопросы, связанные с реализаций динамического связывания данных. \r\nНачнем с того, что, на текущий момент, браузер не способен динамически отслеживать изменение значений в переменных. \r\nКонечно, есть такая штука как Object.observe(), но эта вещь, пока еще, не является частью стандарта. \r\nПоэтому исходим из того, что  \r\nСоответсвенно, необходимо как-то понимать: когда нужно произвести синхронизацию между Model и View. \r\nВ современных фреймворках, типа Angular или Knockout, к этому вопросу подходят очень просто: вешают слушатели на разные события от элемента, которому необходимо динамическое связывание данных. \r\nНапример, для text input вешается слушатель на событие keyup. \r\nДля button — click \r\nИ т.д. \r\n \r\nВнутри обработчика происходит чтение новых данных и затем запускается механизм синхронизации оных с Model. \r\n \r\nВот, собственно, и вся история. \r\n \r\nКстати, если вы используете Angular, то, скорее всего, вам очень часто приходится прибегать к использованию таких вещей, как сервис $timeout… \r\nЕсли вы используете $timeout на автоматизме, потому что так написано где-то на stack overflow, но не понимаете его сути, то знайте, что $timeout ждет, пока закончится текущий $digest-цикл, затем выполняет код, который вы ему передали, и затем снова запускает $digest-цикл. Именно так и достигается обновление данных, если оно было инициировано не из внутренностей Angular. \r\n \r\nЧто такое $digest-цикл? \r\nВ AngularJS это как раз и есть процесс синхронизации значений между Model и View. \r\n \r\nИ, как обещал,  , в котором реализовано простейшее динамическое связывание данных. \r\n \r\nВ нескольких словах: \r\n \r\n \r\nСпасибо за внимание.\n\n      \n       \n    ", "hub": "HTML / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 09:57", "article": "\n       \r\n \r\nМы часто им пренебрегаем. Написание функциональных, интеграционных и юнит-тестов давно стало повсеместной практикой. Вёрстке мы обычно уделяем гораздо меньше времени. \r\n \r\nПроблема тестирования вёрстки в том, что только живой человек может сказать, хорошо свёрстан блок на странице или нет. Поэтому чаще всего мы тестируем HTML и CSS вручную: проверяем, как будет вести себя блок, если в нем будет слишком много (или слишком мало) текста или дочерних элементов; смотрим, чтобы все возможные варианты отображения блока смотрелись корректно; помним о том, как блоки должны адаптироваться к разным устройствам и разрешениям экрана. \r\n \r\n \r\nНам не нужно придумывать ничего нового. Мы можем применить те же подходы, которые используем при написании автотестов. \r\n \r\nСначала нужно посмотреть на требования, дизайн. На основе этого составить список всех   приложения, которые нужно проверить. Далее, дело за малым — проверить каждый тест-кейс. \r\n \r\nВ статье я расскажу, как мы создали для себя  , как изменили свой процесс разработки интерфейсов и как это упростило нам жизнь. \r\n \r\nMakeup — графический интерфейс для быстрого и комфортного   регрессионного тестирования вёрстки, основанной на методологии BEM. Это инструмент, для которого мы готовим тестовые данные так, чтобы можно было проинициализировать любой независимый блок с разными данными и быстро посмотреть его во всех интересующих нас состояниях. \r\n \r\nОписанный подход может помочь, если на вашем проекте соблюдаются 2 условия: \r\n \r\n \r\nА теперь обо всём по порядку. \r\n \r\n \r\nПервая версия Makeup (тогда у него ещё не было имени) возникла в файле spec/index.html. На этой странице прогонялись юнит-тесты по всем модулям (читай: блокам) нашего приложения. Всё было традиционно: мы инициализировали каждый модуль с разными наборами тестовых данных и проверяли тестами то, что нас интересовало.  \r\n \r\nНо этого было недостаточно. Несмотря на то, что эти тесты были сильно связаны с вёрсткой, они не могли ответить на вопрос, хорошо ли свёрстан модуль и правильно ли он будет вести себя в разных обстоятельствах.  \r\n \r\nВ сети можно найти огромное количество чек-листов на качество вёрстки. Проверку многих пунктов из них можно легко поручить анализаторам кода. Но обычно эти чек-листы проверяют качество работы по косвенным или нерелевантным признакам.  \r\n \r\nПо большому счету, критериев качества вёрстки всего два: \r\n \r\n \r\nПри несоблюдении любого из двух пунктов проделанная работа не имеет никакого смысла. \r\n \r\n \r\nСравнить вёрстку с исходным макетом и найти отличия. Но это порой не так просто. Помните, в детских журналах были головоломки «найди 10 отличий»? \r\n \r\n \r\n \r\nЛюбой инженер мгновенно предложит очевидное решение, как проще находить отличия. Достаточно наложить одно изображение на другое и верхнее изображение сделать полупрозрачным.  \r\n \r\nМожно сделать ещё удобнее — инвертировать цвета для верхнего полупрозрачного изображения. Тогда при идеальном совпадении мы должны увидеть однородный серый фон. \r\n \r\n \r\n \r\n \r\nДля реализации подобной задумки не нужен специальный инструмент. Если нужно сравнить вёрстку с исходным дизайном страницы сайта, то такой подход можно реализовать прямо в браузере. \r\n \r\n \r\n \r\n \r\n \r\nПо такому принципу работает огромное количество существующих инструментов:  \r\n \r\n \r\nПри желании вы найдёте ещё несколько десятков или сотен таких инструментов. \r\n \r\n \r\nЭтот подход применим только в том случае, когда при разработке мы можем построить однозначное соответствие между макетами и страницами верстки. \r\n \r\n \r\nНа деле мы всё чаще работаем со сложными веб-приложениями. И обычно мы не используем термин «страница». В привычных нам терминах веб-приложение с точки зрения вёрстки состоит из произвольного набора BEM-блоков и их состояний. \r\n \r\n \r\n \r\nСостояние блока — это его конечное отображение при определенном наборе элементов, модификаторов и при определенном контенте. Другими словами, каждое состояние блока — это один кейс его использования. \r\n \r\nНа практике это значит, что если, к примеру, у вас на проекте всего 100 независимых блоков, и для каждого из них предусмотрено всего 10 значимых кейсов использования, то вам придется помнить о 1000 уникальных состояний вашего приложения. \r\n \r\nМожно разбивать вёрстку на блоки поменьше, уменьшая количество состояний в каждом, но порядок числа вряд ли сильно изменится.  \r\n \r\n1000 состояний — это очень много. Ни один человек не в состоянии удержать всё это в голове. И тем более быть уверенным в качестве вёрстки каждого блока. \r\n \r\n \r\nРаньше при разработке сложных блоков с большим количеством состояний для сравнения верстки отдельного блока с дизайном я использовал невероятно медленный способ. \r\n \r\n \r\nПосле этого правим CSS и повторяем всю последовательность заново. Пока макет не станет идеальным. \r\n \r\nЭту последовательность можно научиться выполнять по-настоящему быстро, но процесс всё равно будет занимать огромное количество времени. Но такой подход никогда не даст уверенности при регрессии или рефакторинге. Да и заниматься подобной ерундой не хочется. \r\n \r\n \r\nМы не нашли инструмент, который бы нам позволил в любой момент времени… \r\n \r\n \r\nПоэтому мы решили сделать для себя Makeup.  \r\n \r\nСначала мы добавили сравнение с дизайном на страницу с юнит-тестами. Затем добавили пару ползунков для управления отображением блоков. А со временем всё это переросло в отдельный интерфейс, который стал основой рабочего процесса разработки интерфейсов в нашей команде. \r\n \r\n \r\nНа этой иллюстрации почти все возможности Makeup. Это невероятно простой инструмент. \r\n \r\n \r\n \r\n \r\n \r\nВы можете решить, что написание и поддержка актуальных конфигурационных файлов — немыслимая задача. При разработке приложения мы хотим, тратя минимальные усилия на описание структуры блоков, иметь инструмент, который поможет держать под контролем всю вёрстку. Для того, чтобы решить эту задачу, надо тесно интегрировать такой подход со сборкой вашего приложения. \r\n \r\nНаша команда почти полностью собирает конфигурацию автоматически на основе имеющихся тестовых данных. Вручную остается дописать только «заплатки» стилей, сниппеты и ссылки на документацию. На этапе сборки приложения формируются все необходимые конфигурационные файлы и создаётся отдельный порт, на котором запущен Makeup. \r\n \r\n \r\n \r\nВ нашей команде Makeup — основа разработки интерфейса приложения. Мы активно используем его на всех этапах жизни блока. \r\n \r\n \r\nПри этом нужно отдавать себе отчёт в том, что положиться на инструмент можно только в том случае, если описанные нами тест-кейсы использования обеспечивают достаточное покрытие. Здесь работают те же принципы, как и при написании тестов. \r\n \r\nЕсли своевременно добавлять все необходимые тест-кейсы и использовать Makeup на всех этапах разработки, можно спать спокойно — никаких неожиданных неприятностей ваша вёрстка вам не принесет. \r\n \r\n \r\nВ начале статьи я использовал термин «значимые состояния». Пора рассказать о том, как мы в работе выбираем значимые кейсы и как пытаемся обеспечить хорошее покрытие для вёрстки. \r\n \r\nМы пришли к выводу, что достаточно фиксировать 3 типа состояний. \r\n \r\n \r\n \r\nЕсли нам важно точное соответствие исходному дизайну, к сожалению, нет. Но в наших силах сделать тестирование вёрстки комфортным, быстрым и надежным. \r\n \r\nПоэтому мы сделали для себя простой и удобный инструмент. По большому счету он просто выводит те данные, которые мы сами сохраняем в конфигурационных файлах блоков. Но несмотря на кажущуюся простоту, для нашей команды Makeup стал основой рабочего процесса разработки интерфейсов. С его помощью мы всегда знаем о самочувствии проекта и можем в любой момент увидеть полную картинку проекта с точки зрения вёрстки. \r\n \r\nА как тестируете вёрстку вы? \r\n \r\n \r\n \n\n      \n       \n    ", "hub": "HTML / Интересные публикации / Хабрахабр", "title": "Тестируем вёрстку правильно"}
{"date": "18 февраля в 15:49", "article": "\n      Под катом вы узнаете о том, как быстро и легко оформить взаимодействие с SVG-иконками, добавить плавный скролл с помощью одного CSS-правила, анимировать появление новых элементов на странице, переносить текст на новую строку с помощью CSS и о новых способах оформления декоративной линии текста. \r\n \r\n \r\n \r\n \r\nВ Firefox и Safari уже довольно давно появились дополнительные возможности для оформления декоративной линии, которая добавляется к тексту с помощью свойства  . \r\n \r\nК примеру, можно задавать свойству   сразу несколько значений (причем это работает уже очень давно): \r\n \r\n \r\nМожно задавать цвет для оформления текста: \r\n \r\n \r\nА также стиль линии: \r\n \r\n \r\nУчтите, что в данный момент   новые свойства только в   и, частично, в  . Посмотреть рабочий пример можно  \r\n \r\n \r\n , но очень полезное свойство   позволит нам одной строкой сделать скролл на странице плавным. Работает как при прокрутке в нужное место при переходе по якорям, так и при прокрутке страницы JS-ом.  \r\n \r\n \r\nСвойство может принимать 3 основных значения: \r\n \r\nКогда нибудь (надеюсь, совсем скоро) нам не придется больше писать функции для плавной прокрутки на JS или подключать сторонние библиотеки.  \r\n \r\nЕсли плавная прокрутка страницы на вашем сайте не является чем-то критичным, смело используйте это свойство. Вы получите плавный скролл при переходе по якорям с помощью всего одного CSS-правила как минимум, в  . \r\n \r\n \r\n \r\nПредставьте, что вам нужно создать страницу с динамически подгружаемым контентом. К примеру, ленту новостей, в которой при прокрутке появляются все новые и новые элементы. Чтобы элементы не мелькали перед глазами, неплохо было бы анимировать их появление.  \r\n \r\n \r\nКак это часто делали раньше: \r\n1) на сервер посылаетcя запрос; \r\n2) после загрузки ответа данные добавляются в скрытый на странице блок; \r\n3) блоку присваивается класс, в котором прописана анимация его появление (либо (о, ужас!) блок анимируется JS-ом).  \r\n \r\nТак вот, последний пункт можно считать избыточным, ведь у нас есть старое доброе CSS-свойство  . По умолчанию анимация срабатывает при загрузке страницы либо при изменении DOM-дерева (а именно при добавлении элементу класса с анимацией или самого элемента). Поэтому, важно не хранить незаполненные блоки в DOM, а добавлять их динамически в контейнеры по мере загрузки. \r\n \r\nВот и все, что нужно для создания простой анимации появления. Плюсы такого подхода очевидны: \r\n \r\nМинус у данного подхода только один: новые элементы не могут храниться в DOM и ждать, пока мы наполним их контентом. Их разметку придется хранить на стороне JS… \r\n \r\nИзучить рабочий пример можно  . \r\n \r\n \r\nЕсли в определенном месте на странице вам нужно добавить перенос строки, а в HTML лезть не хочется (или невозможно), на помощь придет CSS. Первое, что приходит в голову — добавить псевдоэлемент с тегом <br> внутри: \r\n  \r\n \r\nК сожалению (а может, и к счастью), добавлять теги в псевдоэлементы, нельзя. Но выход есть! \r\n \r\n \r\n . \r\n \r\n \r\nЕсли вам когда-нибудь приходилось оформлять взаимодействие с SVG-элементами, вы знаете, что сделать это не так-то просто. Чтобы обращаться в CSS к отдельным SVG-элементам, приходится добавлять на страницу не тег  , а код всего SVG-изображения целиком. Это делает HTML-код ужасно громоздким. В результате нам приходится жертвовать размером страницы и лаконичностью кода ради визуальных эффектов. \r\n \r\nНо! У нас есть неплохая альтернатива — прописывать все стили взаимодействия прямо в SVG: \r\n \r\n \r\nКазалось бы, если мы прописали стили в самом изображении, то они должны отрабатывать при добавлении SVG как обычного  ! Однако, не все так просто. Добавленные таким образом стили все равно работать не будут. Но мы можем сделать ход конем и добавить SVG на страницу с помощью   или  : \r\n  или \r\n \r\n \r\n \r\nВозможно, это не самое красивое решение, но оно явно лучше, чем захламленная SVG-кодом страница. Если вы знаете более красивый способ, пожалуйста, поделитесь им в комментариях! \r\n \r\n  Пользователь    поделился классным решением, которое подробно описано  .  \r\nЕше одно   от пользователя   .  \r\n \r\nКстати, при желании в SVG-файл можно добавить и CSS анимацию: \r\n \r\n \r\n \r\nНадеюсь, описанные здесь вещи показались вам интересными, а кому-то даже пригодятся на практике. До новых встреч!\n\n      \n       \n    ", "hub": "HTML / Интересные публикации / Хабрахабр", "title": "Несколько неочевидных frontend-хитростей"}
{"date": "18 марта 2016 в 14:39", "article": "\n    Каждый начинающий программист на STM32 после мигания светодиодом и часов должен сделать WEB-сервер на полюбившемся камушке. Проблема только в том, что они не знают толком, что такое WEB-сервер, как он вообще работает и каких результатов можно добиться от этого МК. Кроме этого надо еще покупать дополнительные платы Ethernet или даже WiFi, ждать доставки, подключать и потом еще думать а надо ли оно вообще. Но есть способ познакомиться с WEB-технологиями проще, пользуясь только тем, что и так есть на большинстве отладок… \r\n \n\n     \n   \n     \r\n \r\nВ процессе изучения Python стало интересно попробовать его в связке с API VK. В ВК есть телефонная книга, она показывает телефоны ваших друзей в более-менее удобном формате. Так как далеко не всегда люди охотно оставляют там полые(валидные) номера своих телефонов, мне показалась интересной идея написать скрипт, который отбирал бы только валидные номера моб.телефонов и выдавал бы их отдельной таблицей. Наша телефонная книга будет генерировать csv-файл, который затем можно будет открыть, например, в excel. \r\n \n\n     \n   \n    Если в django-проекте присутствует достаточное развесистое дерево шаблонов, то возникает куча неудобств при любой попытке это дерево перепроектировать, перенося ветки с место на место. Нужно не забыть исправить пути наследования в большом количестве файлов. \r\n \r\n \r\n \r\n  состоит в том, что в шаблонах Django в тегах 'extends' нельзя использовать относительные пути. \r\n \r\n \r\n \r\nДля устранения этой досадной проблемы был  , который разрешает использование относительных путей в теге 'extends' (и 'include', до кучи) по правилам, аналогичным правилам для   в python. Т.е. в шаблоне можно написать \r\n \r\n \r\nдля наследования от base.html, который находится в том же каталоге, что и ваш шаблон. Или  \r\n \r\n \r\nдля наследования от base.html, который находится на два каталога выше от вашего шаблона. \n\n     \n   \n    Среди способов конвертации hg репозитория в git наиболее часто рекомендуемый в интернете — использование утилиты fast-export, написанной на python: \r\n \r\n \r\n \r\nРешил им воспользоваться и я. В процессе столкнулся с определёнными сложностями, рецептами устранения которых хотел бы поделиться, т.к. со всеми этими сложностями наверняка столкнутся программисты, работающие в Windows и не пишущие на python. \r\n \n\n     \n   \n    68 страниц отзывов в разделе «Отзывы» на типичном корпоративном сайте? Да, это почти везде так, но можно попробовать сделать этот раздел более полезным для посетителей. \r\n \n\n     \n   \n    В этой статье будет рассмотрена ситуация, когда необходимо сделать не слишком замороченную интерактивную валидацию формы сайта для отправки сообщений. В статье я делюсь своим опытом и не претендую на звание супер эксперта. Всем приятного прочтения. \r\n \r\n \r\nНеобходимо сделать форму для отправки заявки на регистрацию пользователя. Форма должна содержать следующие поля: \r\n \r\n \r\nОтправка данных осуществляется кнопкой «Отправить заявку», при условие что все поля заполнены правильно. \r\n \n\n     \n   \n    На Хабре уже   об одной известной уязвимости, связанной с переменной  . Только почему-то говорилось об SQL-инъекциях и о чем угодно, кроме XSS. \r\n \r\nВозможности комментировать исходную статью у меня нет, поэтому в двух строках уточню. Допустим, у вас на сайте есть ссылки с абсолютным адресом, вида: \r\n \r\n \r\nДопустим, адрес вашего сайта \" \". Допустим, адрес моего сайта \" \". Все, что мне нужно сделать, — это скинуть вам ссылку на специальную страницу моего сайта, откуда я сделаю самый обычный редирект на ваш же собственный сайт. Только для редиректа использую немного необычные заголовки. В качестве HTTP-target я укажу ваш собственный сайт \" \", а вот в качестве хоста — мой \" \". Хотя, в принципе, в   нет ничего необычного. \r\n \r\nЗаголовки примут вид: \r\n \r\n \r\nИ ваш скрипт вместо   подставит мой хост. Ваша ссылка  \" примет вид не как вы ожидали \" \", а \" \". В результате чего с моего сайта ваш браузер с чистой совестью скачает скрипт и выполнит его. \r\n \r\nА я получу ваши cookie. \r\n \n\n     \n   \n    Большинство используют Fail2ban для защиты различных программ на сервере. Но как быть, если надо защитить сервис, находящийся внутри локальной сети? Тут нам понадобится связка из iptables для маркировки трафика и Fail2ban для принятия решения по блокировке. \r\n \r\nВсё будем делать на Centos 6. \r\n \n\n     \n   \n    Всю жизнь считал себя технарём и вообще — человеком далеким от искусства. Но так сложилась жизнь, что последние годы профессиональная деятельность связана с искусством вообще и театрами в частности. Историей создания одного из проектов хочу здесь поделиться. \r\n \n\n     \n   \n     \r\n \r\nЭтот пост не очередной обзор софта, а некий, на мой взгляд, способ, позволяющий решить задачу обозначенную в заголовке. \r\n \r\nПримеров того, как люди используют продукт известной американской компании на хабре масса, вот некоторые из них: \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nДа, Sync упрощает жизнь многим решая проблемы синхронизации данных с помощью одноименного BitTorrent протокола. Однако, думаю мало кто в курсе, что начиная с  , Sync умеет «создавать»  \n\n     \n   \n    Просматривая описание SDN контроллера, натолкнулся на знакомую аббревиатуру. Само по себе упоминание LISP не вызвало бы у меня никакого интереса. Честно скажу, никогда в жизни не пользовался LISP для программирования, в моём случае знания о нём просто факт из истории вычислительной техники. Удивило сочетание: LISP в списке протоколов SDN контроллера. Решил поинтересоваться, чтобы это могло значить. И оказалось, что этот LISP к тому историческому LISP не имеет практически никакого отношения. \r\n \n\n     \n   \n    В последнее время зачастили обращения: «Как настроить google почту». Так начитается статья компании 3CX о настройки SMTP сервера от Google на ATC. \r\n \r\nК сожалению нам предлагают почти форменное безобразие, а именно: включить использование ненадежных приложений и выключить двухэтапную аутентификация. Я не буду пропагандировать за эти действия, по этому давайте пойдем другим путем: \n\n     \n   \n    Наконец, нашлось время разобраться, что собой представляет PSR-7. Как известно, на сегодняшний день существует публичный интерфейс   и две его реализации: \r\n  \r\n \r\nИтак, что можно сказать о реализациях? \r\n \r\n \r\nМетод Psr\\Http\\Message\\UriInterface::setScheme() предусматривает, что реализация должна поддерживать схемы «http» и «https» и может также поддерживать и другие схемы. То есть, если мне нужно быстро создать ссылку на ftp — ресусрс, mailto или file я мог бы воспользоваться уже существующим классом Uri. \r\n \r\nК сожалению, ни одна из реализаций подобных возможностей не предоставляет. Массив в теле класса с двумя методами «http» и «https» тоскливо смотрится, учитывая какие титаны это писали. \r\n \r\nКроме того, ни в интерфейсе, ни в реализациях ничего не сказано о том, что метод UriInterface::withUserInfo для протокола http, по сути, является  @depricated, о чем говорится в  . \r\n \r\n \r\nКлассы, реализующие Psr\\Http\\Message\\StreamInterface являются, по сути, обертками для стандартных потоков PHP. Вот как аргументирует использование потоков автор Guzzle в своей статье «Несколько слов о более высоком уровне (абстракции) потоков PHP в PSR-7»: \n\n     \n   \n    Виртуализация дает возможность получать доступ к приложениям, работать с которыми было бы невозможно. Не будем перечислять все плюсы, но есть и некоторые минусы. Например, вы никак не сможете работать с USB и серийными портами ни в одной виртуальной машине. По умолчанию у вас не будет такой возможности ни в одной из виртуальных машин — будь то  . Вы не сможете просмотреть содержимое флешки или даже работать с подключенным напрямую к вашему компьютеру принтером.  \r\n \r\nВыходов из такой ситуации несколько — вы можете залезть в настройки вашей виртуалки или воспользоваться дополнительным софтом. Чаще всего, такие программы не бесплатны и их использование может вылиться в круглую сумму, тем более, если повод будет единоразовым.  \r\n \r\nПоэтому для начала советую разобраться с настройками вашей виртуальной машины.  \r\n \n\n     \n   \n    Подключившись к оптическому интернету, я был очень рад оптическим скоростям даунлинка/аплинка 100/100 Мбит и, думая, что это на всю жизнь (а в реальности только на один год), забыл о зависаниях ADSL модемов, потере скорости, пакетов, связи, перезагрузке. \r\n \r\nИ опять началось! \r\n \r\nСначала грешил на сервер, принимающий и раздающий интернет — после его жесткого включения/выключения через сетевой фильтр со всем подключенным (в том числе концентратором и GPON роутером) — связь на день появлялась… При следующей потере интернета уже отдельно перегрузил сервер и передернул концентратор — и понял, что дело не в них. \r\n \r\nОбдумал возможные причины:  \r\n \r\n1) место проклятое; \r\n2) тотальное невезение; \r\n3) перегрев \r\n4) китайские конденсаторы или комплектующие, то бишь заводской брак, а если мы правильно понимаем — заложенная в продукт быстрая смерть. \r\n \r\nНо не нашел их подтверждения. \r\n \r\nМой GPON роутер Eltex NTP-RG-1402G-W:rev.C был с прошивкой версии 2.12.5.15. Логин/пароль user/user. Выяснил, читая логи proxy squid сервера и через «tcpdump -ni eth0 -XXX» что при нажатии в интерфейсе на кнопочку Reboot он со страницы /resetrouter.html, идёт на /rebootinfo.cgi?sessionKey=626295982, но кей каждый раз меняется. \r\n \r\nСтал искать, как Eltex перезагружать программно, но не нашел в интернете работающего скрипта. Нашел похожий, разобрался, переделал, чем и делюсь с вами. \r\n \n\n     \n   \n    Практически год прошел с WWDC 2015, где инженеры компании Apple представили новый подход к Deep Linking, но в сети так и не появилось хороших статей на этот счет — сейчас буду пробовать это исправить. \r\n \r\n \r\nВ iOS 9 была добавлена поддержка перехода в приложение по веб URL с http:// и https:// схемами — Seamless Linking (читаем как “бесшовные ссылки”). Т.е. пользователь просто нажимает на обычную ссылку на сайт, а попадает в приложение без открытия Safari. Если же приложения нет, то откроется браузер. Браузер же можно открыть и из приложения по нажатию кнопки, которая появится вместо индикатора заряда. Также можно открыть эту ссылку из самого приложения вызовом  . Более того, Вы можете указать какие разделы сайта в каком приложении открывать — и для всего этого не придется писать ни одной строки кода на сервере. Далее я подробно объясню, как все это настроить и избежать возможных граблей. \r\n \n\n     \n   \n     \r\n \r\n \r\n \r\nОчень часто, отвечая на форумах или при написании статей приходится переключать раскладу с русской\\украинской на английскую для написания названий программ и терминов. Можно и в транслитерации писать, но как-то не серьёзно, а иногда и вырвиглазно. Я задумался — нужно просто реализовать автозамену для FireFox. \r\n \n\n     \n   \n      — хорошее слово, которое не стоит забывать ни при разработке, ни при поиске работы. \r\n \r\nОдно из заданий, которое давали на собеседованиях, написать функцию calc так, чтобы console.log(calc(2)(\"+\")(3)) выдавал 5, а console.log(calc(2)(\"*\")(3)) выдавал 6. \r\n \n\n     \n   \n     \r\nЯ студент 2-го курса, в свободное от учебы время пилю для себя небольшой сайтик на ASP .NET. И в какой-то момент, при написании очередной формы, которая отправляет данные при помощи AJAX, я поймал себя на мысли, что мне надоело снова и снова указывать URL для отправки запроса, и было бы неплохо подключить к JavaScript’y роутинг, который есть у меня на back-end’е. \r\n \r\n \r\nНемного подумав и спросив на StackOverflow (и не получив ответа), я пришел к выводу, что у меня есть два простых пути реализации этого: \r\n \r\n \r\nПервый вариант практически сразу был отброшен как слишком скучный, и я приступил к реализации задуманной функциональности. \n\n     \n   \n    Радиолюбительство привитое с детства в радио кружке привело окольными путями к программированию микроконтроллеров и изготовлению полезных поделок для домашнего использования и не только. Одна из таких поделок была   дачного участка от «любителей халявы».  \r\n \n\n     \n  ", "hub": "*nix / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 11:38", "article": "\n      Очень часто в статьях про Хаскель сплошь и рядом встречаются функторы и особенно монады. \r\nТак часто, что порой не реже встречаются комментарии «сколько можно про какие-то новые монады» и «пишите о чём-либо полезном». \r\nНа мой взгляд это свидетельствует о том, что люди порой не понимают зачем же нужны все эти функторы и монады. \r\n \r\nЭто статья попытка показать, что сила функциональных языков и в первую очередь Хаскеля — это в том числе и силе функторов и монад. \r\n \r\n \r\n \r\nПопытаюсь показать это на примере достаточно искусственном и наверняка бесполезном, однако акцент будет поставлен на важности использования общего кода и переиспользования. \r\n \r\nТермин «чистый» перенагружено в программировании.  \r\nНапример, фразу «Руби — чисто объектный язык» мы понимаем как «Руби — язык, где всё — объекты». \r\nА вот фразу «Хаскель — это чистый функциональный язык» следует понимать как «Хаскель — функциональный язык без побочных эффектов». \r\nВ этой статье мы будем использовать термин «чистый» ещё в одном контексте. \r\n«Чистые данные» — это данные, которые я хочу получить.  \r\nВ основном примитивные типы — это числа, строки, иногда более сложные, например — картинка или несколько значений. \r\nСоответственно, «грязные данные» — это данные, которые содержат, помимо того что я хочу, дополнительную информацию. \r\n \r\nВот сочиняем программку: \r\n \r\nПрограмма проста до безобразия — мы просим ввести пользователю 2 строчки, а после выводим результат вычисления. \r\nВидим, что наша функция foo ещё не определена (она всегда вызывает падение программы), хотя Хаскель уже может откомпилировать наш код. \r\n \r\nТеперь перепишем более детально нашу функцию, используя только «чистые» данные: \r\n   \r\nКак видим, тут тоже понятно, функция   по сути является [неважно какой] смесью целочисленного деления и сумм. \r\nБольшинство функциональных языков программирования позволяют легко и просто создавать функции, основанные на чистых данных. \r\n \r\nКазалось бы всё замечательно — простая и элегантная программка. Но нетушки! \r\nРезультат функции намного сложнее, чем нам того хотелось бы. \r\nКак мы понимаем, на   делить нельзя, да и пользователь может ввести не числа, а левые строки, и при преобразовании строк в числа может выкинуть ошибку. Наш код получился небезопасным. \r\nИмперативный подход к разрешения подобных проблем делится на 2 группы: или использовать ветвления, или использовать исключения. Зачастую оба подхода комбинируется. \r\nЭти подходы настолько эффективны, что в основном используются и в функциональных языках. \r\nСкажем прямо — в Хаскеле присутствуют исключения, однако они недоразвиты, нуждаются в реформировании, не лучшим образом отлавливаются. Да и самое важное — в большинстве случаев они просто не нужны. \r\nНо нем не менее — можно. \r\nПоэтому попытаемся переписать наш код используя ветвления и исключения.  \r\n \r\n \r\nВ Хаскеле (да и многих функциональных языках) есть достойный ответ на подобные задачи.  \r\nОсновная сила заключена в Алгебраических Типах Данных. \r\n \r\nЕсли мы рассматриваем вышеприведённый пример, видно, что наши функции могут падать. \r\nРешение — пользоваться нулабельными типами данных.  \r\nВ ML языках и Scala такой тип называется  , в Хаскеле он называется  . \r\n \r\nМы не обращаем внимание на   часть, мы тут просто говорим, что просим компилятор самостоятельно уметь переводить в строку наш тип данных. \r\nА именно,  \r\n \r\nТип данных принимает значение   если у нас нет данных, и  , если есть. \r\nКак видим, тип данных — «грязный», так как содержит лишнюю информацию. \r\nДавайте перепишем наши функции более правильно, более безопасно и без исключений. \r\n \r\nПрежде всего заменим функции, которые вызывали падение на безопасные аналоги: \r\n   \r\nТеперь эти функции вместо падения дают результат  , если всё в порядке — то  . \r\n \r\nНо весь остальной код у нас зависит от этих функций. Нам придётся изменить почти все функции, в том числе и те, которые много раз тестировались.  \r\n   \r\nКак видим простая программа превратилась в достаточно монстро-образный код.  \r\nМного обёрточных функций, много избыточного кода, много изменено. \r\nНо именно на этом останавливаются многие функциональные языки программирования. \r\nТеперь можно понять почему в тех языках, несмотря на возможность создания множества АТД, АТД не так уж часто используются в коде. \r\n \r\nМожно жить с АТД, но без подобной вакханалии? Оказывается можно. \r\n \r\n \r\nНа помощь нам в начале приходят функторы. \r\n \r\nФункторы — это такие типы данных, для которых существует функция  \r\n \r\nа так же его инфиксный синоним: \r\n \r\nтакая что для всех значений типа данных всегда выполняются следующие условия: \r\n \r\nУсловие идентичности: \r\n \r\nУсловие композиции: \r\n \r\n  \r\nГде   — функция идентичности \r\n \r\nИ   — функциональная композиция \r\n \r\nФунктор — это класс типов, где мы создали специальную функцию  . Посмотрим на её аргументы — она берёт одну «чистую» функцию  , берём «грязное» функторное значение   и получаем на выходе функторное значение  . \r\n \r\nТип данных   является функтором. Создадим инстанс (экземпляр) для типа  , так чтобы не нарушались законы функторов: \r\n \r\nКак нам использовать чистую функцию с функтором  ? Очень просто: \r\n \r\nМы тут видим главное — мы не переписывали нашу функцию  , а значит нам не надо её ещё раз тестировать на баги и ко всему она осталась универсальной и чистой, зато мы с лёгкостью создали её безопасную версию, которая на вход принимает не числа, а нулабельные числа. \r\n \r\nОднако, если мы захотим применить функтор, пытаясь переписать  , мы потерпим фиаско. \r\nФункторы работают только с функциями с единственным функторно-«грязным» аргументом. \r\nЧто же делать для функций с несколькими параметрами? \r\n \r\n \r\nТут нам на помощь приходят аппликативные функторы: \r\n \r\nАппликативные функторы — такие функторы, для которых определены 2 функции:   и  \r\n \r\nТакие, что для них для любых значений одного типа данных всегда выполняются следующие правила: \r\n \r\nУсловие идентичности: \r\n \r\nУсловие композиции: \r\n \r\nУсловие гомоморфизма: \r\n \r\nУсловие обмена: \r\n \r\n \r\nОсновное отличение фунтора от аппликативного фунтора состоит в том, что фунтор протаскивает сквозь функторное значение чистую функцию, в то время как аппликативный фукнтор позволяет нам протаскивать сквозь функторное значение функторную функцию  . \r\n \r\nMaybe является аппликативным функтором и определяется следующим образом: \r\n \r\nСамое время переписать  . \r\nВ основном функцию переписывают, совмещая функторый   для первого аргумента, и аппликативное нанизывание остальных аргументов: \r\n \r\nНо можно переписать функцию, пользуясь исключительно аппликативными функциями (монадный стиль) — вначале «чистую» функцию делаем чисто-аппликативной, и аппликативно нанизываем аргументы: \r\n \r\nЗамечательно! \r\nМожет можно заодно переписать функцию   с помощью аппликативных функторов? Увы. \r\n \r\n \r\nДавайте обратим внимание на подпись функции  : \r\n \r\nФункция берёт на вход «чистые» аргументы, и выдаёт на выходе «грязный» результат. \r\nТак вот, в большинстве своём в реальном программировании, именно такие функции встречаются чаще всего — берут на вход «чистые» аргументы, и на выходе — «грязный» результат. \r\nИ когда у нас есть несколько таких функций, вместе совместить их помогают монады. \r\n \r\nМонады — это такие типы данных, для которых существует функции   и  \r\n \r\nтакие, что выполняются правила для любых значений типа: \r\n \r\nЛевой идентичности: \r\n \r\nПравой идентичности: \r\n \r\nАссоциативности: \r\n \r\n  \r\nДля удобства, есть дополнительная функция с обратным порядком аргументов: \r\n \r\nГде  \r\n \r\nМы понимаем, что тип   является монадой, а значит можно определить его инстанс (экземпляр): \r\n \r\nКстати, если мы присмотримся внимательнее к внутреннему содержанию, и подписям, увидим, что: \r\n \r\n \r\n \r\nПришло время переписать функцию  \r\n \r\nДа уж, вышло не намного красивее. Это связано с тем, что монады красиво пишутся для одной переменной. К счастью существуют много дополнительных функций. \r\nМожно написать функцию  \r\n \r\n \r\nИли использовать функцию   и  \r\n \r\nНа крайний случай, можно воспользоваться синтаксическим сахаром для монад, используя   нотацию: \r\n \r\n \r\nЕсли мы сведём основные функции к одному виду, то увидим: \r\n \r\nВсе используются для того, чтобы передавать функциям «грязные» значения, тогда как функции ожидают «чистые» значения на входе. \r\nФунторы используют «чистую» функцию. \r\nАппликативные функторы — «чистую» функцию внутри «загрязнения». \r\nМонады используют функции, которые на выходе имеют «грязное» значение. \r\n \r\n   \r\nЧто ж, наконец, можно полностью и аккуратно переписать всю программу: \r\n \r\nКод снова стал прост и понятен!  \r\nПри этом мы не поступились ни пядью безопасности! \r\nПри этом мы почти не изменили код! \r\nПри этом чистые функции остались чистыми! \r\nПри этом избежали рутины! \r\n \r\n \r\nМожно ли жить в функциональном мире без функторов и монад? Можно. \r\nНо, если мы хотим вовсю использовать всю силу Алгебраических Типов Данных, нам для удобной функциональной композиции различных функций придётся использовать функторы и монады. \r\nИбо это отличное средство от рутины и путь к краткому, понятному и часто пере-используемому коду! \r\n \r\n  Следует понимать, что для различных типов данных, аналогия с «чистыми» и «грязными» типами данных не совсем уместна. \r\nНапример, для списков \r\n \r\nА монада: \r\n \r\nна самом деле является \r\n \r\nЧто не всегда очевидно с первого взгляда.\n\n      \n       \n    ", "hub": "Haskell / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 15:25", "article": "\n      2015 год выдался богатым на нововведения, связанные с улучшениями веб-платформы. Аксель Рошмайер рассматривает 6 технологий, которые ему кажутся наиболее интересными: \r\n \r\n1. Electron; \r\n2. React Native; \r\n3. Прогрессивные веб-приложения; \r\n4. Visual Studio Code; \r\n5. Rollup; \r\n6. WebAssembly. \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n  – технология, разработанная GitHub. Она позволяет строить кроссплатформенные десктопные приложения с использованием веб-технологий. Среди её фич: \r\n \r\n• автоматические обновления; \r\n• отчеты об ошибках; \r\n• установщики Windows; \r\n• отладка и профилирование; \r\n• нативные меню и уведомления. \r\n \r\nТехнология была изначально создана для GitHub-редактора Atom, который на данный момент используют многие компании, включая Microsoft (в Visual Studio Code, о котором речь пойдет ниже), Slack и Docker. \r\n \r\nАрхитектура Electron включает как среду выполнения Node.js, так и встроенный браузер Chromium. Приложения, сделанные с помощью этой технологии, выполняются несколькими процессами: главный процесс запускает скрипт из файла package.json. Этот скрипт может открывать окна для отображения пользовательского интерфейса. Аналогично вкладкам в веб-браузере каждое из окон выполняет отдельный процесс – рендеринг. \r\n \r\n \r\n \r\n  позволяет разрабатывать нативные iOS- и Android-приложения при помощи React. Расположение виртуальной модели DOM осталось неизменным, и вам всё еще нужно использовать JSX для её создания. Но теперь UI собирается с помощью нативных компонентов, например UITabBar для iOS и Drawer для Android. Расположение этих компонентов настраивается с помощью Flexbox.  \r\n \r\nС одной стороны, это означает, что разные UI-слои есть у каждой из платформ: веб, iOS и Android. С другой – у вас будет возможность повторно использовать большую часть кода и получить опыт нативной разработки для каждой из платформ.  \r\nОбычно я скептически отношусь к переносу языка с нативной платформы на другую. Но несколько месяцев назад,  : \r\n \r\n \r\n \r\nЭто примечательно, поскольку ему пришлось выучить и JavaScript, и React, прежде чем он смог продуктивно работать с React Native. \r\n  принадлежит Энди Матушаку (Andy Matuschak), человеку из команды UIkit, который помогал разрабатывать iOS с версии 4.1 по 4.8: \r\n \r\n \r\n \r\n \r\n \r\nВ чём-то нативные приложения уже догнали веб-приложения: примером тому служит применение индексации и технологий глубинного связывания. Прогрессивные приложения не столько технология, сколько обобщение характеристик современных веб-приложений. Это может означать, что в одних областях веб-приложения идут в ногу с нативными, в других – перегоняют их: \r\n \r\n• Метод постепенного улучшения: приложение использует как можно больше окружений. Если необходим какой-либо сервис, приложение применяет то, что есть в наличии, или корректно прекращает работу, если ничего не найдено. \r\n• Адаптивный пользовательский интерфейс: приложение подстраивается под различные способы ввода (касание, речь и т. д.) и вывода данных (разные размеры экрана, вибрация, аудио, брайлевские дисплеи). \r\n• Независимость от соединения: приложение работает в режиме офлайн, когда соединение прерывается или вовсе отсутствует. \r\n• UI, подогнанный под приложение: приложение перенимает UI-элементы нативных платформ, в том числе и быструю загрузку пользовательского интерфейса, который может быть заархивирован путем кэширования ассетов с помощью Service Worker. \r\n• Постоянные обновления: Service Worker API определяет процесс для автоматического обновления приложений.  \r\n• Защищенное соединение: используется защищенный протокол передачи данных HTTPS для предотвращения слежки и атак.  \r\n• Обнаружение приложения: метаданные, такие как  , позволяют поисковым системам находить веб-приложения.  \r\n• Взаимодействие push-уведомлений: они помогают пользователям быть в курсе событий.  \r\n• Нативная установка: на некоторые платформы можно установить веб-приложение, ничем не отличающееся от нативного (иконка на главном экране, отдельная позиция на панели многозадачности, UI браузера опционален). \r\n• Залинкованность: возможность с легкостью расшаривать приложения по URL и запускать их без установки.  \r\n \r\nЯ решил рассказать о прогрессивных веб-приложениях, потому что мне нравятся все вышеупомянутые техники и технологии, но я не до конца уверен, в чем именно состоит отличие между прогрессивными веб-приложениями и современными веб-приложениями. Единственное, что приходит в голову –  Главная особенность в том, что этим приложениям вовсе не нужны баннеры. \r\n \r\n \r\n \r\n•  (сайт Google); \r\n• Статья Эндрю Беттса (Andrew Betts)  . Эндрю критично относится к прогрессивным веб-приложениям как к бренду и начинает статью с его анализа. \r\n \r\n \r\n \r\n  – это редактор JavaScript-кода, нечто среднее между интегрированными средами разработки и текстовыми редакторами, который, по-моему, неплохо себя зарекомендовал. Его преимущество также в том, что он написан на JavaScript и основан на Electron. В 2015 году VSC получил статус   и  \r\n \r\n \r\n \r\n  – это сборщик ES6-модулей, который преобразовывает их в отдельную сборку, являющуюся модулем в формате ES6 или CommonJS. Благодаря Rollup в мире JavaScript-модулей появляются 2 инновации: \r\n \r\n• Получаемая сборка состоит только из используемых экспортов благодаря технике tree-shaking. Эта техника существенно зависит от статической структуры ES6-модулей. Под статической структурой подразумевается возможность анализа во время компиляции без использования кода экспортов. Исключение неиспользуемого кода позволяет изменять размер модулей, не переживая о размерах сборок. \r\n• Rollup наглядно показывает, что ES6-модули можно упаковать в ES6-формат, обходя при этом любую кастомную загрузку. \r\n \r\n \r\n \r\n•  \r\n•  \r\n•  \r\n \r\n \r\n \r\n  – это бинарный формат статического формального языка, производного от asm.js, который можно использовать для создания динамичных исполняемых программ, поддерживаемых JavaScript-движками. Формальный язык более высокоуровневый, чем байткод, поэтому его легче поддерживать. Выходные файлы существуют в рамках JavaScript и, как следствие, хорошо в него интегрируются. Принимая во внимание быстроту работы asm.js, скорость работы скомпилированного кода С++ в Web Assembly будет равняться примерно 70 % от скорости компиляции C++ в нативный код. \r\n \r\nВероятно, что в скором времени Web Assembly будет поддерживать JavaScript OOP и станет универсальной виртуальной машиной для веба.  \r\n \r\n \r\n \r\n•  \n       \n    ", "hub": "JavaScript / Интересные публикации / Хабрахабр", "title": "6 впечатляющих веб-технологий 2015 года"}
{"date": "18 февраля в 16:27", "article": "\n       \r\n \r\nМожно настроить объекты в javascript так, чтобы, например, установить им свойства псевдо-private или readonly. Эта функция доступна начиная с ECMAScript 5.1, поэтому поддерживается всеми браузерами последних версий. Чтобы сделать это, вам необходимо использовать метод   для  , например так: \r\n \r\n \r\nСинтаксис выглядит так: \r\n \r\n \r\nИли для нескольких свойств: \r\n \r\n \r\nГде   включает следующие атрибуты: \r\n \r\n  — если это не getter (см. ниже), то value обязательный атрибут.  \r\n  — устанавливает свойство в readonly. Обратите внимание, если свойство является вложенным, то оно доступно для редактирования. \r\n  — устанавливает свойство как скрытое. Это значит, что for...of и stringify не будут в своем результате выдавать его, хотя оно по прежнему существует. Примечание. Это не значит, что свойство становится приватным. Оно все так же доступно изнутри, просто не будет выводиться. \r\n  — устанавливает свойство, как не изменяемое, например защищенное от удаления или предопределения. Опять же, если свойство вложенное, то его можно редактировать. \r\n \r\nТак что для того, чтобы создать приватное постоянное свойство, вы должны определить его как: \r\n \r\n \r\nПомимо настройки свойств,   может определять их динамически, благодаря второму параметру являющемуся строкой. Например, предположим, я хочу создать свойства в соответствии с какой-либо конфигурацией: \r\n \r\n \r\nНо это не все! Дополнительные свойства позволяют создавать геттеры и сеттеры, подобно другим языкам ООП. Однако в этом случае нельзя использовать   и  \r\n \r\n \r\nЗа исключением случаев очевидного преимущества инкапсуляции и расширенных аксессоров, можно заметить, что мы не «вызываем» геттер, а получаем его как свойство, без скобок! Это восхитительно! Например, давайте представим, что у нас есть объект с кучей вложенных свойств: \r\n \r\n \r\nТеперь, вместо a.b.c[0].d (где одно из свойств может оказаться undefined и вывалить ошибку), мы можем создать алиас: \r\n \r\n \r\n \r\nЕсли попытаться установить геттер без сеттера и попытаться задать value, вы получите ошибку. Это особенно важно при использовании вспомогательных функций, таких как $ .extend или _.merge. Будьте осторожны! \r\n \r\n \r\n \n       \n    ", "hub": "JavaScript / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 11:22", "article": "\n       \r\n \r\nДобрый день. Наша команда, которая занимается созданием  , решила заняться переводом статей, которые нам показались очень интересными. Занявшись этим делом мы обнаружили, что на Хабре еще не пробегал перевод великолепнейшей статьи под названием: \" \". Именно перевод ее части мы и предлагаем Вашему вниманию сегодня. Статья весьма не нова и от того еще больше удивило, что на Хабре не встретили ее перевода, но представляется актуальной (в своих идеях) и по сей день, хотя, само собой, много бенчмарков устарели. Если Хабра-сообществу понравилось начало, то мы опубликуем и вторую часть, а также начнем публикацию переводов других статей, которые нам показались весьма интересными и не представленными на Хабре. \r\n \r\nМоя предыдущая статья, где я доказывал, что  , вызвала необычайно много интересных разговоров. Завязалась дискуссия, как онлайн, так и в реальной жизни, но, увы, она не была настолько  , насколько бы мне этого хотелось. \r\n  \r\nВ связи с этим, в данном посте я собираюсь представить реальные   для обсуждения этой проблемы, а не просто устроить голословное перекрикивание. Вы увидите ключевые моменты, услышите мнения специалистов и даже прочитаете кристально честные   на данную тему.   Я не гарантирую, что данная статья убедит вас, или что абсолютно вся представленная информация не содержит ошибок (это невозможно для статьи такого объема), но я могу гарантировать, что это – наиболее полный и подробный анализ мнения многих разработчиков iOS о том, что мобильные веб-приложения работают и будут работать медленно в обозримом будущем. \r\n \r\nПредупреждаю вас: это очень занудная и длинная статья почти на 10 тысяч слов. Таков формат. С недавнего времени я отдаю предпочтение хорошим,  . Это – моя попытка написать хорошую статью, а также осуществить на практике то, к чему я ранее призывал: поощрять интересную дискуссию, основанную на фактах, и воздерживаться от остроумных комментариев. \r\n \r\nЯ пишу эту статью отчасти и по той причине, что данная тема обсуждается бесконечно (в форме обмена колкими фразами). Это не очередная статья на избитую тему, так что если вы ищете 30-секундную болтовню в духе «Нет, реально веб-приложения – отстой!» «Нет, не отстой», то эта статья не для вас. В Сети полно таких дискуссий в духе «О, прекратите это, не могу дышать, пожалуйста, остановитесь, так много мнений и мало фактов» и т.д. С другой стороны, насколько я могу судить,  . Это может показаться очень глупой идеей, но данная статья представляет собой мою попытку спокойно обсудить тему, которая уже породила совершенно бессмысленные банальные споры, заполненные флеймом. Моя позиция такова: проблема заключается скорее в том, что люди,   более адекватно обсуждать вопрос,   в дискуссии, а не в самом предмете обсуждения. Думаю, мы выясним, так ли это. \r\n \r\nТак что, если вы хотите узнать,   нашло на ваших друзей-разработчиков, продолжающих писать злосчастные собственные приложения накануне уже очевидной веб-революции, или же узнать  , добавьте эту страницу в закладки, заварите себе чашку кофе, найдите свободный вечер, сядьте в удобное кресло, и вот, мы оба готовы к дискуссии. \r\n \r\n \r\nВ   я утверждал на примере SunSpider, что на сегодняшний день мобильные веб-приложения работают медленно. \r\n \r\n \r\nДанную статью действительно стоит прочитать, но я все же покажу вам тест производительности:  \r\n \r\nВ целом данный сравнительный тест критикуется с трех позиций: \r\n \r\n \r\nЯ поставил перед собой довольно амбициозную цель: опровергнуть в данной статье все три претензии. Да, JS работает медленно, и это действительно имеет значение. Нет, в ближайшем будущем он не станет заметно быстрее. Нет, ваш опыт с серверным программированием не подготовит вас должным образом к тому, чтобы «начать с малого» и грамотно рассуждать о производительности мобильных приложений. \r\n \r\nГлавная же проблема заключается в том, что во всех статьях на данную тему редко приводятся цифры, показывающие,   код JS, или приводится какой-нибудь  (медленно…  ). Для того чтобы это исправить, я в данной статье приведу не  , а   эквивалента производительности для JavaScript. Поэтому я не только опровергну «старые песни» о «медленной работе JS», но и покажу в цифрах, насколько медленно он работает, и сравню со многими показателями из реального программирования, чтобы, когда вы столкнетесь с необходимостью выбора платформы, вы смогли бы самостоятельно быстро произвести вычисления и определить, эффективен ли JavaScript для решения вашей конкретной проблемы. \r\n \r\n \r\nХороший вопрос. Чтобы ответить на него, я выбрал   из Benchmarks Game. Затем я нашел более старую программу на C, которая выполняет тот же тест (более старую, так как у новых имеется множество деталей, зависящих от x86). После этого я сравнил Nitro с LLVM на моем проверенном iPhone 4S. Весь этот код можно  . \r\n \r\nВсе это очень произвольно, но ведь и код, который вы выполняете в реальной жизни, является таким же произвольным. Если вам нужен эксперимент получше, что ж, экспериментируйте. Это всего лишь мой эксперимент, поскольку других экспериментов для сравнения LLVM и Nitro пока нет. \r\n \r\nТак или иначе, в данном синтетическом тесте производительности LLVM неизменно оказывается в 4,5 раза быстрее, чем Nitro: \r\n \r\nТак что, если вам интересно, насколько быстрее ограниченная процессором функция в собственном коде по сравнению с Nitro JS, ответом будет «примерно в 5 раз». Этот результат в целом совпадает с результатами Benchmarks Game с x86/GCC/V8,  , GCC/x86 в целом в 2 – 9 раз быстрее, чем V8/x86. Таким образом, результат, судя по всему, близок к истине и не зависит от того, используете ли вы ARM или x86. \r\n \r\n \r\nДостаточно для x86. В самом деле, насколько сильно нагружает процессор визуализация таблицы? Не так уж и сильно. Проблема в том, что ARM – это не x86. \r\n \r\n , сравнение последней модели MBP с последней моделью iPhone показал коэффициент 10, так что все нормально, таблицы не такие уж и тяжелые. Можно работать и с производительностью в 10%. И вы еще хотите разделить   на пять? Браво, дружище! Теперь у нас получилась производительность в 2% от настольного компьютера (Я произвольно оперирую единицами, однако мы имеем дело с порядком величин. Ладно, сойдет). \r\n \r\nХорошо, но насколько трудна обработка текстов  ? Разве нельзя делать это на процессоре типа m68k, присоединив к нему еще один процессор? Что ж, на этот вопрос можно ответить. Возможно, вы не помните, но взаимодействие с Google Docs в реальном времени на самом деле не было просто функцией запуска. Они очень многое переписали заново к апрелю 2010 года. Давайте посмотрим, какова была производительность браузеров  .  \r\n \r\n \r\n \r\nСудя по данному графику, совершенно очевидно, что iPhone 4S абсолютно не сопоставим с веб-браузерами в эпоху взаимодействия с Google Docs в реальном времени. Впрочем, он может конкурировать с IE8, с чем я его и поздравляю. \r\n \r\nРассмотрим другое серьезное приложение JavaScript: Google Wave.  , Wave никогда не поддерживал IE8, так как тот работал слишком медленно. \r\n \r\n \r\n \r\n \r\n \r\nВидите, насколько эти браузеры быстрее, чем iPhone 4S? \r\n \r\nВидите, как все поддерживаемые браузеры показывают результат ниже 1000, а тот, что показал результат 3800, исключен из-за низкой скорости? iPhone показывает результат 2400. Как и IE8, он работает недостаточно быстро, чтобы запускать Wave. \r\n \r\nВнесем ясность:  но нельзя делать это  . Разница в производительности между собственными и веб-приложениями сопоставима с разницей в производительности между FireFox и IE8. Это  . \r\n \r\n \r\nВсе зависит  Если ваша программа на C выполняется за 10 мс, то скорость в 50 мс у программы на JavaScript будет «почти» равна скорости C. Если же ваша программа на C исполняется за 10 секунд, то 50 секунд программы на JavaScript для большинства обычных людей явно не будут скоростью, почти равной C. \r\n \r\n \r\nИ все же коэффициент 5 приемлем  , прежде всего потому, что x86 в 10 раз быстрее, чем ARM. Имеется много пространства для маневра. Это решение явно предназначено лишь для ускорения ARM в 10 раз, так что оно сопоставимо с x86, а затем можно добиться производительности, как у JS на настольном компьютере без лишних усилий! \r\n \r\nБудет ли это работать, зависит от вашей веры в закон Мура касательно попытки зарядить чип аккумулятором весом 3 унции. Я не инженер по аппаратуре, но когда-то я работал в крупной компании-производителе полупроводников, и ее работники говорили мне, что в настоящее время производительность зависит в основном от технологического   (т. е., величины, измеряемой в нанометрах). Впечатляющая производительность iPhone 5 связана большей частью с уменьшением технологического процесса с 45 нм до 32 нм, то есть, уменьшением примерно на треть, но чтобы повторить это, Apple пришлось бы ужать техпроцесс до 22 нм. \r\n \r\nПросто для справки: версия 22 нм x86 Atom Bay Trail от Intel  . Intel пришлось изобретать  , так как стандартная разновидность не работала в масштабе 22 нм. Думаете, они продадут ARM лицензию на нее? Подумайте еще раз. В мире   лишь несколько предприятий по производству интегральных схем 22 нм, и большинство из них контролирует Intel. \r\n \r\nНа самом деле, ARM, судя по всему, стремится уменьшить техпроцесс до 28 нм или около того (см. A7), а Intel между тем стремится к 22 нм, а возможно даже к 20 нм. Если рассматривать только аппаратный аспект, то мне кажется гораздо более вероятным, что чип x86 с производительностью класса x86 найдет применение в смартфоне гораздо раньше, чем появится возможность уменьшить чип ARM с производительностью класса x86. \r\n \r\nПримечание от бывшего инженера Intel, приславшего мне письмо по электронной почте: \r\n \r\n \r\nПримечание от бывшего инженера Robotics, приславшего мне письмо по электронной почте: \r\n \r\n \r\nТаким образом, в конечном итоге закон Мура, возможно, и верен, но только в том, что для перехода к x86 потребуется целая мобильная экосистема. Нельзя сказать, что это невозможно, раньше  . Правда это было в эпоху, когда ежегодный объем продаж составлял порядка  , а сейчас продается 62 миллиона  . Это было сделано с помощью готовой среды виртуализации, которая могла имитировать старую архитектуру со  . Между тем, производительность современных гипотетических поисковых систем виртуализации для оптимизированного (O3) кода приближается к 27%. \r\n \r\nЕсли вы полагаете, что в конечном итоге JavaScript придет к этому, то наиболее эффективным способом будет усовершенствование аппаратуры. Либо у Intel через 5 лет будет жизнеспособный чип (вероятно), а Apple поменяет концепцию (маловероятно), либо ARM в течение следующего десятилетия выйдет из игры (пообщайтесь с 10 инженерами по аппаратуре, и вы получите 10 разных мнений о вероятности такого варианта). Впрочем, десятилетие – это долгий срок для проекта, который  . \r\n \r\nБоюсь, на этом мои познания в области аппаратуры заканчиваются. Могу сказать вам только одно: если хотите верить, что ARM преодолеет разрыв с x86 в ближайшие 5 лет, в первую очередь вам необходимо найти того, кто работает над ARM или x86 (то есть, человека, действительно обладающего знаниями), чтобы он согласился с вами. Для написания данной статьи я консультировался со многими квалифицированными инженерами, и все они отказались официально озвучить такую позицию, поэтому мне кажется, что в в ней ничего хорошего нет. \r\n \r\n \r\nЗдесь многие компетентные инженеры ПО заходят в тупик. Они мыслят так: JavaScript стал быстрее, и будет ускоряться и в дальнейшем! \r\n \r\nПервая часть данного утверждения соответствует истине. JavaScript стал  быстрее. Тем не менее, он уже достиг пика и в будущем его скорость существенно не вырастет. \r\n \r\nПочему? Во-первых, большая часть улучшений в JavaScript за всю его историю на самом деле касалась  . Вот что   Джефф Этвуд: \r\n \r\n \r\nЕсли связать ускорение JS с аппаратурой в целом, то улучшение производительности (аппаратной) JS  . Именно поэтому, если вы хотите верить, что JS будет ускоряться, на сегодняшний день это скорее всего будет происходить за счет ускорения аппаратуры, поскольку об этом свидетельствует историческая тенденция. \r\n \r\nА как насчет JIT,V8, Nitro/SFX, TraceMonkey/IonMonkey, Chakra и остальных? Что ж, в момент выхода они были чем-то значимым, хотя и не таким значимым, как вы думаете. V8 вышел в сентябре 2008 года. Примерно в это же время я откопал копию Firefox 3.0.3: \r\n \r\nПоймите меня правильно, рост производительности в 9 раз нельзя игнорировать, в конце концов, это число почти равно разнице между ARM и x86. Таким образом, разница в производительности между Chrome 8 и Chrome 26 остается на том же уровне, поскольку с 2008 года ничего крайне важного не произошло. Другие производители браузеров наверстали упущенное (кто-то быстрее, кто-то медленнее), но никто с тех пор по-настоящему не увеличил скорость самого кода процессора. \r\n \r\n \r\nВот   на моем Mac (самая ранняя версия, которая все еще работает, декабрь 2010), а вот  . \r\n \r\n \r\nНе видите разницы? Это потому, что ее нет.  \r\n \r\nЕсли Сеть кажется вам быстрее, чем в 2010 году, то это, вероятно, из-за того, что вы работаете на более быстром компьютере, а улучшения в Chrome здесь ни при чем. \r\n \r\n  Некоторые умные люди отметили, что в наши дни SunSpider не является хорошим тестом производительности (при этом они отказались предоставить какие-либо актуальные цифры). Чтобы начать осмысленную дискуссию, я выполнил Octane (тест Google) на старых версиях Chrome, и он показал некоторое улучшение: \r\n \r\n \r\nПо моему мнению, улучшения производительности, приобретенного за этот период, недостаточно подтверждения заявления о том, что JS ликвидирует разрыв в обозримый промежуток времени. Тем не менее, надо признать, что я переоценил данный случай, так как   все же происходит в JavaScript, ограниченном процессором. Тем не менее, для меня эти числа подтверждают большую гипотезу: эти улучшения – совсем не те величины, что могут устранить разрыв с собственным кодом в обозримом будущем. Необходимо улучшить все в 2 – 9 раз, чтобы конкурировать с LLVM. Эти улучшения хороши, но не до такой степени.  \r\n \r\nДело в том, что идея применения JIT в JavaScript возникла 60 лет назад. За ней последовали 60 лет исследований и буквально тысячи реализаций для всех языков программирования, чтобы доказать, что это была хорошая идея. Сейчас же, когда мы сделали это, мы исчерпали идеи 60-летней давности. Все, друзья, шоу окончено. Возможно, нам удастся выработать еще одну хорошую идею в течение следующих 60 лет. \r\n \r\n \r\nЕсли это правда, то почему мы постоянно слышим обо всех великих улучшениях производительности в JavaScript? Чуть ли не каждую неделю кто-то рекламирует огромные ускорения в каком-нибудь тесте. Здесь Apple заявляет об ошеломляющем ускорении в 3,8 раза в JSBench: \r\n \r\n \r\n \r\nВыходит, Safari 7 в 3,8 раза быстрее других браузеров? \r\n \r\nВозможно, для удобства Apple, информация о данной версии Safari не разглашается, так что никто не может опубликовать независимые данные о быстродействии Safari. Тем не менее, позвольте мне высказать ряд соображений по данному заявлению, основанных только на общедоступной информации. \r\n \r\nПрежде всего, меня заинтересовал тот факт, что в открытых сообщениях о JSBench Apple сообщает о гораздо более высоких результатах, нежели в сообщениях о традиционных тестах вроде SunSpider. Сейчас за JSBench  , например, Брендан Эйх, создатель JavaScript, однако в отличие от традиционных тестов, в JSBench не пишется программа, где решающую роль играют целые числа или что-то в этом роде. Вместо этого JSBench автоматически сгребает все, что преподносят Amazon, Facebook и Twitter, и создает из всего этого тесты. Если вы пишете веб-браузер, который (скажем честно) большинство людей использует для посещения Facebook, я представляю себе, насколько полезно иметь тест исключительно для Facebook. С другой стороны, если вы пишете программу для работы с таблицами, игру или приложение для фильтрации изображений, мне кажется, что традиционный тест с его арифметикой целых чисел и хэшированием md5 пригодится вам гораздо больше, чем наблюдение за скоростью аналитического кода Facebook. \r\n \r\nДругой важный факт состоит в том, что, по заявлению Apple, улучшение производительности в SunSpider не всегда означает улучшение чего-либо еще. В том же самом документе, который представляет предпочтительный тест Apple, Эйх и другие пишут следующее: \r\n \r\n \r\n  создатель JavaScript и один из ведущих архитекторов Mozilla открыто признает, что за последние два года с производительностью JavaScript сайта Amazon совершенно ничего не произошло, и   не случалось ничего невероятно захватывающего. Это говорит о том, что маркетологи просто годами навязывают свою продукцию. \r\n \r\nВ продолжение темы они утверждают по сути, что тестирование производительности Amazon является лучшим средством прогнозирования работы Amazon, чем тестирование производительности SunSpider [э-э-э… очевидно…], следовательно, оно хорошо подходит для веб-браузеров, которые используются для посещения Amazon. Тем не менее, все это не поможет написать приложение для обработки фотографий. \r\n \r\nНо в любом случае, опираясь на открытую информацию, я могу сказать, что заявления Apple об увеличении производительности в 3,8 раза не всегда означают что-то полезное для вас. Также могу сказать, что если бы у меня были тесты, опровергающие заявления Apple о превосходстве над Chrome, мне бы не позволили опубликовать их. \r\n \r\nТак что давайте закончим данный раздел следующим выводом: если у кого-то имеется график, показывающий, что его веб-браузер быстрее, это не означает, что JS в целом становится быстрее. \r\n \r\nОчень надеемся, что статья Вам понравиться на столько же, на сколько она понравилась в свое время и нам. Если это так, то в скором времени Вас ждет вторая часть перевода.\n        \t \n\n       \n    ", "hub": "JavaScript / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 17:25", "article": "\n       ARM Cortex-M3 — это, пожалуй, самое популярное на сегодняшний день 32-разрядное процессорное ядро для встраиваемых систем. Микроконтроллеры на его базе выпускают десятки производителей. Причина этому — универсальная, хорошо сбалансированная архитектура, а следствие — непрерывно растущая база готовых программных и аппаратных решений. \r\n \r\nРугать Cortex-M3, в общем-то, не за что, но сегодня я предлагаю подробно рассмотреть Cortex-M4F — расширенную версию всеми любимого процессорного ядра. Перенести проект с микроконтроллера на базе Cortex-M3 на кристалл на базе Cortex-M4F довольно просто, а для ряда задач такой переход стоит затраченных усилий. \r\n \r\nПод катом краткий обзор современных Cortex'ов, обстоятельное описание блоков и команд, отличающих Cortex-M4F от Cortex-M3, а также сравнение процессорных ядер на реальной задаче — будем измерять частоту мерцания лампы на микроконтроллерах с разными ядрами. \r\n \r\n \r\n \r\n   \r\nО том как сменяли друг-друга поколения процессорных ядер ARM написано огромное количество статей и обзоров. Не вижу смысла расписывать всё то, что есть в википедии, но напомню основные факты. \r\n \r\nКомпания ARM Ltd. разрабатывает микропроцессорные и микроконтроллерные ядра с RISC-архитектурой и продает производителям электронных компонентов лицензии на производство кристаллов по соответствующей технологии. Таких производителей по всему миру десятки и даже сотни, есть среди них и отечественные компании. \r\nСовременные ядра ARM объединены названием Cortex.  \r\n   \r\n \r\nИтак, процессорные ядра ARM Cortex разделены на три основные группы:  \r\n   \r\n  \r\nРассмотрим последнюю группу, постепенно приближаясь к паре Cortex-M3 / Cortex-M4F. Всего на конец 2015 года представлено шесть процессорных ядер: Cortex-M0, -M0+, -M1, -M3, -M4, -M7.  \r\nИз этого списка часто «выпадает» Cortex-M1, это оттого, что -M1 разработан и используется исключительно в приложениях связанных с FPGA. Остальные ядра не имеют столь специализированной области применения и отличаются по производительности — от самого простого -M0 до высокопроизводительного -M7. \r\n \r\n \r\n \r\nПо сравнению с Cortex-M0, Cortex-M0+ дополнительно оснащен блоком защиты памяти MPU, буфером Micro Trace Buffer для отладки программ, а также имеет двухступенчатый конвейер вместо трехступенчатого и упрощенный доступ к периферийным блоками и линиям ввода/вывода. \r\n \r\nCortex-M0 и Cortex-M0+ имеют одношинную фон-неймановскую архитектуру, а ядро Cortex-M3 — уже гарвардскую. Cortex-M3 довольно сильно отличается от «младших» представителей линейки и имеет гораздо более широкие возможности. \r\n \r\nCortex-M4 построен по абсолютно той же архитектуре и «структурно» не отличается от Cortex-M3. Разница заключается в поддерживаемой системе команд, но об этом позже. Cortex-M4F отличается от -M4 наличием блока вычислений с плавающей точкой FPU. \r\n \r\nАрхитектура Cortex-M7 представлена относительно недавно и отличается от Cortex-M3/M4 так же сильно, как Cortex-M3/M4 отличаются от Cortex-M0. 6-ступенчатый суперскалярный конвейер, отдельная кэш-память для данных и команд, конфигурируемая память TCM и другие отличительные функции этого ядра «заточены» для достижения максимальной производительности. И действительно, возможности контроллеров на базе Cortex-M7 сравнивают скорее с Cortex-A5 и -R5, чем с другими контроллерами группы Embedded Processors. Границы применения технологий продолжают размываться. \r\n \r\nНесмотря на совершенно разные возможности ядер группы Cortex-M, набор команд каждого ядра включает в себя все команды, поддерживаемые в более младших ядрах. Так обеспечивается возможность разработки программно-совместимых микроконтроллеров на базе разных ядер, этим и занимается большинство производителей микроконтроллеров.  \r\n \r\n \r\n \r\nЯдра Cortex-M0 и Cortex-M0+ имеют одну и ту же систему команд. Набор инструкций Cortex-M3 включает все команды Cortex-M0 и около сотни дополнительных инструкций. Процессорные ядра Cortex-M4 и Cortex-M7 имеют, опять же, идентичный набор команд — набора команд Cortex-M3 плюс так называемые DSP-инструкции. Ядро Cortex-M4F дополнительно к набору Cortex-M4 / -M7 поддерживает команды вычислений с плавающей точкой, а система команд Cortex-M7F включает ещё 14 команд для операций над числами с плавающей точкой двойной точности. \r\n \r\n   \r\nИтак, ближайшими «соседями» популярного процессорного ядра Cortex-M3 являются Cortex-M4, дополненный поддержкой DSP-инструкций, и Cortex-M4F, дополнительно содержащий блок FPU и поддерживающий соответствующие команды. Рассмотрим DSP- и FPU-команды. \r\n \r\n \r\nАббревиатура DSP чаще всего расшифровывается как Digital Signal Processor, т.е. отдельный и вполне самостоятельный контроллер или сопроцессор, предназначенный для задач цифровой обработки сигналов. Не стоит путать специализированную DSP-микросхему и набор DSP-инструкций. DSP-команды (расшифровывается Digital Signal Process  вместо Process ) — это набор команд, который поддерживается рядом процессорных ядрер ARM и соответствует некоторым типовым для цифровой обработки сигнала операциям. \r\n \r\nПервая группа таких операций — это   (Single-cycle Multiply Accumulate или просто MAC). \r\nДля самых маленьких: умножение с накоплением описывается формулой S = S + A x B. Соответствующие команды описывают умножение двух регистров с суммированием результата в аккумулятор и смежные операции: умножение с вычитанием результата из аккумулятора, умножение без использования аккумулятора и т.д.  \r\n \r\nОперации предусмотрены для 16- и 32-разрядных переменных и играют важную роль во многих типовых алгоритмах цифровой обработки сигналов. Например,   (это классический, почти банальный «например») по сути представляет собой последовательность операций умножения с накоплением, а значит скорость его работы напрямую зависит от скорости выполнения умножения с накоплением.  \r\nВсе MAC-инструкции в микроконтроллерах с ядром Cortex-M4(F) выполняются за один машинный цикл.  \r\n \r\nВторая группа DSP-инструкций — это операции  (Single Instruction Multiple Data, SIMD), позволяющие оптимизировать обработку данных за счет параллелизма вычислений. Пары независимых переменных попарно помещаются в один регистр большей размерности, а арифметические операции проводятся уже над «большими» регистрами. \r\nНапример, команда   подразумевает одновременное сложение двух пар 16-разрядных знаковых чисел с записью результата в регистр, хранящий первый операнд. \r\n \r\n \r\n \r\n \r\nПоскольку регистры общего назначения имеют разрядность 32 бит, в каждый из них можно записать не только по две 16-разрядных переменных (полуслов), но и до четырех 8-разрядных переменных (байт). Несложно прикинуть зачем нужна команда SADD8. \r\n \r\nВот более сложная операция: умножение старших полуслов, умножение младших полуслов и суммирование произведений между собой и с 64-разрядным накоплением. Команда SMLALD описывает все эти действия и выполняется Cortex-M4 за один машинный цикл. SMLALD, как и многие другие команды, совмещает умножение с накоплением и обработку данных по принципу SIMD. \r\n \r\n \r\n \r\n \r\nИ простые SIMD-команды (знаковые и беззнаковые 8- и 16-разрядные сложение и вычитание и т.п.), и сложные команды, подобные SMLALD, выполняются за один машинный цикл. \r\n \r\nСледующая группа DSP-инструкций — команды   (Saturating instructions). Они также известны как операции с отсечкой и представляют собой своеобразную защиту от переполнений. При использовании стандартных команд, регистр, хранящий результат, при переполнении «перезагружается» с нуля. Команды, предусматривающие насыщение, при переполнении фиксируют результат на допустимом разрядностью максимуме и с программиста снимается необходимость заботиться о флагах переполнения.  \r\n \r\n \r\n \r\nСреди команд процессорного ядра Cortex-M4 есть и «обычные» арифметические операции, и те же операции с насыщением. Использование последних особенно востребовано в задачах, где точностью вычислений можно пожертвовать ради скорости и таких в ЦОС немало. \r\n \r\n \r\nАппаратная поддержка вычислений с плавающей запятой (или точкой, кому как больше нравится) — это особенность ядра Cortex-M4F и более старших представителей линейки Cortex-M.  \r\n \r\nКоманды вычислений с плавающей точкой позволяют выполнять операции над вещественными числами с максимальной производительностью. Вообще, для представления вещественных чисел сегодня используется два формата — с фиксированной и плавающей точкой. В первом случае количество разрядов для записи целой и дробной частей зафиксировано и вычисления сводятся к операциям над целыми числами, во втором число представляется как совокупность знакового бита, нескольких разрядов порядка и мантиссы: \r\n \r\n(-1)  * m × b , \r\nгде s — знак, b-основание, e — порядок, а m — мантисса \r\n \r\nИспользование формата с плавающей точкой предпочтительно при обработке сигналов за счет гораздо более широкого диапазона значений переменных формата float. Использование операций FPU также избавляет разработчика от необходимости следить за разрядностью. Формат чисел с плавающей точкой одинарной точности описывается стандартом IEEE 754, это представление и используется в микроконтроллерах с ядром Cortex-M4F. Диапазон допустимых значений составляет (10 … 10 ) при приблизительном пересчете в десятичные числа.  \r\n \r\n \r\n \r\nДля формата чисел с плавающей запятой двойной точности, как в Cortex-M7F, используется тот же принцип, но вместо 32-разрядного представления используется 64-разрядное, на порядок приходится 11 бит, а на мантиссу 52. \r\n \r\nО том как и зачем используется формат с плавающей точкой не раз написано на хабре ( , например, отличная статья). Мне, пожалуй, не написать лучше, поэтому идем дальше. \r\n \r\n \r\nЧтобы немного прочувствовать масштаб и понять насколько может быть ускорена обработка данных с использованием Cortex-M4 можно поизучать полный перечень DSP- и FPU-инструкций. У меня есть большие сомнения на счет практической ценности этих таблиц, это хотя бы показательно. Все DSP- и большинство FPU-инструкций выполняются за один машинный цикл.  \r\n \r\n \r\n \r\n \r\n \r\nВпрочем, на практике сами инструкции ядра используются не часто. Обычно при разработке достаточно разобраться с документацией на контроллер и сишными библиотеками от производителей ядра и кристалла. В частности, для ядер Cortex существует ARM-овский набор библиотек CMSIS, который используется для процессоров Cortex-M от разных производителей. В состав CMSIS входит и библиотека CMSIS-DSP, она включает в себя: \r\n \r\n \r\n \r\n   \r\n \r\nКак правило, сравнение ядер Cortex-M3 и Cortex-M4(F) заканчивается красивыми графиками — гистограммами, на которых показано значительное ускорение работы контроллера на базе -M4 при выполнении типовых для ЦОС операций (КИХ-фильтр, БПФ, матричные вычисления, ПИД-регулятор и т.п). Без указаний используемых контроллеров, методики вычислений и измерений. \r\n \r\nНо мы не будем сравнивать Тайд и Обычный стиральный порошок, а возьмем реальную аппаратную и программную платформу. \r\n \r\n \r\n \r\nТак мы вполне логично дошли до того чтобы рассмотреть микроконтроллеры серии EFM32 Wonder Gecko от SiLabs. Они классные и вы можете купить их в ЭФО по отличным ценам оптом и в розницу. Кхе-кхе. \r\n \r\n  — серия микроконтроллеров на базе ядра Cortex-M4F. Как и другие EFM32, они ориентированы на малопотребляющие устройства и предоставляют разные программные и аппаратные средства для контроля над энергопотреблением. Эти средства мы будем использовать для сравнения ядер Cortex-M3 и Cortex-M4F. \r\n \r\n \r\n \r\nПлата EFM32WG-STK3800 — кит для работы с микроконтроллером на базе ядра Cortex-M4F \r\nПлата EFM32GG-STK3700 — кит для работы с микроконтроллером на базе ядра Cortex-M3 \r\nПлаты отличатся между собой только целевым микроконтроллером. \r\n \r\n \r\n \r\nВ описываемом эксперимента использовалась платформа Simplicity Studio. Это SiLabs-овская оболочка, которая объединяет все программы, утилиты, примеры и документы, доступные для микроконтроллеров от Silabs. Сейчас понадобятся несколько её компонентов — IDE, утилита для контроля энергопотребления energy profiler, а также готовый проект из набора примеров и user guide на используемые платы. \r\n \r\n \r\nОдна из программ из набора готовых примеров использует Быстрое Преобразование Фурье для измерения частоты мерцания внешнего источника света. Если вкратце, то сигнал с датчика освещенности поступает на АЦП, результаты измерений буферизируются, и раз в 0,5 сек производятся вычисления: по выборке из 512 результатов измерений с использованием БПФ выделяется частота основной гармоники. На ЖКИ выводится результат вычислений и количество машинных циклов за которые была исполнена функция ProcessFFT(). \r\nРесурсоемкой является только часть алгоритма, связанная с анализом измерений. Запустим одну и ту же программу на двух платах и сравним длительность вычислений и уровень энергопотребления. \r\n \r\n \r\n \r\nОткрываем Simplicity Studio, включаем Simplicity IDE, компилируем проект.  \r\n \r\n \r\n \r\nПодключаем плату в режиме DBG, программируем микроконтроллер, подключаем вход АЦП к выходу интерфейса датчиков (через него опрашивается light sensor), запускаем. \r\nВывод первый: программа работает корректно. Когда работает лампа дневного света, результат работы вычислений — 100 Гц (в сети переменного тока частота 50 Гц, а «максимальная интенсивность» света включенной в сеть лампы достигается и на минимуме, и на максимуме синусоиды, т.е. дважды за период). Помещая датчик освещенности в тень получаем результат «DARK», а при естественном освещении — «прыгающие» цифры. \r\n \r\n \r\n \r\nТеперь воспользуемся утилитой  , которая предоставляет график изменения уровня энергопотребления, обновляющийся по ходу исполнения программы.  \r\nЗапускаем профилирование для платы EFM32WG-STK3800. \r\n \r\n \r\n \r\nНа вычисления у микроконтроллера EFM32WG990F256 ушло около 49 мс, среднее энергопотребление — 411 мкА. Запомним этот результат и попробуем запустить ту же сишную программу на модуле с микроконтроллером на базе Cortex-M3, то есть без всяких DSP- и FPU-инструкций ядра. \r\n \r\nВ свойствах проекта для этого необходимо \r\n \r\n \r\n \r\nЕстественно, в других IDE процесс может проходить несколько иначе, для разных серий микроконтроллеров также возможны различные нюансы, однако принцип перехода на другое ядро будет тот же. \r\n \r\nИтак, после сохранения новых настроек и подключения другой отладочной платы повторим эксперимент. \r\n \r\n \r\n \r\nРезультаты можно сравнивать с чистой совестью: оба кристалла работают с тактовой частотой 48 МГц, опрос датчиков и обработка данных идут с одинаковой периодичностью, результаты выводятся в одном и том же формате на одинаковые ЖКИ. \r\n \r\nПо графикам видно, что энергопотребление кристалла действительно почти полностью определяется уровнем потребления на этапах вычислений и вывода их результатов. Измерения же проводятся в режиме «сна» и практически не влияют на общее энергопотребление. Вычисления на ядре Cortex-M3 проводятся в 2.2 раза медленнее, в той же пропорции изменяется и среднее энергопотребление устройства. \r\n \r\nС одной стороны, вся математика, необходимая для решения задачи может исполняться и на контроллере с ядром Cortex-M3, однако разница в скорости вычислений может быть существенной для многих устройств, критичных к энергопотреблению или скорости работы. \r\n \r\n \n\n      \n       \n    ", "hub": "Системное программирование / Интересные публикации / Хабрахабр", "title": "На что стоит променять Cortex-M3?"}
{"date": "18 февраля 2015 в 09:47", "article": "\n       \r\n \r\nСегодня спецификации новой версии HTTP приобрели окончательный вид. Председатель рабочей группы IETF HTTP Марк Ноттингем в своём блоге  , что IESG формально одобрила спецификации  . Теперь они отправляются в  , где им присвоят официальный номер RFC, немного отредактируют и опубликуют. \r\n \r\nСкоро это замечательное событие отразят в  , а пока что Марк Ноттингем от себя лично благодарит всех, кто внёс свой вклад в разработку новых спецификаций. В особенности — разработчиков протокола SPDY, который лёг в основу HTTP/2. \r\n \r\nСреди   бинарного протокола HTTP/2, который пришёл на смену текстовому HTTP/1.1: \r\n \r\n \r\nHTTP/2 — самое важное нововведение в Hypertext Transfer Protocol с момента выхода HTTP/1.1 в 1999 году. \r\n \r\nПотестировать HTTP/2 можно уже сейчас. Он поддерживается браузерами Firefox и Chrome (настройка отключена по умолчанию, идентификатор протокола “h2-14”). Есть и   от Akamai, Google и Twitter, а также open source реализации. \r\n \r\n \r\n \n\n      \n       \n    ", "hub": "IT-стандарты / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 15:06", "article": "\n         \r\nПродолжаем погружаться в новшества RDS Windows Server 2016.  \r\n  часть была посвящена новому типу развертывания RDS – MultiPoint Services.  \r\nСегодня будем говорить о не менее прекрасном, а именно Personal Session Desktop и других нововведениях, которые расширяют и дополняют имеющуюся функциональность RDS в средах Windows Server 2012 и Windows Server 2012 R2. \r\n \r\nЕще раз отмечу, что на данный момент отталкиваемся от той версии Windows Server 2016, которая официально доступна и открыта для использования –  . В RTM-версии, возможно, будут изменения, о которых непременно оповестим. \r\n \r\nПриступим. \r\n \r\n \r\n \r\nЕсли Вы сервис-провайдер и хотите предоставить полноценный «десктоп» своим клиентам, то в текущих условиях сценарий VDI и SPLA (Service Provider License Agreement) являются непреодолимой стеной ну пути решения поставленной задачи. Согласно SPLA, нельзя предоставлять инфраструктуру VDI, построенную на базе клиентских ОС, «облачным» пользователям. \r\n  \r\nХабр любит доказательства, поэтому привожу цитаты из официальных руководств по лицензированию DaaS (Desktop As A Service).  \r\n \r\n \r\n \r\n \r\n \r\n \r\nПод   понимается ситуация, когда клиент лицензируется через VL, и Вы, как сервис-провайдер, предоставляете выделенные физические ресурсы под VDI на базе клиентских ОС. Важно отметить, что этот сценарий так же не даёт нам право на предоставление ресурсов по схеме one to many (1 апп. сервер <-> много клиентов). Только соотношение 1:1. По сути, мы отдаём пару юнитов в нашем шкафу в аренду одному и только одному клиенту. \r\n \r\n \r\n \r\nДля обхода подобных ограничений, как правило, строится система на базе классических терминальных решений (session-based архитектура, конечно, с desktop experience на борту) и отдаётся на «растерзание» клиентам или пользователям. \r\n  \r\nВ Windows Server 2016 решили это дело упростить и добавить метод «привязки» пользователей к конкретным терминальным узлам (в рамках RDS это узлы Remote Desktop Session Host, RDSH). В итоге, получаем новый вид RDS-коллекции — Personal Session Desktops (PSD) или частные рабочие столы на базе терминальных сессий. Очевидно, что можно провести аналогию с Personal Virtual Desktops в VDI, предназначенные так же для выделения «изолированной» среды пользователям. \r\n \r\nДавайте посмотрим на пару сценариев, которые успешно решаются благодаря PSD: \r\n \r\n \r\nВпервые PSD был анонсирован в Technical Preview 2 и изменен в TP3/TP4 (одним из изменений была замена структуры cmdlet, отвечающего за конфигурацию PSD). На текущий момент единственный способ развернуть PSD – PowerShell. Опцию GUI планируют добавить не ранее RTM-релиза.  \r\n \r\nДля целей демонстраций и тестирования можно использовать тип развертывания Quick Start на базе сессий. При этом будут установлены RD Connection Broker, RD Web Access и RD Session Host на одном физическом сервере. Для реального использования рекомендуется сформировать распределенную архитектуру. Не забываем, что каждый компонент RDS поддерживает виртуализацию (к примеру, 1 VM RDSH <-> 1 PSD User) и высокую доступность (например, RD Connection Broker иметь высокодоступную конфигурацию). \r\n \r\nНа всякий случай привожу шаги по конфигурации  \r\n \r\nДополнительно устанавливаем Desktop Experience на узле RDSH \r\n \r\n \r\n \r\nПереходим к созданию коллекции PSD. \r\n \r\n  был дополнен свитчем  , который используется для создания коллекции типа   (в Technical Preview 2 наименование свитча было другим, а именно  ) \r\n \r\n \r\nЕсли RDSH уже находится в одной из PSD-коллекций, то на его основе нельзя создать новую коллекцию. Только после удаления данного RDSH из текущей коллекции появится возможность определить его в новую. \r\n \r\n \r\nПосле создания коллекции PSD и привязки к ней пользователя, перейдем в RD Web Access узел (https://host fqdn/rdweb) для дополнительной проверки, используя учетные данные нашего пользователя. Должен появиться список коллекций, доступных пользователю \r\n \r\n \r\n \r\nОтмечу, что в привычном для администратора списке коллекций (Server Manager –> RDS –> Collection List) данный вид коллекций не отображается, поскольку он создается и управляется только с помощью PowerShell (по крайней мере, до выхода RTM-версии). \r\n  \r\nВот такой вид имеет панель «Пуск» в сессии PSD: \r\n \r\n \r\n \r\nНаша коллекция была создана с ключом  , поэтому пользователь автоматически был добавлен в группу Администраторов на выделенном сервере RDSH. \r\n \r\n \r\n \r\nДвигаемся дальше. \r\n \r\n \r\n \r\nВ Windows Server 2012 R2 RemoteFX-адаптер имеет ряд ограничений: 256 MB максимальный объем выделенной VRAM, поддержка только OpenGL 1.1, отсутствие поддержки OpenCL. Всё это сказывается на поддерживаемом количестве мониторов, разрешении и адекватной работе новых графических приложений (к примеру, Autocad Re-Cap требует OpenGL 3.3 и 1 GB VRAM, Photoshop CC — OpenGL 2.0 и 512 MB VRAM как минимум). \r\n \r\nWindows Server 2016 призван решить данные проблемы и вносит ряд изменений: \r\n \r\n \r\nПараметры RemoteFX определяются как через GUI, так и через PowerShell. \r\n \r\n \r\n \r\n \r\n \r\nЕщё буквально недавно у Microsoft не было ни одного мобильного клиента для доступа к удаленным рабочим столам, и приходилось использовать платные решения сторонних производителей с поддержкой RD Gateway. Однако по мере продвижения и использования RDS-решений были выпущены и добавлены следующие клиенты: \r\n \r\n  (MSTSC.EXE) был обновлен до 10-ой версии, которая имеет улучшенную поддержку кодека AVC/H.264 и режима  , призванного улучшить fps, понизить потерю цветности за счет использования функций аппаратного декодера H.264 в высоких разрешениях вплоть до   (GPU должен иметь поддержку DirectX 11.0, H.264 декодер Level 4.1/BT.709). Пока только в рамках полноценного клиента, но планируется добавить поддержку и для мобильных клиентов, обозначенных выше.  \r\nРежим AVC444 используется по умолчанию в RemoteFX, но есть возможность использования AVC444 и в других сценариях с помощью настройки групповой политики: \r\n \r\n \r\n \r\nи \r\n \r\n \r\n \r\n \r\nВМ второго поколения (Generation 2) стали доступны ещё в Windows Server 2012 R2, но их использование в рамках RDS/VDI (и не только) откладывалось. Например, возможность создания шаблонов сервисов VMM на базе Gen2 была добавлена только в рамках  .  \r\nВ Windows Server 2016 мы можем задействовать оба поколения для использования в различных типах коллекций (personal/pooled или personal session). Дополнительная конфигурация не требуется. \r\n \r\n \r\n \r\nЕсли ваше устройство, например Surface, поддерживает работу со стилусом, а локальная система не ниже Windows 10, то вы можете использовать стилус в рамках RDP-сессии. \r\n  \r\nВ Windows Server 2012/2012 R2 подобные устройства также перенаправляются, но используются в качестве замены мыши. В рамках Windows Server 2016 и Windows 10 стилусом можно рисовать или писать, открыв, например, граффити-приложение в Microsoft Edge, который так же обзавелся поддержкой при работе в удаленной сессии. \r\n \r\n \r\n \r\nОбновленные RemoteFX и RDP с поддержкой разрешения 4K увеличивают отдачу от ВМ с «тяжелыми» приложениями в рамках VDI и повышают их быстродействие по сравнению с Windows Server 2012 / 2012 R2 (конечно, необходимо провести тестирование и взглянуть на реальные цифры). \r\n \r\nMultiPoint Server, переехавший под «крыло» RDS, расширяет область применения удаленных рабочих столов и делает более привлекательным их использование (интерактивность в dashboard, простота настройки играют в этом не последнюю роль). \r\n  \r\nPersonal Session Desktop (PSD) упрощает предоставление рабочих столов в рамках DaaS-услуги и расширяет возможности RDS в Azure. Ожидать глобального изменения условий SPLA, думаю, не приходится. Скажем «спасибо», что и тут не забыли о нас. \r\n \r\nНадеюсь, что было интересно. Всем хорошей виртуализации и RDS-имплементации.\n\n      \n       \n    ", "hub": "Серверное администрирование / Интересные публикации / Хабрахабр", "title": "Что нового в Windows Server 2016 RDS. Часть 2"}
{"date": "18 февраля в 15:54", "article": "\n      В   мы бегло рассказали об основных возможностях Zabbix 3.0. Теперь хотим предложить вашему вниманию серию мини-обзоров новых возможностей Zabbix 3.0, начнем с  .  \r\n \r\nПочему мы смотрим прогноз погоды? Чтобы в солнечный казалось бы день, выбравшись в парк, не оказаться застигнутым врасплох под проливным дождем. Точный прогноз погоды помогает нам принять контрмеры: взять зонт, плащ, или вообще отправиться в кино вместо парка.  \r\n \r\n В новой версии Zabbix добавилась возможность также делать прогнозы, чтобы узнать будет ли шторм на сети или в серверной стойке, и когда это может произойти. \r\n \r\n Zabbix смотрит историю собранных данных и предсказывает, как будут развиваться события в дальнейшем. Затем триггеры могут быть настроены на срабатывание еще до наступления будущей проблемы с последующим запуском таких действий, как уведомления ответственных или автоматический запуск скриптов, удаленных команд. \r\n \r\n \r\n Чтобы визуализировать предсказание на графике, Вы выбираете элемент данных (например, кол-во свободного места на жестком диске) и создаете новый   элемент данных (назовем его 30m linear forecast) на его основе с использованием функции  . Указываете временной интервал в прошлом, который использовать для статистического анализа(например, 30 минут) и указываете как далеко строить прогноз(тоже допустим, 30 минут). Далее Zabbix делает расчет, находит лучший вариант и возвращает вам его: \r\nПроблема только в том, что forecast будет отображать на графике не в будущем, а значение прогноза будет привязано к времени, когда он был рассчитан. Чтобы увидеть прогноз на графике в будущем, используем другой   (назовем его 30m linear forecast shifted), который будет просто смещать значение прогноза на 30 минут вперед, используя доступную для всех функций возможность timeshift: \r\n \r\n \r\n \r\n \r\n \r\n \r\n Возможность прогнозировать может быть полезна при контроле свободного места на дисках. \r\nТриггер с простым порогом в 1ГБ или 10% не учитывает один очень серьезный нюанс — диски могут заполняться с совершенно различной скоростью. Поэтому он будет срабатывать либо слишком рано, либо (что хуже) слишком поздно, когда мы уже на нуле. \r\nА Триггер с использованием статистической функции timeleft даст нам 1 час чтобы очистить или расширить диск: \r\n И он будет работать для всех дисков, в независимости от их размеров. \r\n \r\nК тому же, для сложных случаев, Zabbix предлагает на выбор функции линии тренда, которые Вы могли использовать в привычном табличном процессоре: \r\n \r\n \r\n \r\n \r\n \r\nТочный прогноз погоды спасает нас от необходимости таскать зонт, когда он просто не нужен, и от возможности промокнуть, когда велика вероятность, что дождь будет. \r\nФункции прогнозирования в Zabbix помогут инфраструктуре работать как можно с наименьшим количеством сбоев, а сервисам оставаться доступными. Ведь предотвратить будущую проблему легче, чем уже свершившуюся. \r\n \r\nПеревод   из нашего блога.\n\n      \n       \n    ", "hub": "Серверное администрирование / Интересные публикации / Хабрахабр", "title": "Zabbix 3.0: Прогнозирование проблем"}
{"date": "18 февраля 2013 в 00:42", "article": "\n      Некий Питер Коттл ( ) сделал интерактивную обучалку по основам ветвления в Git. Есть несколько простых обучающих уровней, где нужно сделать пару коммитов, а затем merge или rebase, есть и сложные уровни, над которыми придется подумать. Можно также сохранять уровни и делиться ими с друзьями. \r\n \r\nХотя сам автор утверждает, что приложение еще сырое, я советую всем, кто интересуется гитом,  . \r\n \r\n \r\n \r\nПоддерживаемые комманды: \r\n \r\n \r\n  Переставил ссылку из под хабраката в начало статьи\n\n      \n       \n    ", "hub": "Системы управления версиями / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2010 в 15:44", "article": "\n      Сегодня я обнаружил,   годится не только для того, чтобы пугать неподготовленных людей неожиданной работоспособностью странных доменных имён (таких,   В этой вики есть   на которой предлагают скачать шрифты, пригодные для написания и чтения текстов на шестнадцати языках: амхарском, арабском, бенгальском, греческом, китайском, корейском, кхмерском, русском, тайском, тамильском, урду, фарси, хинди, японском и паре еврейских (иврит и идиш). \r\n \r\nНа той же странице сказано, что все эти шрифты были выбраны из источников, в которых они представлены как разрешённые к дальнейшему бесплатному распространению.\n\n      \n       \n    ", "hub": "Типографика / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 18:00", "article": "\n      До выхода HTML5 использование формул было сущим наказанием. Судите сами: в 2005-м необходимо было иметь под рукой либо специальный браузер, либо разбивать текст на собственно HTML и вставки из изображений или PDF. Поиск и прочие операции редактирования и/или вывода на экран/бумагу был неоднозначной задачей, коей посвящались целые  .  \r\n \r\nВ 2012-м было уже попроще. Появилась возможность подключать необходимые плагины (Firemath для FireFox и Daum Equation Editor для Chrome). Но неоднозначность стандартов (и поддержки) фактически заставляла писать одну и ту же статью для каждого из браузеров (и для их версий). Или встречать пользователей волшебным приветствием «Ваш браузер надо обновить/дополнить расширением». \r\n \r\nНеудобно? — Да! Отнимало много времени на поиск универсального решения? — Да! Заставляет думать о том, какой тип записи лучше (презентационный или содержательный), каким конвертером пользоваться (а их только общеизвестных с десятка полтора)? — ДА! ДА! ДА! \r\n \r\nВ результате работа по публикации превращалась в освоение двух-трех лексиконов разметки и изучения работы минимум одной программы-перекодировщика. \r\n \r\nТеперь, с приходом HTML5, все стало намного проще. В нем появился новый контейнер  .  \r\n Каждый допустимый экземпляр MathML должен быть внутри этого контейнера.  \r\nОн не допускает вложений, но внутри может быть произвольное число других дочерних элементов. \r\n \r\n \r\nВ дополнение к следующим атрибутам, тэг   воспринимает любые атрибуты из  . \r\n \r\n  \r\n При условии использования вместе с  . \r\n  \r\n Указывает направление формулы:   — слева направо или   — справа налево. \r\n  \r\n Используется для установки гиперссылки на указанный URI. \r\n  \r\n Цвет фона. Вы можете использовать  ,   и   . \r\n  \r\n Цвет текста. Вы можете использовать  ,   и   . \r\n  \r\n Этот атрибут определяет способ вывода. Возможные значения: \r\n  \r\n \r\nЗначение по умолчанию   . \r\n  \r\n mode <span title=«Этот устаревший API больше не используется, но, вероятно, все еще работает.» \r\n  \r\n  \r\n Устаревшие значение   . \r\n Возможные значения:   (который имеет тот же эффект, как   ) и   . \r\n overflow \r\n Определяет, как выражение ведет себя, если текст слишком длинный и не помещается в указанном диапазоне ширины. \r\n Возможные значения:   (по умолчанию),  ,  ,  ,   . \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nНадеюсь, развитие этого нужного и полезного тэга продолжится. \r\n \r\n \r\nТест браузера на поддержку MathML:   или  \n       \n    ", "hub": "HTML / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 03:39", "article": "\n         \r\n \r\nПриветствую. Представляю сообществу проект  . \r\n \r\nKonva.js — это фреймворк, который позволяет работать c canvas 2d в объектном стиле с поддержкой событий. \r\n \r\nКратко список особенностей выглядит так: \r\n \r\n \r\nДалее подробней рассмотрим возможности фреймворка с примерами кода. \r\n \r\n \r\nВсё начинается со Stage, который объеденяет в себе пользовательские слои (Layer). \r\nКаждый слой (Layer) представляет из себя один canvas элемент на странице и может содержать в себе фигуры, группы фигур или группы групп. \r\n \r\nКаждый элемент может быть стилизован и трансформирован. \r\n \r\nКак только вы настроили Stage и слои, добавили фигуры, вы можете подписываться на события, изменять свойства элементов, запускать анимацию, создавать фильтры. \r\n \r\nМинимальный пример кода  : \r\n \r\n \r\n \r\n \r\n \r\n \r\nKonva.js поддерживает следующие фигуры: прямоугольник (Rect), круг (Circle), овал (Ellipse), линия (Line), изображение (Image), текст (Text), текстовый путь (TextPath), звезда (Star), ярлык (Label), svg путь (Path), правильный многоугольник (RegularPolygon). Так же вы можете создать  : \r\n \r\n \r\n \r\n \r\n \r\n \r\nКаждая фигура поддерживает следующие свойства стилей: \r\n \r\n \r\n \r\n \r\nРезультат: \r\n \r\n \r\n \r\n \r\nИспользуя Konva.js, вы легко можете подписываться на события ввода (click, dblclick, mouseover, tap, dbltap, touchstart и так далее), на события изменения аттрибутов (scaleXChange, fillChange), и на события drag&drop (dragstart, dragmove, dragend). \r\n \r\n \r\n \r\n \r\n \r\n \r\nФреймоворк имеет встоенную поддержку drag. На данный момент нет поддержки drop событий (drop, dragenter, dragleave, dragover), но они достаточно просто реализуются  . \r\n \r\nЧтобы элемент можно было перетаскивать, достаточно поставить свойства draggable = true. \r\n \r\n \r\n \r\nПри этом вы сможете подписываться на drag&drop события и настраивать ограничения по перемещению.  . \r\n \r\n \r\nKonva.js включает в себя множество фильтров: размытие, инверсия, сепия, шум и так далее. Полный список доступных фильтров можно посмотреть в  . \r\n \r\nПример использования фильтра: \r\n \r\n \r\n \r\n \r\n \r\nСоздавать анимацию можно двумя способами: \r\n \r\n1. Через объект «Animation»: \r\n \r\n \r\n \r\n \r\n2. Через объект «Tween»: \r\n \r\n \r\n \r\n \r\n \r\nПри построении крупного приложения, крайне удобно использовать поиск по созданным элементам. Konva.js позволяет искать объекты с помощью селекторов, используя методы find (возвращает коллекцию) и findOne (возвращает первый элемент коллекции): \r\n \r\n \r\n \r\n \r\nВсе созданные объекты вы можете сохранить в JSON формат, чтобы, например, сохранить на сервер или локальное хранилище: \r\n \r\n \r\nА так же создать элементы из JSON: \r\n \r\n \r\n \r\nKonva.js имеет множество инструментов, для значительного повышения производительности. \r\n \r\n1. Кеширование позволяет отрисовать некоторый элемент в буфферный canvas и потом рисовать его оттуда. Это может значительно повысить производительность отрисовки сложных объектов как, например, текст или объекты с тенями и контурами. \r\n \r\n \r\n \r\n \r\n2. Работа со слоями. Так как фреймворк поддерживает несколько canvas элементов, вы можете распределять объекты на ваше усмотрение. Допустим, приложение состоит из сложного фона с тектом и нескольких передвигаемый фигур. Логично будет фон и текст перенести на один слой, а фигуры на другой. При этом при обновлении положения фигур, фоновый слой можно не перерисовывать. \r\n \r\n \r\n \r\nБолее подробный список советов по повышению производительности доступен здесь:  \r\n \r\n \r\nGitHub:  \r\nДомашняя страница:  \r\nДокументация с примерами:  \r\nПолное API: \r\n \r\nДанный проект является форком известной библиотеки  , которая автором уже не поддерживается. \r\nСписок изменений от последней официальной версии KineticJS можно увидеть  .\n\n      \n       \n    ", "hub": "HTML / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 00:56", "article": "\n      В данной публикации речь пойдёт о настройке важных параметров пула ASP.NET-приложений при вызове удалённых веб-сервисов и активной работе с сетью на стороне сервера через стандартные классы .NET.  \r\n  \r\n \r\n \r\n \r\nПриходилось ли вам когда-нибудь самим настраивать производственные веб-сервера (production servers) под управлением ОС Windows Server 2008 R2/IIS 7.5 и выше? Для системных администраторов, имеющих большой опыт работы с IIS, скорее всего, это тривиальная задача, но вот для веб-разработчиков, которым по различным причинам порой приходится самим участвовать в настройке «боевых» серверов, данная информация может оказаться весьма полезной.  \r\n \r\nИтак, приступаем. Ускоряем сайт на ASP.NET — экономим деньги предприятия и нервы администратора. \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nВ конце прошлого года в одной крупной организации мы столкнулись с проблемами производительности веб-серверов при резко увеличившейся пользовательской нагрузке. В веб-приложении на тот момент было зарегистрировано более 200 000 клиентов. В обычном режиме одновременно работает около 1000 пользователей, за день примерно 10-15% уникальных посетителей от общего числа зарегистрированных, поэтому нагрузка относительно невысокая. Однако существуют пиковые нагрузки, при которых система оказывается практически неработоспособной.  \r\n \r\nВеб-администаторы проверили всё, что можно, и никак не могли понять, в чём дело. Ведь несмотря на то, что по всем основным параметрам системы на физическом уровне с производительностью было всё хорошо, возникали сбои с доступностью сервисов, а в пуле собиралась огромная очередь запросов. В организации используется NLB-кластер на 4 узла (Windows Server 2008 R2 + IIS 7.5 + .NET 4.5), есть запас по загрузке ЦП и памяти, сетевые каналы большие, количество используемых портов достаточное. Все проверки указывали на то, что проблемы кроются в недрах IIS и настройке пула ASP.NET. Живой пример, когда администраторам не помешала бы помощь опытных веб-разработчиков… \r\n \r\n \r\n \r\nПрежде чем начинать настройку конфигурации IIS, обратите внимание на счетчики производительности ASP.NET, оцените текущую и пиковую загрузки системы, зафиксируйте имеющиеся показатели. Проверьте логи на наличие ошибки “HTTP Error 503.2 — Service Unavailable”. Постарайтесь определить, не блокируется ли часть запросов в очереди.  \r\n \r\nЕсли производительность системы удовлетворяет потребностям заказчика, то лучше оставить параметры по умолчанию, ведь они рассчитаны для большинства ASP.NET приложений. \r\n  \r\nПри конфигурации IIS можно выделить два основных параметра, влияющих на доступность приложения и его производительность. \r\n  \r\n1. Параметр   — максимальное количество одновременных запросов в приложении. Увеличение числа одновременных запросов IIS расширит доступные ресурсы веб-сервера для обслуживания запросов. Значение по умолчанию — 5000. \r\n \r\nНаиболее быстро изменить параметр   можно утилитой   через командную строку. Сделать это можно как глобально для всех сайтов IIS через файл  , так и для отдельного сайта (приложения).  \r\n \r\n \r\nВыполняем команду, затем открываем в IIS раздел «Configuration Editor» для корневого каталога и проверяем новое значение установленного параметра  . Причём здесь же можно вручную изменить это значение. \r\n \r\n \r\n \r\n \r\nДля установки данного параметра наиболее часто используется формула:  \r\n< >, где   — количество одновременно работающих пользователей. \r\n \r\n2. Параметр   — максимальное количество запросов, которые драйвер Http.sys размещает в очереди пула приложений. Когда очередь заполнена, новые запросы получают ошибку «503.2 — Service Unavailable». Значение по умолчанию — 5000.  \r\n \r\nДанный параметр можно настроить несколькими способами: \r\n \r\nВ качестве примера изменим данный параметр для пула «DefaultAppPool» через командную строку: \r\n \r\n \r\nВыполняем команду, затем открываем в IIS раздел «Application Pools», выбираем в списке пул «DefaultAppPool », заходим в меню «Advanced Settings» и проверяем. \r\n \r\n \r\n \r\n \r\n \r\n   \r\nASP.NET ограничивает число рабочих потоков и потоков портов завершения вызова, используемых для выполнения запросов. Если веб-приложение на стороне сервера активно использует вызовы внешних веб-сервисов, стандартные классы из пространства имён System.NET для организации запросов по сети, то могут возникнуть конфликты низкой производительности и взаимоблокировок. Вначале часть запросов может просто “подвисать”, время выполнения будет значительно возрастать. В худшем случае, если используется классический режим настройки пула (classic pipeline), это вообще может привести к перезагрузке пула (recycle). Обнаружение взаимоблокировки ASP.NET не выполняется для пула, запущенного в   (по умолчанию в IIS 7 и выше).  \r\n \r\nРабота пулов приложений в интегрированном режиме имеет несколько преимуществ по сравнению с работой в классическом режиме. Рекомедуется запускать пулы приложений в integrated mode. \r\n \r\nНа рисунке ниже наглядно видно, как происходит обработка запросов в ASP.NET и какие параметры имеют наиболее важное значение: \r\n \r\n \r\n \r\n \r\nДля оптимальной работы веб-приложений по умолчанию включен режим автоконфигурации настроек пула. В этом случае, cвойство   равно \" \" для секции < > в файле  , а другие ключевые параметры не заданы вообще. \r\n  \r\nХорошенько “покопавшись” в MSDN и файле  , я нашёл описание базовой конфигурации пула. Есть 7 основных параметров, влиящих на работу ASP.NET с сервисами и сетью: \r\n \r\nПараметр   определяет максимальное количество одновременных запросов с одного IP-адреса. При включенной по умолчанию автоконфигурации пула этот параметр определяется по формуле:  \r\n  =  , где   — это количество ядер процессора \r\n \r\nТаким образом, на сервере с 4-х ядерным процессором максимальное кол-во одновременных подключений к конечному IP-адресу равно 48=12*4 (по умолчанию). \r\n \r\nСамый простой способ обойти данное ограничение — это прямо в коде своего ASP.NET приложения в методе   в файле   указать следующее: \r\n \r\n \r\nБолее гибко настраивать   лучше через конфигурационные файлы на уровне домена приложения (web.config) или веб-сервера (applicationHost.config). Секция < > содержит параметры, которые определяют, как .NET Framework подключается к сети. \r\n \r\n \r\n  Схема для адреса параметра   должна быть такой: \r\n \r\n \r\nУвеличение   позволяет делать больше одновременных вызовов к удаленным сервисам.  Необходимо понимать, что недостаточно только обойти ограничение на количество одновременных подключений к сервису. Так как увеличение числа одновременных вызовов приводит к увеличению использования потоков CLR, которые используются для создания удаленных и обработки обратных вызовов.  \r\n \r\nASP.NET через параметр   устанавливает ограничения потоков на рабочем процессе   (начиная с IIS 7). В связи с тем, что ASP.NET встроена в IIS, процессы ASP.NET формируют запросы на рабочих потоках. Из-за недостаточного количества потоков в CLR ThreadPool запросы будут становиться в очередь и “подвисать”. \r\n \r\nАттрибуты, заданные в секции < >: \r\n1. Параметр   — указывает максимальное количество рабочих потоков для каждого процессора в пуле потоков среды CLR. Значение по умолчанию — 20. Максимальное значение — 100. \r\n \r\n2. Параметр   — указывает максимальное количество потоков ввода/вывода для каждого процессора в пуле потоков среды CLR. Значение по умолчанию — 20. Максимальное значение — 100.  \r\n \r\n3. Параметр   — указывает минимальное количество рабочих потоков для каждого процессора, которые могут быть предоставлены немедленно для обслуживания удаленного запроса. Значение по умолчанию — 1.  \r\n \r\n4. Параметр   — указывает минимальное количество потоков ввода/вывода для каждого процессора, которые могут быть предоставлены немедленно для обработки обратного вызова. Значение по умолчанию — 1. \r\n \r\nПараметры  /  позволяют оперативно справиться с внезапно возросшим количеством одновременных подключений, когда в случае бездействия пул потоков может не иметь достаточно времени, чтобы достичь оптимального уровня потоков. \r\n \r\nАттрибуты, заданные в секции < >: \r\n1. Параметр   — определяет количество потоков, которые могут быть использованы для работы, кроме обработки входящих запросов к рабочему процессу. Этот параметр не дает процессу ASP.NET использовать потоки из пула для обработки нового HTTP-запроса, если общее число потоков в пуле опустится ниже этого предела. Значение по умолчанию — 8. \r\n \r\n2. Параметр   — определяет минимальное количество свободных потоков, которые ASP.NET держит доступными для выполнения новых локальных запросов. Значение по умолчанию — 4. \r\n \r\nОбратите внимание, параметры  ,  ,  ,   неявно умножаются на число процессоров, а параметры   и   — нет. \r\n \r\nASP.NET не будет выполнять более, чем следующее количество одновременных запросов: \r\n(  *  ) —  \r\n \r\n  на весь пул приложения, то есть на каждый рабочий процесс  , обслуживающий пул, имеется один пул потоков CLR ThreadPool. Для всех доменов приложений (сайтов), настроенных на один пул, используется общий набор потоков. Следовательно, для требовательных к ресурсам приложений лучше использовать отдельные пулы. \r\n \r\n \r\nПрежде всего, необходимо точно определить количество процессоров на веб-сервере. Как вариант, можно посмотреть TaskManager -> вкладка «Performance». Если процессор поддерживает режим   (HTT), значит половина ядер логические (Logical processors), а не физические (Cores). Например, при включенном режиме HTT процессор с 4-мя физическими ядрами будет работать как 8 логических ядер: \r\n \r\n \r\n \r\n \r\nТакже можно попробовать воспользоваться следующими командами в командной строке: \r\n \r\n или \r\nНапример, на сервере с 4-мя процессорами и свойством  =\" \" ASP.NET будет иметь следующие параметры по умолчанию: \r\nmaxConnection – 48; maxWorkerThreads – 80; maxIoThreads – 80, minFreeThreads – 8, minLocalRequestFreeThreads – 4. \r\n \r\nЕсли веб-страница на backend-части делает несколько сетевых вызовов для каждого запроса, то MSDN рекомендует использовать следующие настройки конфигурации: \r\n \r\n \r\nВ этом разделе приведены только рекомендации, а не правила. Причём дата публикации этих данных довольно давняя. Для нашей “боевой” системы мы используем немного другие параметры конфигурации. Данные формулы — хорошая отправная точка для старта оптимизации, они хорошо показывают зависимость параметров друг от друга. Например, увеличив значение параметра   в несколько раз, вы легко можете “прикинуть” базовые значения для остальных параметров.  \r\n \r\nИзменения в секцию < > разрешено вносить только в файле   из-за установленного там же атрибута  =\" \" при добавлении секции  .  \r\n \r\n : \r\n \r\n \r\nЧтобы иметь возможность устанавливать значения секции   для каждого приложения в отдельности через  , необходимо установить свойство  =\" \". \r\n \r\n \r\n  после внесения изменений требуется обновить Application pools.  \r\n \r\nПомните, что увеличивать данные параметры нужно только в случае необходимости при наличии достаточного количества ресурсов ЦП. \r\n \r\nДля анализа производительности веб-серверов рекомендую настроить счётчики ASP.NET через Performance Monitor: \r\n \r\n \r\nДля более глубокого анализа процесса  , обслуживающего пул приложений IIS, можно попробовать отладчик   из  .  \r\n \r\nВозможно, что после проверки счётчиков вам придётся внести корректировки в конфигурацию вашей системы. \r\n \r\n \r\nДля лучшего понимания работы IIS рекомендую ознакомиться, как происходит процесс обработки запроса от браузера пользователя до конечного пула приложения ASP.NET в этой полезной статье  . \r\n \r\nЕсли вы используете IIS8 — не будет лишним обратить внимание на    \r\n \r\n \r\nДля сайтов, которые не совершают частые сетевые запросы на стороне сервера, стандартных настроек пула должно хватать (processModel/autoConfig=“true”). При этом IIS выставит ограничение в 20 рабочих потоков и 12 удаленных соединений на ядро. При превышении этих значений запросы начнут становиться в очередь и производительность веб-приложения упадёт. \r\n  \r\nЕсли ваш сайт работает хорошо и вы можете оценить предполагаемую нагрузку на систему, то не стоит ничего менять. Если же у вас начинаются “зависания” при обработке запросов к различным сервисам — не следует сразу винить во всем железо! Лучше внести изменения в базовую конфигурацию ASP.NET. Имейте в виду, что изменение базовых параметров пула приложений непременно приведёт к увеличению загрузки процессора. Оптимальная балансировка всех параметров системы — ключ к стабильной и производительной работе оборудования. Как говорится, “предупрежден — значит вооружен”.  \r\n \r\nПриглашаю всех поделиться вашим опытом настройки и оптимизации работы производственных веб-серверов на платформе Windows Server. \r\n \r\n \r\n \n       \n    ", "hub": "Серверная оптимизация / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2009 в 15:21", "article": "\n       \r\n \r\nГлобализация типографического рынка и увеличивающийся интерес к созданию многоязычного шрифтового дизайна — источник большого оптимизма среди дизайнеров и печатников. Но, несмотря на широкое распространение новых красивых шрифтов, многие до сих пор не поддерживают ряд европейских языков   и лишь единицы поддерживают африканские и азиатские языки. Получается классическое противоречие между рекламными заявлениями создателей и реально поддерживаемыми языками. \r\n \r\nЦель этой статьи — объяснить основные особенности использования и оформления латинских диакритических знаков, помочь дизайнерам делать более осознанный выбор в их использовании. В статье используются диакритические знаки, используемые в центральной Европе по нескольким причинам: \r\n1) они не очень-то известны западным дизайнерам. Чешский печатник Tomáš Brousil сказал: «Для западных дизайнеров наши диакритические знаки так же чужды, как и надписи по-арабски». То, что они видят эти знаки только как дополнение для знакомых латинских букв, зачастую ведёт к недооценке их важности; \r\n2) они хорошо знакомы автору статьи; \r\n3) многие центрально-европейские языки и чешский язык в частности одни из первых начали использовать диакритические знаки для латинских букв (идея замены   диакритическими знаками была предложена Яном Гусом в его «De Ortographia Bohemica» в 1412 году). \r\n \r\n \r\n \r\n \r\nВ разное время знаки, дорисованные к буквам или расположенные рядом, использовались во множестве различных систем записи. Греки использовали их для яркого выражения тональности (греческая политоника), арабы и евреи — для обозначения гласных звуков. В некоторых индийских слоговых письменностях они использовались для изменения звучания слога (т.е. изменялся оригинальный гласный звук этого слога). \r\n \r\nВ латинском алфавите диакритические знаки чаще всего являются инструментом расширения основного алфавита для использования в конкретном языке. Это делается для представления составных букв (графемы) или для обозначения звуков (фонемы) конкретного языка, в тех случаях, когда не получается их составить из букв основного алфавита или такая замена получается громоздкой  . Грубо говоря, диакритика используется, когда в основном латинском алфавите заканчиваются буквы или же требуется показать специфическое поведение (например, передать мягкость, протяжённость, ударение и т.п.). \r\n \r\n \r\n \r\n \r\nВообще-то диакритика — это далеко не единственный способ расширить систему записи. Можно придумать новые буквы (например,   в немецком языке), создать кластеры из букв (диграфы, триграфы), как это делается в некоторых языках (например, в  )  . \r\n \r\nВ наше время диакритика используется в большинстве европейских языков (  по теме на сайте FontShop). Расширенный вариант латинского алфавита используется в ряде стран Африки, во Вьетнаме. Большое число диакритических знаков используется при транслитерации (например,   для китайского). Ещё одно известное расширение латиницы —   (International phonetic alphabet, IPA). \r\n \r\n \r\nОчень часто (но далеко не всегда) буквы с диакритикой являются полноценными членами национального алфавита. Например, в датском языке алфавит состоит из «a–z, æ, ø, å». Буквы с диакритикой являются составными: они содержат в себе символ и диакритический знак. Т.е. диакритические знаки — такие же неотъемлемые части буквы, как основные штрихи и полуовалы. И хотя очень часто диакритические знаки отделены от основной буквы, это не делает их менее важными или же элементом пунктуации. \r\n \r\n \r\n \r\n \r\nПоследнее утверждение очень абстрактно и стоит его пояснить. Пунктуация — способ разделения и структурирования предложений. Стиль знаков пунктуации может отличаться (а зачастую и обязательно отличается) от стилистических принципов построения букв. А вот диакритические знаки должны гармонировать именно с буквами, к которым они относятся, поскольку их задача вместе с буквами передавать смысл или звучание отдельных слов. \r\n \r\nЭто можно пояснить на примере с альтернативным кароном. В чешском и словацком языках у карона есть специальная вертикальная форма для использования в высоких буквах (ď, ť, ľ, Ľ). Несомненно, что его появление — это решение проблемы ограниченного пространства по вертикали на металлической литере. Обычный карон (ě, š, č, ň, …) не получится разместить над высокими буквами, поэтому его альтернативная форма размещается рядом с буквой. Очень часто его ошибочно называют знаком, похожим на апостроф или даже апострофом. Но альтернативный карон — это не апостроф, хотя их внешнее подобие может сбивать с толку. Чешское слово «rozhoď» (d с кароном) это повелительная форма глагола «рассыпать», а слово «rozhod’» (апостроф в конце слова) обозначает «[он] решил» в неформальном написании, которое распространено и в литературе.  \r\n \r\nПроблема возможного неверного прочтения текста заставляет дизайнеров находить различные пути для визуального разделения карона и апострофа. Решение, используемое большинством современных дизайнеров основано на максимально близком расположении диакритического знака с буквой (см. для примера шрифт   (Peter Biľak) или же шрифты от  ). Карон выполнен как простой вертикальный клин, в то время как апостроф больше и сохраняет свою форму, подобную запятой. Так здесь решена проблема между буквами и знаками препинания. \r\n \r\n \r\n \r\n \r\nНа заре развития компьютеров и Интернета были проблемы с кодировкой букв нелатинского алфавита и многие привыкли игнорировать диакритику в письмах, чатах и т.п. Но это не означает, что диакритические знаки потеряли свою актуальность и их использование необязательно в современном дизайне. Они всё ещё обязательны при оформлении текста и для достижения максимальной читабельности. \r\n \r\n \r\nПроблема диакритических знаков в том, что неясно, как дизайнеру, не сильно знакомому с данным языком, оценить результат. Стоит также отметить, что для одних и тех же знаков ожидаемый результат может отличаться в зависимости от языка или географического региона. Современные крупные изготовители шрифтов создают почти идентичные наборы диакритических знаков для всех создаваемых шрифтов. Это сомнительное поточное производство ведёт к ошибкам в дизайне и ещё больше запутывает ситуацию. С другой стороны существуют традиционные правила для создания корректных диакритических знаков. Такая диакритика обеспечивает лучшую читабельность текста и эстетическую привлекательность для носителей языка. Рассмотренные ниже нюансы помогут дизайнерам определиться с правильным дизайном диакритических знаков. \r\n \r\n \r\n \r\n \r\n \r\n \r\nКак уже сказано выше, диакритические знаки входят в состав буквы. Поэтому они не должны выделяться (быть светлее или темнее) из самой буквы. Более того, буквы с диакритикой и без неё имеют одинаковую семантическую важность, поэтому важность диакритического знака не должна уменьшаться путём уменьшения его размера. Это становится ещё более важным для небольших размеров шрифтов в 10—12 pt. Составители шрифтов часто оценивают диакритические знаки на больших размерах, не обращая внимание на их качества при меньших, используемых в основном тексте. Чем меньше шрифт, тем больше должен быть диакритический знак в сравнении с основной буквой. \r\n \r\n \r\n \r\n \r\n \r\n \r\nРасположение диакритического знака очень важно, т.к. знак должен относиться к правильной букве при чтении. Многие знаки располагаются в визуальном центре над или под буквой. Тем не менее, есть ряд исключений, когда диакритический знак расположен справа от буквы (например, огонек в буквах -a,e,u-) или же за буквой (упоминавшийся выше альтернативный карон). Корректное позиционирование близкорасположенных знаков имеет важное значение, но на практике может превратиться в большую проблему. Результат может сильно отличаться при изменении размера или при использовании курсива. Кроме того, у разных дизайнеров разное мнение по поводу позиционирования. Общее правило простое: диакритические знаки не должны «выпадать» из базовой буквы. Также они должны быть визуально привязаны к ней. \r\n \r\n \r\n \r\n \r\n \r\nНасыщенность и позиционирование, разумеется, имеют б́ольшую важность, чем гармония стиля. Но, тем не менее, и её не нужно недооценивать. На практике диакритические знаки должны находиться в некотором контрасте с базовой буквой. Дизайнеры шрифтов, часто перестаравшись, используют ту же контрастности и оси напряжения, как и для обычных букв. Но диакритические знаки — это не маленькие буквы, поэтому повышенный контраст в таких знаках как акут, гравис или карон может слишком выделять его относительно базовой буквы. \r\n \r\n \r\n \r\n \r\nДля достижения высокой чёткости и выраженности форм дизайнеры часто используют так называемую «симметричную диакритику» в своих шрифтах (см. рисунок ниже). Таким способом достигается хороший баланс стилистического единства и читабельности. Оси напряжения в таких диакритических знаках разные, но имеют одну цель: создание симметричных форм. Создание и позиционирование асимметричной диакритики — это уже для мастеров. Такая диакритика хорошо смотрится в шрифтах, основанных на каллиграфическом письме широкой кистью  . \r\n \r\n \r\n \r\n \r\nРекомендуется уменьшать контрастность между толстыми и тонкими линиями диакритического знака, т.к. они меньше букв и тонкие части становятся чрезмерно тонкими, слабыми. Заслуживают упоминания также и другие принципы: стиль концов штрихов должен быть таким же, как и в буквах. Более изысканные варианты (такие как засечки, полукруглые окончания и пр.) обычно не используются в диакритике.  \r\n \r\nЕсли в шрифте используется открытый внутрибуквенный просвет, то диакритические знаки (седиль, огонек) также должны выглядеть похоже.  \r\n \r\nПоскольку над заглавными буквами место очень ограничено, для них должны использоваться специальные диакритические знаки. Они поменьше, пониже и лучше сочетаются именно с заглавными буквами. Некоторые дизайнеры дополнительно создают диакритику для ещё и капители. Это настоятельно рекомендуется, если стиль заглавных (в т.ч. и капители) и строчных букв значительно отличается. В случаях же незначительных отличий, диакритика, используемая в строчных буквах, применяется и для капители. \r\n \r\nПоследнее, но не менее важное замечание: диакритические знаки не должны быть чрезмерно похожи друг на друга. Слишком плоский акут «´» легко может быть перепутан с макроном «¯». Слишком закруглённый карон «ˇ» легко спутать с бреве «˘». Именно по этой причине при создании диакритических знаков не стоит сильно отдаляться от первоначальных общепринятых форм. \r\n \r\n \r\nДиакритические знаки не должны соприкасаться, и наслаиваться, создавать неразборчивые формы. Поэтому зачастую требуется аккуратная подгонка. Ни один дизайнер шрифтов не может предсказать и учесть все возможные комбинации диакритических знаков, поэтому рекомендуется подгонять буквы по своему мнению и вкусу. \r\n \r\n \r\n \r\n \r\n \r\nБольшая часть проблем при работе с диакритикой осталась в стороне от этой ознакомительной статьи. Но уже из написанного выше должно быть понятно, что диакритические знаки — это не просто дополнение для букв основного алфавита. Они образуют буквы. \r\n \r\nПредоставленные здесь изображения акцентируют внимание на тех шрифтах, к которым по вполне очевидным причинам предъявляются более высокие требования. Внимательное изучение и понимание назначения, эстетики поможет дизайнерам выбирать шрифты с хорошо проработанной диакритикой (или по крайней мере осознавать нехватку чего-либо). При этом повышается качество текстов на родных языках.\n       \n    ", "hub": "Типографика / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 18:24", "article": "\n      В данной публикации хочу рассказать о том, как нетрадиционными методами мы смогли снизить нагрузку на серверы и ускорить время обработки страницы в несколько раз. \r\n \r\n \r\n \r\nТрадиционные методы, думаю, всем известны: \r\n \r\n \r\nВ нашем же проекте всё это было сделано, но при этом проблема скорости обработки страниц сохранялась. Средняя скорость обработки страницы была в районе 500мс. В один прекрасный момент пришла идея проанализировать, какие ресурсы есть, и на что они могут тратиться. \r\n \r\nПосле анализа были выявлены следующие основные ресурсы, которые нужно мониторить: \r\n \r\n \r\nВ нашем проекте мы почти не используем запись на диск, поэтому сразу исключили 4-й пункт. \r\n \r\nДальше начался поиск узких мест. \r\n \r\n \r\nПервое, что было испробовано — профайлинг MySQL запросов, поиск медленных запросов. Подход не принёс особого успеха: несколько запросов было оптимизировано, но среднее время обработки страниц не сильно изменилось. \r\n \r\n \r\nДальше была попытка профайлинга кода с помощью XHProf. Это дало ускорение в узких местах и смогло снизить нагрузку примерно на 10-15%. Но основной проблемы это тоже не решило. (Если будет интересно, могу отдельно написать статью о том, как оптимизировать с помощью XHProf. Вы только дайте знать в комментариях.) \r\n \r\n \r\nВ третьем этапе пришла мысль посмотреть, сколько памяти уходит на обработку запроса. Оказалось, что в этом и есть проблема – простой запрос может требовать загрузить до 20мб кода в ОЗУ. Причём причина этого была непонятна, так как идёт простая загрузка страницы — без запросов в базу данных, или загрузки больших файлов. \r\n \r\nБыло принято решение написать анализатор и выяснить, сколько памяти требует каждый пхп файл при его включении. \r\n \r\nАнализатор очень простой: в проекте уже был автозагрузчик файлов, который на основе имени класса сам подгружал нужный файл (autoload ). В нём просто было добавлено 2 строки: сколько памяти было до загрузки файла, сколько стало после. \r\n \r\nПример кода: \r\n \r\n \r\nПервая строка сохраняет, сколько памяти было в начале, последняя строка – вычитает разницу и добавляет в список загрузок. Так же профайлер сохраняет очередь загрузки файла (backtrack). \r\n \r\nВ конце выполнения всего кода мы добавили вывод панели с собранной информацией. \r\n \r\n \r\nАнализ показал очень интересные причины того, почему может использоваться излишняя память. После того, как мы проанализировали все страницы и убрали загрузку лишних файлов страница стала требовать менее 8мб памяти. Обновление страницы стало быстрее, нагрузка на сервера уменьшилась и на тех же машинах появилась возможность обрабатывать больше клиентов. \r\n \r\nДальше идёт список вещей, которые мы изменили. Здесь стоит отметить, что сам функционал не изменился. Изменилась только структура кода. \r\n \r\nВсе примеры сделаны специально упрощёнными, так как предоставить оригинальный код проекта возможности нет. \r\n \r\n \r\nПример: \r\n \r\n \r\nФайл сам по себе очень маленький, но при этом, он тянет за собой по крайней мере один другой файл, который может быть достаточно большой. \r\n \r\n \r\n \r\n \r\n \r\n \r\nОчень похоже на предыдущий пункт. Пример: \r\n \r\n \r\nЕстественно, что PHP не знает, что вам нужна только 1 функция. Как только идёт обращение к данному классу, идёт подгрузка всего файла. \r\n \r\n \r\nЕсли есть подобные классы, то стоит выделить функционал который используется везде в отдельный класс. В текущем классе можно ссылаться на новый класс для совместимости. \r\n \r\n \r\nПример: \r\n \r\n \r\nИли: \r\n \r\n \r\nТ.е. пример похож на предыдущий, только теперь константа. Решение такое же как и в прошлом случае. \r\n \r\n \r\nПример: \r\n \r\n \r\nЕсли Handler используется часто, то проблемы нет. Совсем другой вопрос, если он используется только в 1 из функций из 20. \r\n \r\n \r\nУбираем из конструктора, переходим на ленивую загрузку через магический метод __get или же например так: \r\n \r\n \r\n \r\nПример: У вас есть большой файл с настройками всех страниц всех пунктов меню. В файле может быть много массивов. \r\n \r\nЕсли часть данных используется редко, а часть часто – то данный файл является кандидатом на разделение. \r\n \r\nТ.е. стоит подгружать данные настройки только тогда, когда они нужны. \r\n \r\nПример использований памяти у нас: \r\n \r\nФайл в 16 кб, просто массив данных – требует от 100 кб и выше. \r\n \r\n \r\nЧасть настроек, которые часто меняются, мы хранили в файле в формате serialize. Мы обнаружили, что это загружает как процессор, так и оперативную память, причём у нас получилось, что на PHP версии 5.3.х у функции unserialize очень сильная утечка памяти. \r\n \r\nПосле оптимизации мы максимально избавились от этих функций где возможно. Данные в файлах мы решили хранить в виде массива, сохранённого через var_export и загружать обратно через с помощью include/require. Таким образом мы смогли задействовать APC режим. \r\n \r\nК сожалению, для данных которые хранятся в memcache, данный подход не работает. \r\n \r\n \r\nВсе данные примеры легко находятся и очень легко правятся не сильно меняя структуру проекта. Мы взяли правило: «любые файлы, которые требуют более 100кб, надо проверять на предмет можно ли оптимизировать их загрузку. Так же мы смотрели на ситуации, когда загружалось несколько файлов из одной ветки. В этой ситуации мы смотрели, можно ли вообще не загружать всю ветку файлов. Ключевая мысль была такой: «всё, что мы загружаем, должно иметь смысл. Если можно не загружать каким-либо способом – лучше не загружать. \r\n \r\n \r\nПосле того, как мы убрали всё, что выше, один запрос к серверу стал требовать примерно в 2 раза меньше оперативной памяти. Нам это дало уменьшение нагрузки процессора примерно в 2 раза без перенастройки серверов и уменьшение средней скорости загрузки одной страницы в несколько раз. \r\n \r\n \r\nЕсли будет интересно, есть в планах идея описать способы профилирования кода и поиска узких мест с помощью XHprof.\n       \n    ", "hub": "Серверная оптимизация / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 21:57", "article": "\n      Хочу поговорить об устройстве управления памятью (Memory Management Unit, MMU). Как вы, разумеется, знаете, основной функцией MMU является аппаратная поддержка виртуальной памяти. Словарь по кибернетике под редакцией академика Глушкова говорит нам, что виртуальная память — это воображаемая память, выделяемая операционной системой для размещения пользовательской программы, ее рабочих полей и информационных массивов. \r\n \r\nУ систем с виртуальной памятью четыре основных свойства: \r\n \r\nВыгода от всех вышеперечисленных пунктов очевидна: миллионы   прикладных программистов, тысячи разработчиков операционных систем и несчетное число эмбеддеров благодарны виртуальной памяти за то, что все они до сих пор при деле. \r\n \r\nК сожалению, по какой-то причине все вышеперечисленные товарищи недостаточно почтительно относятся к MMU, а их знакомство с виртуальной памятью обычно начинается и заканчивается изучением страничной организации памяти и буфера ассоциативной трансляции (Translation Lookaside Buffer, TLB). Самое интересное при этом остается за кадром. \r\n \r\nНе хочу повторять Википедию, поэтому если вы забыли даже про то, что такое страничная память, то самое время перейти по  . Про TLB будет пара строк ниже. \r\n \r\n \r\nА теперь перейдем к делу. Вот так выглядит процессор без поддержки виртуальной памяти: \r\n \r\nВсе адреса, используемые в программе для такого процессора — реальные, «физические», т.е. программист, линкуя программу, должен знать, по каким адресам находится оперативная память. Если у вас припаяно 640 кБ оперативки, отображаемой в адреса 0x02300000-0x0239FFFF, то все адреса в вашей программе должны попадать в эту область. \r\n \r\nЕсли программист хочет думать, что у него всегда четыре гигабайта памяти (разумеется, речь идет о 32-битном процессоре), а его программа — единственное, что отвлекает процессор ото сна, то нам потребуется виртуальная память. Чтобы добавить поддержку виртуальной памяти, достаточно между процессором и оперативной памятью разместить MMU, которое будет транслировать виртуальные адреса (адреса, используемые в программе) в физические (адреса, попадающие на вход микросхем памяти): \r\n \r\nТакое расположение очень удобно — MMU используется только тогда, когда процессор обращается к памяти (например, при промахе кэша), а все остальное время не используется и экономит электроэнергию. Кроме того, в этом случае MMU почти не влияет на быстродействие процессора. \r\n \r\nВот что происходит внутри MMU: \r\n \r\nВыглядит этот процесс так: \r\n \r\n \r\n \r\n \r\nРассмотрим работу TLB на простом примере. Допустим, у нас есть два процесса А и Б. Каждый из них существует в своем собственном адресном пространстве и ему доступны все адреса от нуля до 0xFFFFFFFF. Адресное пространство каждого процесса разбито на страницы по 256 байт (это число я взял с потолка — обычно размер страницы не меньше одного килобайта), т.е. адрес первой страницы каждого процесса равен нулю, второй — 0x100, третьей — 0x200 и так далее вплоть до последней страницы по адресу 0xFFFFFF00. Разумеется, процесс не обязательно должен занимать все доступное ему пространство. В нашем случае Процесс А занимает всего две страницы, а Процесс Б — три. Причем одна из страниц общая для обоих процессов. \r\n \r\nТакже у нас есть 1536 байт физической памяти, разбитой на шесть страниц по 256 байт (размер страниц виртуальной и физической памяти всегда одинаков), причем память эта отображается в физическое адресное пространство процессора с адреса 0x40000000 (ну вот так ее припаяли к процессору). \r\n \r\n \r\n \r\nВ нашем случае первая страница Процесса А расположена в физической памяти с адреса 0x40000500. Не будем вдаваться в подробности того, как эта страница туда попадает — достаточно знать, что ее загружает операционная система. Она же добавляет запись в таблицу страниц (но не в TLB), связывая эту физическую страницу с соответствующей ей виртуальной, и передает управление процессу. Первая же команда, выполненная Процессом А, вызовет промах TLB, в результате обработки которого в TLB будет добавлена новая запись. \r\nКогда Процессу А потребуется доступ ко второй его странице, операционная система загрузит ее в какое-нибудь свободное место в физической памяти (пусть это будет 0x40000200). Случится еще один промах TLB, и нужная запись снова будет добавлена в TLB. Если места в TLB нет — будет перезаписана одна из более ранних записей. \r\n \r\nПосле этого операционная система может приостановить Процесс А и запустить Процесс Б. Его первую страницу она загрузит по физическому адресу 0x40000000. Однако, в отличие от Процесса А, первая команда Процесса Б уже не вызовет промах TLB, так как запись для нулевого виртуального адреса в TLB уже есть. В результате Процесс Б начнет выполнять код Процесса А! Что интересно, именно так и работал широко известный в узких кругах процессор ARM9. \r\n \r\nСамый простой способ решить эту проблему — инвалидировать TLB при переключении контекста, то есть отмечать все записи в TLB как недействительные. Это не самая хорошая идея, так как: \r\n \r\nСпособ посложнее — линковать все программы так, чтобы они использовали разные части виртуального адресного пространства процессора. Например, Процесс А может занимать младшую часть (0x0-0x7FFFFFFF), а Процесс Б — старшую (0x80000000-0xFFFFFFFF). Очевидно, что в этом случае ни о какой изоляции процессов друг от друга речи уже не идет, однако этот способ действительно иногда применяют во встроенных системах. Для систем общего назначения по понятным причинам он не подходит. \r\n \r\nТретий способ — это развитие второго. Вместо того, чтобы делить четыре гигабайта виртуального адресного пространства процессора между несколькими процессами, почему бы просто не увеличить его? Скажем, в 256 раз? А чтобы обеспечить изоляцию процессов, сделать так, чтобы каждому процессу по-прежнему было доступно ровно четыре гигабайта оперативной памяти? \r\nСделать это оказалось очень просто. Виртуальный адрес расширили до 40 бит, при этом старшие восемь бит уникальны для каждого процесса и записаны в специальном регистре — идентификаторе процесса (PID). При переключении контекста операционная система перезаписывает PID новым значением (сам процесс свой PID поменять не может). \r\nЕсли для нашего Процесса А PID равен единице, а для Процесса Б — двойке, то одинаковые с точки зрения процессов виртуальные адреса, например 0x00000100, с точки зрения процессора оказываются разными — 0x0100000100 и 0x0200000100 соответственно. \r\nОчевидно, что в нашем TLB теперь должны находиться не 24-битные, а 32-битные номера виртуальных страниц. Для удобства восприятия старшие восемь бит хранятся в отдельном поле — идентификаторе адресного пространства (Address Space IDentifier — ASID). \r\n \r\nТеперь, когда процессор подает на вход MMU виртуальный адрес, поиск в TLB производится по комбинации VPN и ASID, поэтому первая команда Процесса Б вызовет страничную ошибку даже без предшествующей инвалидации TLB. \r\n \r\n \r\n \r\nВ современных процессорах ASID чаще всего или восьмибитный, или 16-битный. Например, во всех процессорах ARM с MMU, начиная с ARM11, ASID восьмибитный, а в архитектуре ARMv8 добавлена поддержка 16-битного ASID. \r\n \r\nКстати, если процессор поддерживает виртуализацию, то помимо ASID у него может быть еще и VSID (Virtual address Space IDentificator), который еще больше расширяет виртуальное адресное пространство процессора и содержит номер запущенной на нем виртуальной машины. \r\n \r\nДаже после добавления ASID могут возникнуть ситуации, когда нужно будет инвалидировать одну или несколько записей или даже весь TLB: \r\n \r\n \r\nНа этом можно было бы завершать рассказ об MMU, если бы не один нюанс. Дело в том, что между процессором и оперативной памятью помимо MMU находится еще и кэш-память. \r\n \r\nТе, кто забыл, что такое кэш-память и как она работает, могут освежить знания   и  .  \r\n \r\nДля примера возьмем двухканальный (2-way) кэш размером один килобайт с размером строки кэша (или линии кэша — как вам угодно) в 64 байта. Сейчас не важно, кэш ли это команд, кэш данных или объединенный кэш. Поскольку размер страницы памяти у нас 256 байт, то в каждой странице помещается четыре строки кэша. \r\n \r\nПоскольку кэш находится между процессором и MMU, то очевидно, что и для индексации, и для сравнения тэгов используются исключительно виртуальные адреса (физические адреса появляются только на выходе MMU). По-английски такой кэш называется Virtually Indexed, Virtually Tagged cache (VIVT). \r\n \r\n \r\nВ чем же, собственно, проблема? Самое время рассмотреть еще один пример. Возьмем все те же два процесса А и Б: \r\n \r\n \r\nПодождите, скажете вы, мы же только что решили эту проблему, добавив ASID в TLB? А кэш-то у нас стоит до MMU — когда промаха кэша нет, то мы в MMU и не смотрим, и какой там ASID — знать не знаем. \r\n \r\n \r\n \r\nЭто первая проблема c VIVT-кэшем — так называемые омонимы (homonyms), когда один и тот же виртуальный адрес может отображаться на разные физические адреса (в нашем случае виртуальный адрес 0x00000000 отображается на физический адрес 0x40000500 для Процесса А и 0x40000000 для Процесса Б). \r\n \r\nВариантов решения проблемы омонимов множество: \r\n \r\nКакой вариант вам нравится? Я бы, наверное, думал насчет второго. В ARM9 и некоторых других процессорах с VIVT-кэшами реализован третий. \r\n \r\n \r\nЕсли бы омонимы в кэше были бы единственной проблемой, то разработчики процессоров были бы самыми счастливыми людьми на свете. Существует и вторая проблема — синонимы (synonyms, aliases), когда несколько виртуальных адресов отображаются на один и тот же физический адрес. \r\nВернемся к нашим   процессам А и Б. Предположим, что проблему омонимов мы решили каким-нибудь приличным способом, потому что каждый раз флашить кэш — это реально очень дорого! \r\nИтак: \r\n \r\n \r\n \r\n \r\nВ результате один и тот же кусок физической памяти находится в кэше в двух разных местах. Теперь если оба процесса изменят свою копию, а потом захотят сохранить ее в память, то один из них ждет сюрприз. \r\nОчевидно, что эта проблема актуальна только для кэша данных либо для объединенного кэша (повторюсь, кэш команд доступен только для чтения), но решить ее гораздо сложнее, чем проблему омонимов: \r\n \r\n \r\n \r\nНо и это еще не все! Мы живем в эпоху многоядерных процессоров с собственными L1-кэшами, которые так и норовят стать некогерентными. Для борьбы с некогерентностью кэшей были придуманы  . Однако вот незадача: протоколы следят за внешней шиной, а адреса на ней какие? Физические! А в кэше используются виртуальные.  \r\n \r\nКак тогда узнать, какую строку кэша надо срочно записать в память, потому что ее ждет соседний процессор?   Пожалуй, это тема для отдельной статьи. \r\n \r\n \r\nУ VIVT-кэша есть единственный существенный плюс: если физическая страница выгружается из памяти, то соответствующие ей строки кэша нужно флашить, но не нужно инвалидировать. Даже если эта физическая страница через какое-то время будет загружена в другое место, на содержимом кэша это не отразится, потому что при доступе к VIVT-кэшу физические адреса не используются и процессору все равно, изменились они или нет. \r\n \r\nЛет дцать назад плюсов было больше. VIVT-кэши активно применялись в те времена, когда использовалось внешнее MMU в виде отдельной микросхемы. Доступ к такому MMU занимал гораздо больше времени, чем к MMU, расположенному на том же кристалле, что и процессор. Но поскольку обращение к MMU требовалось только в случае промаха кэша, то это позволяло обеспечивать приемлемую производительность системы. \r\n \r\n \r\nИтак, у VIVT-кэшей слишком много недостатков и мало преимуществ (кстати, если я забыл какие-то еще грехи — пишите в комменты). Умные люди подумали и решили: перенесем-ка мы кэш-память за MMU. Пусть и индексы, и тэги получаются из физических, а не из виртуальных адресов. И перенесли. А сработало ли это — узнаете в следующей части.\n\n      \n       \n    ", "hub": "Системное программирование / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 15:52", "article": "\n       \r\n \r\nВ   мы разработали приложение для работы с github, состоящее из двух экранов, разделенное по слоям с применением паттерна MVP. Мы использовали RxJava для упрощения взаимодействия с сервером и две модели данных для разных слоев. Во второй части мы внедрим Dagger 2, напишем unit тесты, посмотрим на MockWebServer, JaCoCo и Robolectric.  \r\n \r\nСодержание: \r\n \r\n \r\n \r\nВ первой части статьи мы в два этапа создали простое приложение для работы с github.  \r\n \r\n \r\n \r\nВсе исходники вы можете найти на  . Ветки в репозитории соответствуют шагам в статье:   — третий шаг,   — четвертый шаг.  \r\n \r\n \r\nПеред тем, как использовать Dagger 2, необходимо понять принцип  . \r\n \r\nПредставим, что то у нас есть объект A, который включает объект B. Без использования DI мы должны создавать объект B в коде класса A. Например так: \r\n \r\n \r\nТакой код сразу же нарушает   и   из принципов  . Самым простым решением является передача объекта B в конструктор класса A, тем самым мы реализуем Dependency Injection “вручную”: \r\n \r\n \r\nОбычно DI реализуется с помощью сторонних библиотек, где благодаря аннотациям, происходит автоматическая подстановка объекта. \r\n \r\n \r\nПодробнее об этом механизме и его применении на Android можно прочитать в этой статье:  \r\n \r\n \r\nDagger 2 — библиотека созданная Google для реализации DI. Ее основное преимущество в кодогенерации, т.е. все ошибки будут видны на этапе компиляции. На хабре есть  , также можно почитать   или  \r\n \r\nДля установки Dagger 2 необходимо отредактировать build.gradle: \r\n \r\n \r\nТакже очень рекомендуется поставить плагин  . Он поможет ориентироваться откуда и куда происходят инжекции.  \r\n \r\n \r\nСами объекты для внедрения Dagger 2 берет из методов модулей (методы должны помечаться аннотацией   , модули —   ) или создает их с помощью конструктора класса аннотированного   . Например: \r\n \r\n \r\nили  \r\n \r\n \r\nПоля для внедрения обозначаются аннотацией   : \r\n \r\n \r\nСвязываются эти две вещи с помощью компонентов (@Component). В них указывается откуда брать объекты и куда их внедрять (методы inject). Пример:  \r\n \r\n \r\nДля работы Dagger 2 мы будем использовать один компонент (AppComponent) и 3 модуля для разных слоев (Model, Presentation, View). \r\n \r\n \r\n \r\nДля Model — слоя необходимо необходимо предоставлять ApiInterface и два Scheduler для управления потоками. Для Scheduler необходимо использовать аннотацию   , чтобы Dagger разобрался с графом зависимостей.  \r\n \r\n \r\n \r\nДля presenter слоя нам необходимо предоставлять Model и CompositeSubscription, а также мапперы. Model и CompositeSubscription будем предоставлять через модули, мапперы — с помощью аннотированного конструктора.  \r\n \r\n \r\n \r\n \r\nСо View слоем и внедрением презентеров ситуация сложнее. При создании презентера мы в конструкторе передаем интерфейс View. Соответственно, Dagger должен иметь ссылку на реализацию этого интерфейса, т.е на наш фрагмент. Можно пойти и другим путем, изменив интерфейс презентера и передавая ссылку на view в onCreate. Рассмотрим оба случая. \r\n \r\nПередача ссылки на view. \r\n \r\nУ нас есть фрагмент RepoListFragment, реализующий интерфейс RepoListView, \r\nи RepoListPresenter, принимающий на вход в конструкторе этот RepoListView. Нам необходимо внедрить RepoListPresenter в RepoListFragment. Для реализации такой схемы нам придется создать новый компонент и новый модуль, который в конструкторе будет принимать ссылку на наш интерфейс RepoListView. В этом модуле мы будем создавать презентер (с использованием ссылки на интрефейс RepoListView) и внедрять его в фрагмент.  \r\n \r\n \r\n \r\n \r\nВ реальных приложениях у вас будет множество инжекций и модулей, поэтому создание различных компонентов для различных сущностей — отличная идея для предотвращения создания  . \r\n \r\nИзменение кода презентера. \r\n \r\nПриведенный выше метод требует создания нескольких файлов и множества действий. В нашем случае, есть гораздо более простой способ, изменим конструктор и будем передавать ссылку на интерфейс в onCreate. \r\nКод: \r\n \r\n \r\n \r\nЗавершив внедрение Dagger 2, перейдем к тестированию приложения. \r\n \r\n \r\nТестирование давно стало неотъемлемой частью процесса разработки ПО.  \r\n , в первую очередь разберемся с модульным (unit) тестированием. \r\n \r\n  процесс в программировании, позволяющий проверить на корректность отдельные модули исходного кода программы. \r\nИдея состоит в том, чтобы писать тесты для каждого нетривиального метода. Это позволяет достаточно быстро проверить, не привело ли очередное изменение кода к регрессии, то есть к появлению ошибок в уже оттестированных местах программы, а также облегчает обнаружение и устранение таких ошибок. \r\n \r\nНаписать полностью изолированные тесты у нас не получится, потому что все компоненты взаимодействуют друг с другом. Под unit тестами, мы будем понимать проверку работы одного модуля окруженного моками. Взаимодействие нескольких реальных модулей будем проверять в интеграционных тестах.  \r\n \r\nСхема взаимодействия модулей: \r\n \r\n \r\n \r\nПример тестирования маппера (серые модули — не используются, зеленые — моки, синий — тестируемый модуль): \r\n \r\n \r\n \r\n Инфраструктура \r\n \r\nИнструменты и фреймворки повышают удобство написания и поддержки тестов. CI сервер, который не даст вам сделать merge при красных тестах, резко уменьшает шансы неожиданной поломки тестов в master branch. Автоматический запуск тестов и ночные сборки помогают выявить проблемы на самом раннем этапе. Этот принцип получил название  . \r\nПро тестовое окружение вы можете почитать в статье  . В дальнейшем мы будем использовать   для написания тестов,   для создания моков и   для проверки покрытия кода тестами. \r\n \r\nПаттерн MVP позволяет быстро и эффективно писать тесты на наш код. С помощью Dagger 2 мы сможем подменить настоящие объекты на тестовые моки, изолировав код от внешнего мира. Для этого используем тестовый компонент с тестовыми модулями. Подмена компонента происходит в тестовом application, который мы задаем с помощью аннотации   (application = TestApplication.class) в базовом тестовом классе.  \r\n \r\n JaCoCo Code Coverage  \r\n \r\nПеред началом работы, нужно определить какие методы тестировать и как считать процент покрытия тестами. Для этого используем библиотеку JaCoCo, которая генерирует отчеты по результатам выполнения тестов. \r\nСовременная Android Studio   или можно настроить его, добавив в build.gradle следующие строки: \r\n \r\n \r\nОбратите внимание на исключенные классы: мы удалили все что связано с Dagger 2 и нашими моделями DTO и VO.  \r\n \r\nЗапустим jacoco (gradlew jacocoTestReport) и посмотрим на результаты: \r\n \r\n \r\n \r\nСейчас у нас процент покрытия идеально совпадает с нашим количеством тестов, т.е 0% =) Давайте исправим эту ситуацию! \r\n \r\n \r\nВ model слое нам необходимо проверить правильность настройки retrofit (ApiInterface), корректность создания клиента и работу ModelImpl.  \r\nКомпоненты должны проверяться изолированно, поэтому для проверки нам нужно эмулировать сервер, в этом нам поможет  . Настраиваем ответы сервера и проверяем запросы retrofit. \r\n \r\n \r\n \r\n \r\nДля проверки модели мокаем ApiInterface и проверяем корректность работы.  \r\n \r\n \r\nПроверим покрытие в Jacoco: \r\n \r\n \r\n \r\n \r\nВ presenter слое нам необходимо протестировать работу мапперов и работу презентеров. \r\n \r\n \r\nС мапперами все достаточно просто. Считываем json из файлов, преобразуем и проверяем. \r\nС презентерами — мокаем model и проверяем вызовы необходимых методов у view. Также необходимо проверить корректность onSubscribe и onStop, для этого перехватываем подписку (Subscription) и проверяем isUnsubscribed \r\n \r\n \r\nСмотрим изменение в JaCoCo: \r\n \r\n \r\n \r\n \r\nПри тестирование View слоя, нам необходимо проверить только вызовы методов жизненного цикла презентера из фрагмента. Вся логика содержится в презентерах. \r\n \r\n \r\n \r\nФинальное покрытие тестами: \r\n \r\n \r\n \r\n \r\nВо второй части статьи мы рассмотрели внедрение Dagger 2 и покрыли код unit тестами. Благодаря использованию MVP и подмене инжекций мы смогли быстро написать тесты на все части приложения.  . Статья написана при активном участии   . В следующей части рассмотрим интеграционное и функциональное тестирование, а также поговорим про TDD.\n\n      \n       \n    ", "hub": "Разработка под Android / Интересные публикации / Хабрахабр", "title": "Построение Android приложений шаг за шагом, часть вторая"}
{"hub": "Разработка под Android / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 14:24", "article": "\n       \r\n \r\nНа этой неделе состоялся долгожданный релиз Kotlin 1.0, с чем я поздравляю всех причастных! Мы с командой Android-разработчиков Avito.ru решили, что это отличный повод встретиться и познакомиться с коллегами, программирующими на Kotlin, обсудить перспективы языка, обменяться накопленным опытом в неформальной обстановке, поесть пиццу, в общем, с удовольствием и пользой провести день субботы. Для этого мы организуем 27 февраля митап  , присоединяйтесь к нам! \r\n \r\nВ программе встречи у нас специальный гость, представитель команды JetBrains Дмитрий Жемеров, который расскажет о том, что предлагает Kotlin 1.0 Android-разработчикам уже сегодня, какие возможности появятся в ближайшем будущем. Команда Avito.ru давно использует сочетание Kotlin и Rx, мы уже выпустили в продакшн два приложения, где нет ни одной строки на Java. С удовольствием поделимся своим опытом и подходами. Доклад нашего третьего спикера, Владимира Миронова, будет посвящён delegated properties, теме, которая волнует тех, кто уже успел погрузиться в разработку на Kotlin.   и приходите на встречу, приглашайте коллег и друзей! \r\n \r\nПод катом подробнее о спикерах, программе и формате мероприятия. \r\n \r\n \r\n \r\nСначала о самом главном, спикерах и докладах, которые будут представлены на встрече.  \r\n \r\nТем, кто следит за развитием Kotlin, не нужно представлять Дмитрия Жемерова, Principal Engineer в команде JetBrains, одного из первых разработчиков языка, руководителя команды, разрабатывающей плагин для IntelliJ IDEA, соавтора книги  . На встрече Дмитрий расскажет о новых возможностях, которые предоставляет Kotlin Android-разработчикам по состоянию на версию 1.0, а также о том, что можно ожидать в следующих версиях (инкрементальная компиляция в Gradle, поддержка Lint-проверок, новые возможности в IDE, уменьшение размера runtime и так далее). Чтобы у всех была возможность задать все накопившиеся вопросы, мы организуем отдельную секцию Q&A.  \r\n  \r\nВладимир Миронов занимается Android-разработкой более четырёх лет, за это время успел поработать над несколькими проектами, в том числе в компании Одноклассники. Последний год Владимир работает в небольшом стартапе и пишет весь код исключительно на Kotlin. Его доклад будет посвящён delegated properties, теме, которая вызывает много вопросов. Узнаем, зачем они нужны, какие delegated properties предоставляет стандартная библиотека, напишем несколько примеров, посмотрим, во что превращаются delegated properties во время компиляции. \r\n \r\nЯ занимаюсь разработкой профессиональных инструментов для пользователей Avito.ru. Недавно мы выпустили в продакшн два приложения, написанных полностью на Kotlin, с архитектурой построенной вокруг Reactive Extensions. Я расскажу про те средства языка, которые позволяют писать на Rx чисто и надежно. Познакомлю с Rx библиотеками, где уже во всю используется мощь Kotlin, затрону вопросы, касающиеся тестирования.  \r\n \r\nКроме всего вышеперечисленного, у нас будет время, чтобы пообщаться в неформальной обстановке, перекусив пиццей.  \r\n \r\nВстреча будет проходить в московском офисе Avito.ru (Лесная, 7) 27 февраля, стартуем в 12:00. К сожалению, количество мест в зале ограничено, а в БЦ действует пропускная система, поэтому необходимо   и получить подтверждение, а в день мероприятия захватить с собой паспорт или водительское удостоверение.  \r\n \r\nЕсли у вас появятся вопросы, лучше их задать их через форму на странице регистрации на  .  \r\n \r\nЖдём вас на митапе!\n\n      \n       \n    ", "hub": "Разработка под Android / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 14:46", "article": "\n       \r\nПриложение Gmail предоставляет возможность унифицировать свои почтовые ящики с разных служб для лучшей организации писем и использования других полезных, ранее исключительно «внутригмейловских», функций. \r\n \r\n \r\n \r\nС прошлого года, приложение Gmail под Android позволяло вам получать доступ к ящикам других почтовых служб, например, Yahoo! Mail или Outlook.com. Команда Gmail получила множество положительных отзывов от тех, кто воспользовался этой функцией. Много клиентов хотели бы использовать и другие полезные фичи Gmail-а, такие как защита от спама и структурирование входной почты, в своих учетных записях, но при этом очень не хотели бы напрягаться со сменой своего почтового ящика. Это было учтено. Начиная с 17 февраля, при использовании Yahoo! Mail или Hotmail/Outlook.com появляется возможность «гмейлифицировать» ваши разделы входящих писем. Эта функция присоединяет вашу существующую учетную запись с другой почтовой службы к приложению Gmail таким образом, что все плюшки — защита от спама, организация входной почты и даже карточки Google Now, связанные с вашей почтой — работают без необходимости смены своего привычного почтового адреса. Все, что нужно сделать — это запустить приложение Gmail, войти в вашу учетную запись и активировать функцию «Gmailify». И конечно же, вы всегда имеете право передумать и в любой момент отвязать вашу учетку, продолжая пользоваться приложением Gmail без «гмейлификации» сторонних почтовых ящиков. \r\n \r\n \r\n \r\nGmail очень рад возможности предоставить свои интересные функции для более широкой аудитории, и в будущем планирует усовершенствовать функцию Gmailify для использования с другими почтовыми службами.\n       \n    ", "hub": "Разработка под Android / Интересные публикации / Хабрахабр", "title": "Gmailify: лучшие из функций Gmail теперь доступны без почтового ящика @gmail"}
{"date": "18 февраля в 07:03", "article": "\n       Это пост о сложностях взаимодействия искусственного и естественного интеллекта. Современные компиляторы имеют довольно развитые средства статического анализа, которые умеют выдавать предупреждения на разные подозрительные конструкции в коде. В теории эти предупреждения должны помочь разработчикам делать меньше ошибок в коде. \r\n \r\nНа практике далеко не всегда предупреждения компилятора одинаково полезны. Зачастую они не помогают разработчикам, а мешают им и могут провоцировать на исправление совершенно правильного кода, т.е. на нарушение правила «работает — не трогай». \r\n \r\nДля примера будет история об одной строке кода из довольно популярной библиотеки с открытым кодом для работы с XML. Контекст этой строки такой: \r\n \r\nЭтот фрагмент пытается прочитать файл на диске целиком. Для этого переводит файловый указатель на конец файла и получает текущую позицию в файле. Дальше делается попытка выделить память под данные всего файла. Перед выделением памяти стоит проверить, поместится ли требуемый размер в size_t, чтобы избежать труднодиагностируемых логических ошибок при дальшейшей работе. \r\n \r\nВыполняется (по сути, а в коде стоит противоположная и возврат кода ошибки) проверка, что filelength строго меньше, чем максимальное значение size_t. Проверка выполняется на «строго меньше», потому что нужно выделить памяти на один элемент больше, чтобы записать завершающий нулевой символ перед дальнейшим разбором содержимого файла. \r\n \r\nВсе бы хорошо, но компилятор негодуэ — ведь с одной стороны сравнения знаковый тип long, а с другой стороны — беззнаковый size_t. ПРЕДУПРЕЖДЕНИЕ -Wsign-compare от gcc!!! \r\n \r\nЧто же делать, куда бежать??? Это же предупреждение!!! \r\n \r\nОдин из разработчиков исправляет проверку: \r\n \r\n… и компилятор   доволен. Успех? Увы, изначально правильный код теперь работает уже не так, как было задумано. Размеры long и size_t не связаны друг с другом. Размер любого из этих типов может быть больше размера второго на конкретной платформе. Первоначальный код требовал в таком случае преобразования обоих значений к одному и тому же типу — достаточного для обоих размера. В «исправленном» коде произойдет потеря части значащих разрядов в случае, если размер long больше размера size_t. \r\n \r\nТак для избавления от предупреждения в изначально правильном коде этот код исправили и сломали. \r\n \r\nОшибка могла бы остаться надолго и еще многие годы радовать пользователей при работе с очень большими файлами на некотором подмножестве систем. Не осталась — код библиотеки открыт и даже лежит на Github, вскоре приходит пул-запрос с правкой этого кода. В исправленном коде проверка выглядит так: \r\n \r\nВ исправленном коде сравниваемые значения оба беззнаковых типов, поэтому ужасная проблема(тм), о которой gcc сообщал предупреждением -Wsign-compare, здесь отсутствует. Приведение long к unsigned long здесь не приводит к неверной интерпретации данных, потому что единственно возможное отрицательное значение обработано ранее. \r\n \r\nУспех? Нет, не так быстро. \r\n \r\nПравку вливают в основную ветвь, после чего начинают поступать сообщения о новом предупреждении. При компиляции под платформы, где размер unsigned long меньше размера size_t, clang выдает предупреждение -Wtautological-constant-out-of-range-compare «ПРЕДУПРЕЖДЕНИЕ!!! Диапазон значений unsigned long так   безнадежно ограничен, что сравнение всегда дает один и тот же результат и не имеет решительно никакого смысла». \r\n \r\nПолностью переносимый код для любой платформы, говорите? Нет, когда явное приведение long к size_t могло повлечь потерю разрядов, это всех устраивало. А теперь clang выдает бесполезное предупреждение и пользователи библиотеки оставляют комментарии к правке и сообщения о дефектах. Это же предупреждение!!! \r\n \r\nЧто же делать… ведь нужно сделать так, чтобы на платформах, где размер unsigned long меньше размера size_t, сравнения не было, а на остальных платформах — было. \r\n \r\nА, вот решение «проблемы»: \r\n \r\n — Что это, Бэрримор??? \r\n — Это шаблон Баскервилей, сэр!!! \r\n \r\nЗдесь можно было бы применить шаблон функции, но значения параметров шаблона по умолчанию для шаблонов функций поддерживаются только начиная с C++11, а библиотека — чтобы расширить аудиторию — старается без него обходиться. struct вместо class используется, чтобы не указывать видимость public явно. \r\n \r\nПрименяется так: \r\n \r\nВ зависимости от соотношения между размерами long и size_t компилятор выбирает либо специализацию, либо реализацию по умочанию. В первом случае бессмысленной проверки не будет, и компилятору не о чем будет ПРЕДУПРЕЖДАТЬ. \r\n \r\nВ основную ветвь вливают проверку с использованием этого шаблона — и «проблема» решена. \r\n \r\nКраткое содержание предыдущего текста… Совершенно правильный код с одним сравнением целочисленной переменной и целочисленной константы правили трижды, чтобы сделать счастливыми два очень популярных компилятора. Первая же из правок этот совершенно правильный код сломала, зато сделала счастливым компилятор, ради которого вносили правку. В итоге вместо простого сравнения получили что-то ближе к  . Зато нет предупреждений. \r\n \r\nТеперь давайте порассуждаем… \r\n \r\nНасколько популярным должен быть сферический кусок открытого кода в вакууме, чтобы он имел шансы пройти весь путь от первой правки до последней, и насколько более вероятно, что менее популярный код так и остался бы сломанным первой правкой? Понадобилось в общей сложности явно больше одного часа работы разработчиков с не самой низкой квалификацией, чтобы найти показанное выше решение, которое даже не исправляет ошибку, а превращает просто правильный код в правильный код без предупреждений. \r\n \r\nПочему так произошло? Причин ровно две. \r\n \r\nПервая причина — разработчики почти безоговорочно доверяют компилятору. Рядом с такой сложной программой многие разработчики ощущают себя как рядом со   на проспекте Мира в Москве. Современный компилятор — это очень сложная программа, и многие считают, что она не может ошибаться. Увы, это мнение неверно — чем программа сложнее, тем больше у нее возможностей ошибиться. Компилятор может выдать предупреждение на совершенно правильный код. \r\n \r\nВторая причина — разработчики статических анализаторов (особенно входящих в состав компиляторов) обычно считают, что ложное срабатывание — это не проблема и ничего страшного, главное ПРЕДУПРЕДИТЬ, а «разработчик должен разобраться». Разберется непременно, но далеко не факт, что в результате код станет лучше. \r\n \r\nВ обсуждениях этой проблемы проскакивают идеи, что компилятор никак не может разобраться, есть ли в коде проблема, и только «разработчик должен». \r\nДействительно ли компилятор так безнадежно беспомощен и не может «понять» код? Самое время вспомнить об оптимизациях кода. Нетривиальные оптимизации кода — автоматическая векторизация и размотка циклов, вычисление инвариантов, удаление лишних проверок и другие полезные и умные преобразования — требуют от компилятора очень вдумчивого анализа кода. \r\n \r\nНасколько вдумчивый анализ нужен в приведенном примере? Проверяемое значение получено от выполнения функции ftell(), ее поведение задокументировано. gcc, например, уже умеет оптимизировать — и «доламывать» не соответствующий Стандарту — код, используя требования Стандарта передавать в «строковые функции» — например, memcpy() — только ненулевые указатели, подробности есть  . Как он это делает? Разработчики gcc добавили в него такую возможность — опознавать ряд функций как «строковые» и делать некоторые предположения о значениях передаваемых в эти функции указателей. \r\n \r\nТочно так же разработчики компилятора могут добавить в него возможность использовать данные о поведении других функций стандартной библиотеки — вряд ли тривиально, но определенно выполнимо. Сторонние статические анализаторы также «знают» о функциях стандартной библиотеки и популярных сред, это позволяет им сообщать, например, о возможной логической ошибке в приведенном выше коде в случае, когда результат ftell() не проверяется на сообщающее об ошибке значение -1L. \r\n \r\nТехническая возможность для значительных улучшений есть. Это вопрос приоритетов и отношения к проблеме. \r\n \r\nМожно «научить» компилятор оптимизировать и «доламывать» не соответствующий Стандарту код с вызовом строковых функций, а можно «научить» его выдавать меньше бесполезных предупреждений. \r\n \r\nЧто на выходе? Многие средства статического анализа пока работают по принципам «главное предупредить» и «разработчик должен», этим они провоцируют разработчиков вносить в код ненужные правки, попутно иногда ломать работающий код. Можно, конечно, объявить этих разработчиков недостойными и криворукими, но не стоит забывать, что от их работы зависит судьба миллионов строк кода, используемых каждый день сотнями миллионов человек. \r\n \r\nСтатический анализ был задуман как средство содействия написанию правильного кода — для достижения этой цели средства статического анализа должны помогать разработчикам, а не мешать им бесполезными сообщениями. \r\n \r\nКаждый раз как средство статического анализа выдает предупреждение на правильный код, где-то грустит один котик(тм) и, возможно, в совершенно правильный код вносится ненужная правка, провоцирующая очередную ошибку с последствиями как у Heartbleed. \r\n \r\n \n\n      \n       \n    ", "hub": "Разработка / Интересные публикации / Хабрахабр", "title": "Как непродуманные предупреждения компиляторов помогают портить совершенно правильный код"}
{"date": "18 февраля в 17:33", "article": "\n      Вчера, 17 февраля, команда разработчиков Github   новый функционал, которого пользователям, участвующим в групповой разработке, могло серьезно недоставать: теперь в GitHub есть шаблоны для Issue и Pull-реквестов.  \r\n \r\n \r\n \r\nПо словам   \"это первое из многих улучшений Issues и Pull-реквестов, сделанных благодаря фидбеку наших пользователей\".  \r\n \r\nДля добавления шаблона Issue в репозиторий достаточно создать файл с именем   в корневом каталоге. Расширение файла роли не играет, но поддерживаются Markdown-файлы (.md). Это существенный плюс, так как md-разметка позволяет быстро и легко форматировать текст, добавлять ссылки, заголовки и  .  \r\n \r\nШаблон для Pull-реквеста добавляется по тому же принципу: создание файла с именем   в корневой директории вашего репозитория.  \r\n \r\nДля тех, кто опасается бардака в корневом каталоге была добавлена поддержка папки  . В ней можно с чистой совестью разместить файлы  ,   и   и все будет работать в лучшем виде. Если не работает, то можно почитать  . \r\n\n\n      \n       \n    ", "hub": "Разработка / Интересные публикации / Хабрахабр", "title": "GitHub добавил поддержку шаблонов для Issue и Pull-реквестов"}
{"date": "18 февраля 2015 в 16:47", "article": "\n      Добрый день. \r\n \r\nЯ занимаюсь IT-образованием.  \r\nВ данный момент готовлю курс   для одной западной MOOC-платформы. Думаю этот детальный план может быть полезен кому-то для подготовки к собеседованию. \r\n \r\nТакже я веду курс   на платформе для онлайн-образования udemy.com (аналог Coursera/EdX). \r\n \r\nПо некоторому размышлению выходит, что за 4-5 часов видео (стандартный формат на платформе) максимум полезного, что можно рассказать в 5-10 минутных кусочках, попадает в Первую часть (6 тем:  ,  ,  ,  ,  ,   + 35 вложенных подтем).  \r\n \r\nЦенное, но не помещающееся, пошло во Вторую часть (3 темы:  ,  ,   + 10 вложенных подтем). \r\n \r\nОжидается, что слушатель после курса будет готов приступить к технологиям «следующего уровня» после JDBC —  ,  ,  . \r\n \r\nЗнаю, что разумное замечание состоит в том, что можно «просто почитать JDBC 4.2 Specification». Да, можно. Этот план и строится как анализ спецификации + предлагаемые целевые варианты использования ( ,  ,  , ...). \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nP.S. Кроме подготовки этого курса для сторонней платформы (готовится переводной вариант на английском, оригинал на русском пока негде выложить) автор с 27 февраля стартует курс из 16 вебинаров   (3 месяца). \r\nКонтакты: \r\nskype: GolovachCourses \r\nemail: GolovachCourses@gmail.com\n\n      \n       \n    ", "hub": "JAVA / Интересные публикации / Хабрахабр", "title": "Учебный план по JDBC"}
{"date": "18 февраля 2015 в 14:29", "article": "\n      В этой статье я расскажу о нашей реализации hot deploy — быстрой доставки изменений Java-кода в работающее приложение.  \r\n \r\nДля начала немного истории. Мы уже несколько лет делаем корпоративные приложения на платформе  . Они очень разные по размеру и функциональности, но все они похожи в одном — в них много пользовательского интерфейса.  \r\n \r\nВ какой-то момент мы поняли, что разрабатывать пользовательский интерфейс, постоянно перезагружая сервер — крайне утомительно. Использование Hot Swap сильно ограничивает (нельзя добавлять и переименовывать поля, методы класса). Каждая перезагрузка сервера отнимала минимум 10 секунд времени, плюс необходимость повторного логина и перехода на тот экран, который ты разрабатываешь.  \r\n \r\nПришлось задуматься о полноценном hot deploy. Под катом — наше решение проблемы с кодом и демо-приложением. \r\n \r\n \r\nРазработка экранов в платформе CUBA предполагает создание декларативного XML-описателя экрана, в котором указывается имя класса-контроллера. Таким образом класс-контроллер экрана всегда получается по полному имени.  \r\n \r\nТакже следует заметить, что в большинстве случаев контроллер экрана является вещью в себе, то есть не используется другими контроллерами или просто классами (такое бывает, но не часто). \r\n \r\nСначала мы пытались использовать Groovy для решения проблемы hot deploy. Мы стали загружать исходный Groovy-код на сервер и получать классы контроллеров экранов через GroovyClassLoader. Это решило проблему со скоростью доставки изменений на сервер, но создало много новых проблем: на тот момент Groovy относительно слабо поддерживался IDE, динамическая типизация позволяла написать некомпилируемый код незаметно для себя, неопытные разработчики регулярно старались написать код как можно безобразнее, просто потому, что Groovy позволяет так делать. \r\n \r\nУчитывая то, что в проектах были сотни экранов, каждый из которых потенциально мог сломаться в любой момент, нам пришлось отказаться от использования Groovy в контроллерах экранов. \r\n \r\nТогда мы крепко задумались. Нам хотелось получить преимущества мгновенной доставки кода на сервер (без перезагрузки) и в то же время не рисковать сильно качеством кода. На помощь пришла фича, появившаяся в Java 1.6 — ToolProvider.getSystemJavaCompiler() ( ). Этот объект позволяет получать объекты типа java.lang.Class из исходного кода. Мы решили попробовать. \r\n \r\n \r\nСвой класслоадер мы решили сделать похожим на GroovyClassLoader. Он кэширует скомпилированные классы и при каждом обращении к классу проверяет, не обновился ли исходный код класса в файловой системе. Если обновился — запускается компиляция и результаты попадают в кэш.  \r\n \r\nДетальную реализацию класслоадера вы можете увидеть, перейдя по  . \r\n \r\nЯ же в статье остановлюсь на ключевых моментах реализации. \r\n \r\nНачнем с главного класса — JavaClassLoader. \r\n \r\n \r\nПри вызове loadClass мы производим следующие действия: \r\n \r\nЕсли обратить внимание на метод  , то можно заметить, что мы обновляем Spring-контекст после каждой загрузки классов. Это было сделано для демонстрационного приложения, в реальном проекте такое частое обновление контекста обычно не требуется. \r\n \r\nУ кого-то может возникнуть вопрос — как мы определяем, от чего зависит класс? Ответ простой — мы разбираем секцию импортов. Далее приведен код, который это делает. \r\n \r\n \r\nВнимательный читатель спросит — а где же сама компиляция? Ниже приведен ее код. \r\n \r\n \r\n \r\nДля данной статьи я написал маленькое  , в котором использовал наш класслоадер. \r\nЭто приложение демонстрирует, как можно получить пользу от динамической компиляции. \r\n \r\nВ приложении объявлен контроллер WelcomeController и Spring-bean SomeBean. Контроллер использует метод SomeBean.get() и отдает результат на уровень представления, где он и отображается. \r\n \r\nСейчас я продемонстрирую, как с помощью нашего класслоадера мы можем поменять реализацию SomeBeanImpl и WelcomeController без остановки приложения. Для начала развернем приложение (для сборки вам понадобится  ) и перейдем на  :8080/mvcclassloader/hello. \r\n \r\nОтвет таков:  \r\n \r\nТеперь давайте слегка поменяем реализацию SomeBeanImpl. \r\n \r\n \r\nПоложим файл на сервер в папку tomcat/conf/com/haulmont/mvcclassloader (папка, в которой класслоадер ищет исходный код настраивается в файле mvc-dispatcher-servlet.xml). Теперь нужно вызвать загрузку классов. Для этого я создал отдельный контроллер — ReloadController. В реальности обнаруживать изменения можно разными способами, но для демонстрации это подойдет. ReloadController перезагружает 2 класса в нашем приложении. Вызвать контроллер можно перейдя по ссылке  :8080/mvcclassloader/reload. \r\n \r\nПосле этого перейдя снова на  :8080/mvcclassloader/hello мы увидим: \r\n \r\nНо это еще не все. Мы можем также поменять код WebController. Давайте сделаем это. \r\n \r\n \r\n \r\nВызвав перезагрузку классов и перейдя на основной контроллер мы увидим: \r\n \r\nВ данном приложении класслоадер полностью перезагружает контекст после каждой компиляции классов. Для больших приложений это может занимать значимое время, поэтому существует другой путь — можно менять в контексте только те классы, которые были скомпилированы. Такую возможность нам предоставляет DefaultListableBeanFactory. Например, в нашей платформе CUBA замена классов в Spring-контексте реализована так: \r\n \r\n \r\nКлючевой здесь является строка  \r\nЗдесь есть одна тонкость — DefaultListableBeanFactory по умолчанию не перегружает зависимые бины, поэтому нам пришлось слегка доработать ее. \r\n \r\n \r\n \r\n \r\nСуществует несколько способов доставки изменений в серверное Java-приложение без перезапуска сервера. \r\n \r\nПервый способ — это конечно же Hot Swap, предоставляемый стандартным отладчиком Java. Он имеет очевидные недостатки — нельзя менять структуру класса (добавлять, изменять методы и поля), его очень проблематично использовать на «боевых» серверах.  \r\n \r\nВторой способ — Hot Deploy предоставляемый контейнерами сервлетов. Вы просто загружает war-файл на сервер, и приложение стартует заново. У этого способа также есть недостатки. Во-первых, вы останавливаете приложение целиком, а значит оно будет недоступно какое-то время (время развертывания приложения зависит от его содержания и может занимать значимое время). Во-вторых, сборка проекта целиком может сама по себе занимать значимое время. В-третьих, у вас нет возможности точечно контролировать изменения, если вы где то ошиблись — вам придется разворачивать приложение заново. \r\n \r\nТретий способ можно считать разновидностью второго. Можно положить class-файлы в папку web-inf/classes (для веб приложений) и они переопределят классы имеющиеся на сервере. Этот подход чреват тем, что существует возможность создать бинарную несовместимость с существующими классами, и тогда часть приложения может перестать работать. \r\n \r\nЧетвертый способ — JRebel. Я слышал, что некоторые используют его даже на серверах заказчика, но сам бы я так делать не стал. В тоже время, для разработки он отлично подходит. У него есть единственный минус — он стоит довольно больших денег.  \r\n \r\nПятый способ — Spring Loaded. Он работает через javaagent. Он бесплатен. Но работает только со Spring, и к тому же не позволяет менять иерархии классов, конструкторы, и т.д. \r\n \r\nИ конечно, есть еще динамически компилируемые языки (например Groovy). Про них я написал в самом начале. \r\n \r\n \r\n \r\n \r\nКонечно, есть и недостатки. Усложняется механизм установки изменений. В общем случае необходимо строить архитектуру приложения таким образом, чтобы она позволяла менять реализацию на лету (например не использовать конструкторы, а получать классы по имени и создавать объекты с помощью рефлексии). Несколько увеличивается время получения классов из класслоадера (за счет проверки файловой системы).  \r\n \r\nОднако, при правильном подходе преимущества с лихвой перекрывают недостатки. \r\n \r\nВ заключении хочу сказать, что мы используем данный подход в наших приложениях уже около 5 лет. Он сэкономил нам много времени при разработке и много нервов при исправлении ошибок на боевых серверах.\n\n      \n       \n    ", "hub": "JAVA / Интересные публикации / Хабрахабр", "title": "Динамическая компиляция Java-кода своими руками"}
{"date": "18 февраля 2015 в 12:46", "article": "\n      Хочу поделиться еще одним способом решения такой проблемы, как запуск более одного mysql-server на одном linux-сервере. Я думаю, что некоторые из вас уже пробовали это делать, запуская руками, например, вот так: \r\n \r\n   \r\nЯ считаю этот подход не совсем правильным хотя бы потому, что корректно завершить такой запущенный mysql-сервер получится не всегда. Можно ещё, конечно, использовать  , но лично я предпочитаю полностью «изолировать» друг от друга инстансы. Именно поэтому я и предлагаю вам в своей публикации использовать в помощь систему инициализации.  \r\n \r\nДано: OS Linux CentOS 6.5. Как ставить MySQL-сервер или его производные (например, Percona), я рассказывать не стану, перейду сразу к делу. Поставили, запустили. \r\n \r\n   \r\n \r\nСервер запустился, всё работает. Все параметры по-умолчанию, если не указано обратное в  . \r\n \r\nТеперь займёмся запуском второго инстанса. Создаем место для второй базы: \r\n \r\n \r\nДля хранения логов: \r\n \r\n \r\nКопируем дефолтный конфигурационный файл для второго инстанса: \r\n \r\n \r\nЗаймёмся отдельной директорией в /var/run: \r\n \r\n \r\n . \r\n \r\nПравим /etc/my2.cnf: \r\n \r\n \r\nЧем различаются конфигурационные файлы:  \r\n \r\n \r\nНу и, наконец, подготовим init-скрипт для запуска второго сервера. Здесь начинается самое интересное: \r\n \r\n \r\nОткрываем и правим отдельные строчки в файле /etc/init.d/mysqld2: \r\n1) Во-первых, нам нужно закомментировать (или удалить) функцию  , дабы она не подсовывала mysqld_safe значения по-умолчанию: \r\n \r\n \r\n \r\n2) Также комментируем её использование в дальнейшем и записываем в нужные переменные уникальные пути и директории для нового инстанса: \r\n \r\n \r\n \r\n3) Последний штрих: в функции start() явно указываем mysqld_safe использовать наш   конфигурационный файл и писать в отдельную папку логи. Для этого приводим следующую строку к приблизительно такому виду: \r\n \r\n \r\nСохраняем. Запускаем, выполняя /etc/init.d/mysql2 start. Остановка: /etc/init.d/mysql2 stop. Проверка: \r\n \r\n \r\nНу и, наконец-то, подключаемся с помощью клиента к нашему второму MySQL-серверу путем явного указывания сокет-файла: \r\n \r\n \r\n… и видим, что у нас всё получилось. \r\n \r\nСпасибо за внимание. \r\n \r\nP.S. В директории второй базы можно сначала использовать либо дефолтную, скопировав её из  , либо заново созданную — с помощью  , либо уже имеющуюся (например, бэкап, созданный  ). Самое главное — корректный пользователь-владелец ( ) на файлы базы.\n       \n    ", "hub": "Серверное администрирование / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2010 в 16:20", "article": "\n      Блуждая в просторах   наткнулся на очень простое и, на мой взгляд, довольно таки интересное расширение. \r\n \r\nНазывается оно  . \r\n \r\nCуть его заключается в том, чтобы на страницах с установленным скриптом google analytics вызвать функцию (ga.js, __trackEvent), тем самым, оставив в статистике данного сайта свой след (в категории «Guest List» -> «Visit»). \r\n \r\nСсылки под катом! \r\n \r\nРасширение не работало с асинхронным способом подключения… Так что я его немного дополнил. Автор на контакт не выходил, и я решил выложить новое:  \r\n \r\nЕсли вы решились установить его — не забудьте поменять настройки и выставить нужные данные, которые будут отправляться на сервер! \r\n \r\nСсылка на оригинал:  \r\n \r\nP.S. \r\nВ «консоле» показывается, что отправилось на сервер.\n\n      \n       \n    ", "hub": "Google Chrome / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 23:14", "article": "\n       В современном мире компании используют большое количество каналов привлечения клиентов: контекстная реклама, баннерная реклама, реклама на радио или ТВ, наружная реклама и так далее. В каждом из этих каналов еще могут присутствовать параметры размещения, например, ключевые слова или площадки. Реклама — дело затратное и у бизнеса возникает понятное желание изучить эффективность тех или иных каналов и площадок. В случае онлайн-продаж все более-менее просто и отработано, есть специальные метки и cookie, которые цепляются к браузеру клиента и, когда он совершает покупку, Google/Yandex/любая другая система выдает вам информацию о том что клиент пришел из определенного канала по каким-то определенным ключевым словам. Но не все компании продают товары или услуги онлайн, многие до сих пор делают это по телефону, у каждого бизнеса своя специфика и телефон как канал продаж до сих пор не утратил своей актуальности. Для анализа эффективности маркетинговых каналов в случае телефонных продаж используется call tracking, дальше мы рассмотрим типы колл-трекинга и расскажем о том как его можно реализовывать. \r\n \r\n Существует несколько типов колл-трекинга — статический и динамический. В случае статического колл-трекинга каждому рекламному каналу выделяется свой номер и за ним закрепляется, потом все звонки через номера фиксируются и анализ статистической информации показывает какой из каналов наиболее эффективен. Данный способ хорошо работает, если нужно отслеживать какое-то ограниченное количество каналов. В современном мире рекламодатели хотят отслеживать не только каналы, но и эффективность тех или иных ключевых слов и отлавливать отдельных клиентов, что в конечном итоге ведет к необходимости большого количества номеров.  \r\n \r\n В этом случае начинают применять динамический колл-трекинг: при динамическом колл-трекинге существует некоторый пул номеров телефонов, который используется для всех каналов, но связка номера с посетителем сайта происходит на лету и действует какое-то ограниченное количество времени, если в течение этого времени клиент позвонит по номеру, то система это отследит. Существует соотношение между количеством посещений сайта и количеством номеров, которое необходимо для трекинга приходящих на сайт посетителей из разных каналов, поэтому сервисы колл-трекинга обычно привязывают цену своих услуг к посещаемости ресурса. \r\n \r\n   \r\n \r\nНу все, с теорией покончили, можно переходить к практике. Мы рассмотрим вариант пула номеров для трекинга, подключенных к платформе  , которая будет отвечать за обработку входящих звонков и перенаправление их на номер клиента. В сценарии обработки звонков мы предусмотрим получение данных для переадресации с бэкенда по HTTP(S) и запись информации о звонках. Если звонок успешный, то мы сохраним его дату, длительность, URL записи разговора, номер на который пришел звонок и номер на который мы переадресовали вызов, в случае если звонок не состоялся еще добавим информацию об ошибке — почему именно не дозвонились. Перейдем к реализации: \r\n \r\n \r\nЭто самый простой сценарий, который показывает концепцию использования VoxImplant для колл-трекинга, его можно усложнить и пытаться прокинуть звонок сразу на несколько номеров одновременно, или пытаться дозваниваться на несколько номеров по очереди. Если номеров нужно больше чем есть в наличии, всегда можно воспользоваться добавочными, которые клиент будет вводить при звонке на номер. В данном случае в код нужно будет добавить обработку нажатий клавиш, примерно так: \r\n \r\n \r\nНаверное, можно придумать еще много всего интересного, но в рамки данной статьи это не входит. Важно помнить, что полноценный сервис колл-трекинга требует много работы и готовые сервисы, которые этим занимаются, не просто так берут за это деньги. Буду рад ответить на любые вопросы в комментариях.\n\n      \t \n\n       \n    ", "hub": "Разработка систем связи / Интересные публикации / Хабрахабр", "title": "Делаем call tracking"}
{"date": "18 февраля 2015 в 15:28", "article": "\n      Это кажется радикальным, и я не верю в то, что мы близки к тому, чтобы увидеть мир, полный столов и кресел, телефонов, клавиатур, и компьютеров в заброшенных зданиях контакт-центров, все еще наполненных эхом фразы «Чем я могу вам помочь?» Однако многие специалисты, которых я встречал, кажется держат в центре внимания снижение стоимости обслуживания клиентов. Это повсеместная забота о том, что придется снижать стоимость, возможно до той воображаемой точки, когда снижать уже некуда! \r\n \r\nПрактически мы все в повседневной жизни стремимся покупать товары все более лучшего качества. Когда мы что-то покупаем, мы обычно ориентируемся на соотношение «цена-качество», и оба эти фактора влияют на возможность продавцов получать прибыль. Прибыль обычно инвестируется в развитие предприятия, оно становится все больше и переживает конкурентов! \r\n \r\nКаждый день я встречаюсь со специалистами, которые ищут способы улучшения обслуживание клиентов, что необходимо для сохранения репутации компании и повышения ее конкурентоспособности. За редким исключением эти специалисты одновременно стремятся снизить стоимость обслуживания. И это только то изменение, которое произошло за мои 10 лет пребывания в индустрии контакт-центров (и плюс 30 лет в области информационных технологий) — снижение стоимости ускоряется. \r\n \r\nВ последние 10 лет индустрия контакт-центров с переменным успехом переживала изменения с SOA, BPM, облачными технологиями, анализом речи, записью экранов, IVR, объединенными коммуникациями, WFO и т.д. Хотя все эти технологии внедрялись, миллионы долларов были потрачены на повышение производительности контакт-центров, почему же руководители индустрии все еще видят недостаток эффективности? \r\n \r\nСпециалисты в сфере информационных технологий и бизнеса всегда помогали рынку, внедряя инновационные решения и эффективные технологии, и я наблюдаю повсеместное повышение активности в этой области. Итак, что же они видят и куда направят усилия в 2015 году и далее? \r\n \r\n \r\nВсе просто, это преобразование и упрощение через автоматизацию. Автоматизация — ключ для руководителей, позволяющий им выполнить свои обещания акционерам об увеличении прибыли. Автоматизация — это ключ к быстрому возврату инвестиций, к улучшению обслуживания клиентов и одновременному снижению стоимости. Существует множество видов автоматизации. \r\n \r\nАвтоматизация бизнес-процессов. Этот термин становится повсеместным для больших проектов, но он также эффективен, когда используется как часть малых проектов. \r\n \r\nРоботизация. Эта технология берет то что человек вводит с клавиатуры и выполняет это программируемым роботом. Это эффективно, поскольку не изменяет какие-либо части процесса или приложения, а также принятых бизнес-правил, но автоматизирует многие нажатия клавиш, клики мышкой и не допускающие изменения действия, ранее выполнявшиеся человеком. Новые технологии делают это лучше, замещая устаревший анализ экранных данных. \r\n \r\nРоботизация позволяет выполнять процессы значительно быстрее, но это не всегда важно. Даже если это выполняется с той же скоростью, с какой может и человек, все равно имеет смысл использовать роботы (виртуальные машины), если вы хотите увеличить часовую производительность! \r\n \r\nАвтоматизация рабочего места. Этот тип автоматизации подобен роботизации, но он не замещает человека, а помогает ему в работе за компьютером. Это является ключевым шагом в процессах, которые не требуют участия человека. Опять же это не изменяет используемых приложений, не требует переписи приложений, и существующая бизнес-логика остается на 100% неизменной. Операторы все еще привлекаются к работе с клиентами — это то, что еще не автоматизировано. \r\n \r\nАвтоматизация — не новое в сфере информационных технологий или бизнеса, но вот что ново — так это качество продукции на рынке и успех, который она принесла организациям, занимающимся обслуживанием клиентов. Это подход использовался многими контакт-центрами в мире, и автоматизация обеспечила существенный рост производительности. Известно, что многие специалисты построили успешную карьеру, внедряя автоматизацию на предприятиях отрасли. \r\n \r\nАвтоматизация — удачное решение, поскольку она решает проблемы и контакт-центра, и бэк-офиса. «Как получить прибыль без особых расходов и не тратить годы на это?». Автоматизация — логичное решение, поскольку, когда вы смотрите на работу операторов контакт-центра сотрудников бэк-офиса, вы осознаете, что компьютер выполняет множество функций, которые раньше выполнялись людьми. Не надо быть специалистом в ракетной технике (или иметь черный пояс по системе «Шесть сигма»), чтобы заглянуть через чье-то плечо и сказать: «Почему вы ввели эти данные дважды?» или «Почему вы пытаетесь продать неподходящий продукт?». \r\n \r\nАвтоматизация легко масштабируется. Вы можете начать автоматизацию с полудюжины очевидных процессов, как например: «Автоматическая регистрация», «Единый вход в систему», «Оформление заказов в реальном масштабе времени», «Автоматическое обновление», и двигайтесь к более сложной автоматизации, которая предоставит объединенные обзоры, «умное» рабочее место, интеграцию с компьютерной телефонией, управление процессами, горячие связи и т.д. В дальнейшем вы можете автоматизировать все, что можно, и немедленно получать выгоду и экономить время. \r\n \r\nКлючевое отличие автоматизации от других «интеграций» или стратегий снижения стоимости в том, что эти другие стратегии обычно не приносят прибыль до полного завершения проекта, и, как следствие, иногда ошибки исправлять уже поздно. Автоматизация позволяет быстрее вернуть инвестиции, ее последствия очевидны и предсказуемы, и вы быстро получаете то, что ожидали. \r\n \r\nПоследнее, но не менее важное: автоматизация идет рука об руку с уменьшением ошибок с того момента, как вы автоматизируете часть действий оператора, это снижает его нагрузку при обслуживании клиента. Это может играть ключевую роль при решении особенно сложных проблем, которые могут возникнуть при ручной обработке вызова. \r\n \r\nВ действительности специалисты контакт-центров добиваются повышения эффективности многие годы, но, проявляя благосклонность к информационным технологиям, замечу: только недавно появились способы эффективного решения многих проблем. И эти новые технологии не стоят на пути прогресса, а, наоборот, ускоряют его. \r\n \r\nИтак, инновационные технологии необходимы для контакт-центров. Напрашивается последний вопрос: «А это действительно работает?» Ответ безоговорочный: «Да». Многие годы автоматизация была вне поля видимости, до тех пор, когда ее полезность и необходимость не стала очевидной. И сейчас она широко применяется. От небольшого контакт-центра на 150 операторских мест, до большого распределенного контакт-центра с 35000 операторов автоматизация рабочих мест — катализатор процесса преобразований, ведущих к достижению мирового уровня обслуживания клиентов. Большой или малый, любой успешный контакт-центр использует комбинацию из автоматизации бизнес- процессов, роботизации и автоматизированных рабочих мест. \r\n \r\nРуководители и специалисты контакт-центров находятся под постоянным прессингом из-за проблем, касающихся повышения качества обслуживания клиентов и снижения стоимости этого процесса. Эта ситуация не должна приводить к гибели контакт-центра. Вооруженный современными технологиями автоматизации и всесторонним планом ее развертывания, контакт-центр в конце концов достигнет оптимальной эффективности.\n       \n    ", "hub": "Разработка систем связи / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 12:07", "article": "\n        Некоторое время назад, хаброжитель    написал статьи о создании электронных часов. В них он рассмотрел  , а так-же  . Были упомянуты Arduino, STM, Raspberry PI, ESP8266, но совсем  .  \r\n \r\nДавайте заполним этот небольшой пробел. Узнаем, на сколько просто сделать часы на ПЛИС и какие аппаратные ресурсы для этого потребуются. К тому же, мне подарили микросхему ПЛИС очень малого объема — 64 макроячейки. Это ПЛИС LC4064v фирмы Lattice с которыми я до этого никогда не работал. Я думаю, будет интересно! \r\n \r\nЦели: \r\n \r\n \r\nМеня ожидает несколько очень приятных вечеров, посвященных разработке на ПЛИС! \r\n \r\n \r\n \r\n \r\n  — американский производитель микросхем. Компания располагается в Орегоне и выпускает высокопроизводительные ПЛИС. Есть информация о том, что микросхемы популярны в промышленных разработках из за открытости документации и протоколов. Широкий   и их назначений говорит о том, что они делают серии ПЛИС для автоматизации, автомобилестроения, создания компактных, портативных, экономичных устройств. Так же есть серии ПЛИС больших объемов (до 100k и более макроячеек).  \r\n \r\n \r\n \r\n К сожалению, я пока не смог найти магазин, где можно было бы купить ПЛИС объемом в 1 — 4 тысячи макроячеек и ценой в одну-две тысячи рублей. Однако, плату под свои требования я нашел —  . Количество ячеек — 6900, на плате уже есть программатор ( ),  . Цена 25$, однако, стоимость доставки, при оформлении покупки в Lattice Store, оказалась 45$. Чтож, будут деньги — буду искать на   или   и заодно их осваивать. Однако, еще нашелся вариант покупки на Родине, в магазине  , по цене 2992 рубля. На Ebay/Aliexpress выбор пока не большой, однако удалось найти чип без платы на 5 тысяч ячеек —  . \r\n \r\nПри разборке старой электроники, периодически попадаются CPLD небольших размеров (32, 64 ячейки). Одну такую микросхему мне и подарил мой товарищ. Микросхема ПЛИС LC4064V ( ) — представитель  . Мой вариант работает от напряжения 3.3В, на частотах до 400 МГц! В корпусе TQFP имеется целых 32 линии ввода/вывода, из них две линии используются под тактовые входы. Линии толерантны к 5 В в режимах LVCMOS3.3, LVTTL и PCI. Все выводы могут быть \"притянуты\" pull-up, pull-down либо могут висеть в воздухе. Так же, можно работать в режиме open-drain.  \r\n \r\n \r\n \r\nДля небольших ПЛИС, к которым относится LC4064v нужен:  . \r\nДля прошивки нужно установить:  . \r\nДля более крупных ПЛИС:  . \r\n \r\nДля активации программ, нужно  е, указать свой MAC адрес и скачать ключи лицензий. \r\n \r\nДля создании общего представления о том, как разрабатывать проекты в среде Lattice Diamond, рекомендую ознакомиться с  . \r\n \r\n \r\nПрограмматоры LPT уже практически ушли в прошлое. Да и собирать программатор нет времени и желания(есть желание делать проект). Как и в случае с Altera, я решил найти и купить готовый китайский клон программатора, на площадках Ebay или Aliexpress. Однако, если программатор для Altera стоит в пределах  , то для Lattice цены уже порядка  . За, казалось бы, одинаковые изделия, отличающиеся только наклейкой. Но если хорошенько поискать, то можно найти дешевле —  ! Беру сразу два: себе и товарищу. По приезду, один программатор  . Начал было общаться с продавцом, тот уверил, что они все проверяют. В итоге, я его сам и починил, пропаяв землю на разъеме USB. \r\n \r\nСравнил с программатором для Altera, чтобы убедиться, что они всетаки разные, а не \"отличаются только ID USB\". \r\n \r\n \r\n \r\nЕще немного фото:  ,  ,  . Микросхем действительно больше, так что вполне очевидно, что программатор получается дороже. \r\n \r\nДля проверки микросхемы на скорую руку делаю плату со светодиодами и кнопкой. Саму микросхему припаиваю на плату-переходник.  \r\n \r\n \r\nВ качестве генератора не нашел ничего проще, чем   :) Поделил частоту тактового генератора 50 МГц до 2 Гц и вывел обратно. Этот сигнал подал на тактовый вход LC4064v. Сделал простейший 4х битный счетчик, а его биты вывел наружу и подключил к светодиодам. \r\n \r\nПрошиваю — работает! \r\n \r\n \r\nТеперь, когда стало ясно, что ПЛИС работает и поддается программированию, можно придумать себе   задачу и героически ее решить! \r\n \r\n \r\nНа самом деле, идея сделать часы пришла не сразу. Количество ячеек, равное 64, заставляет задуматься над задачей, которую можно им поручить. Вспоминаются старые времена, когда в наличии имелся строго определенный МК, который я умел программировать, и я пытался уместить в него решаемую задачу. Программирование ПЛИС предполагает обратное: решение задачи и выбор ПЛИС нужного размера по результатам синтеза. Но тут мы переворачиваем всё с ног на голову, и ограничиваем себя размером. Это заставит нас потрудиться, как в выборе задачи, так и в ее реализации. Эдакое экстремальное программирование в ограниченных условиях. \r\n \r\nЧто сделать же сделать? Электронный синтезатор? Маловато ячеек: только приемник MIDI команд занимает не одну сотню. Нужно что-то простое! Например декодер звукового I2S потока и вывод его через ЦАП на резисторах? Как то очень специфично, просто и не очень зрелищно. Линий много — можно повесить семисегментные индикаторы (7х4 = 28 даже статически!). А что изображать? Можно сделать либо частотомер (отличный вариант, ведь микросхема работает аж до 400 МГц) либо часы. Решено выбрать часы. \r\n \r\n \r\nДля часов нам потребуются семисегментный индикатор на четыре цифры. Сначала я хотел приобрести новый индикатор в магазине, а потом вспомнил, что у меня уже есть часовой индикатор, который лежит в ящике уже, наверное, лет десять. Мне его когда-то очень давно подарил regenerator с форума  . Схема включения моего индикатора имеет следующий страшный вид: \r\n \r\n \r\n \r\nВ рамках избавления ящиков от залежавшихся компонентов, а так же издевательства над ПЛИС(и собой), было решено использовать именно этот, на первый взгляд, непонятный индикатор. На самом же деле все достаточно просто: сверху мы подаем управляющее напряжение на индикаторы. Как правило, один провод идет сразу на два сегмента. А какой из двух сегментов будет работать, определяется шиной, которую мы подключили к общему проводу в данный момент. Схема, в целом, соответствует классической   с той разницей, что индикаторов у нас получается всего два. Для проверки работы всех индикаторов, вывел логическую единицу на все выходы, а сигнал на транзисторы вывел по очереди. \r\n \r\n \r\nПо результатам теста, выяснилось, что мы, в принципе, можем выводить цифру на любой индикатор. Однако, в самом левом индикаторе отсутствует один сегмент. Это не позволит выводить на него все возможные цифры, но самое важное — цифру \"ноль\". Этот факт подтверждается и вот  . Поэтому ноль в десятках часов будем гасить. \r\n \r\nЕще нам потребуется \"часовой генератор\" — это генератор на частоту 32768 Гц. Почему такая частота? Потому что, если взять 15 битный двоичный счетчик и подать на него эту частоту, то на выходе мы получим 1 Гц (2^15 = 32768). То есть интервал в одну секунду. Хочу отметить еще один важный момент в выборе частоты генератора. Для того, чтобы поделить частоту на 32768 потребуется 15 битный двоичный счетчик, а это как минимум займет 15 ячеек в ПЛИС, что сразу отнимет почти четверть ресурсов. Счетчик для генератора в 50 МГц потребует уже 30 бит для деления до секундного интервала. При таких затратах, ячеек на логику работы часов может не хватить. \r\n \r\nПока часового кварца нет, в качестве генератора частоты опять же будет выступать Altera Cyclone, в которой я целочисленно поделил частоту 50 МГц до 32768 Гц. \r\n \r\nДля деления частоты применяю свой модуль на Verilog. Работать с ним очень просто: на clk подаем исходную частоту, из s_out получаем результат. Коэффициент деления задаем параметром DIV. В моем случае 50000000 Гц это входная тактовая частота, а 32768 Гц — частота, которую нужно получить. \r\n \r\n \r\n \r\n \r\nПервая итерация по созданию часов — это создание схемы двоичного счетчика с асинхронным сбросом (клик по картинке откроет схему на сайте   где вы сможете ее корректировать по собственному желанию) \r\n \r\n \r\n \r\nПри достижении счетчиком значения \"10\" (b1010)- мы должны его сбросить. Этот же сигнал будет использоваться в качестве тактового для счетчика следующего разряда. \r\n \r\n \r\n \r\nЦепочка RC используется для того, чтобы увеличить период сигнала достижения \"10\". Если этот сигнал будет слишком коротким, то некоторые триггеры могут не успеть сброситься, или поменять свое значение. Но применение асинхронного сброса в ПЛИС может привести к ряду проблем. Для того, чтобы увеличить период сигнала, мы применяем аналоговую схемотехнику, хотя разрабатываем цифровую схему. Если мы захотим применить такой подход в ПЛИС, то нам потребуется вывести этот сигнал наружу микросхемы — на внешний вывод, установить туда цепочку, и потом подать это обратно — уже на другой вход ПЛИС. \r\n \r\n \r\nВ качестве эксперимента, я все-таки сделал часы на счетчиках с асинхронным сбросом. При этом никаких RC цепочек я не выводил и не использовал. Моей целью было выяснить, действительно ли не стоит использовать асинхронную логику, или \"да ладно и так сойдет\". До какого-то этапа часы исправно отсчитывали время \r\n \r\n \r\nНо в тот момент, когда я решил добавить в часы мигание секунд, всё внезапно поломалось. \r\n \r\nПри разработке на ПЛИС настоятельно рекомендуется использовать синхронную логику. В случае с часами и счетчиком, текущее значение счетчика должно храниться в регистре, а следующее значение регистра мы должны вычислять комбинационной схемой. Следующее значение однозначно будет вычислено(мы надеемся на это) до наступления следующего такта, но загружено в регистр будет по фронту тактового импульса. То есть никаких \"иголок\" сброса проскакивать не будет. Исходя из текущих значений регистров, мы всегда однозначно определяем значение на следующем такте. \r\n \r\n \r\nДля вывода значения регистра на семисегментный индикатор, нам потребуется модуль преобразователя из двоичного в семисегментный код: \r\n \r\n \r\nДля экономии ресурсов, описываем варианты только для чисел от 0 до 9. Для эксперимента, пробуем альтернативный модуль преобразователя: \r\n \r\n \r\nНо объем затраченных ресурсов не изменился. Радуемся за умный синтезатор. \r\n \r\nДальше просто создаем регистры для секунд(единицы, десятки), минут(единицы, десятки), часов(единицы, десятки). Для каждого задаем расчет следующего значения. Но изменение значения секунд мы делаем при каждом секундном импульсе, а вот изменение значения десятков секунд — только при переполнении единиц. Соответственно, единицы минут меняются только когда переполнились и десятки и единицы секунд. И так далее. \r\n \r\nПри этом, синтез показал, что все отлично, но ячеек устройства уже не хватает. Упс. Для того, чтобы исправить ситуацию, решено секундный регистр упростить. Он все равно не выводится, поэтому два двоично-десятичных счетчика меняем на один двоичный от 0 до 59. А забегая немного вперед, я сделал его на один бит больше, зато сократил счетчик общего делителя частоты часового кварца. Поэтому счет идет от 0 до 119, но с частотой 2 Гц (см ниже, часть про установку времени). \r\n \r\n \r\n \r\nКаждый регистр ЧЧ: ММ мы выводим на семисегментные индикаторы. Но перед этим преобразовываем в семисегментный код.  \r\n \r\n \r\nНо у нас динамическая индикация. Причем, при выводе переключаются не отдельные семисегментные блоки, а отдельные элементы индикаторов. Поэтому, спустимся до уровня каждого элемента индикатора: \r\n \r\n \r\nДальше, исходя из схемы индикатора, понимаем, что там две линии по минусу. Будем их переключать по очереди: \r\n \r\n \r\nПрисваиваем совмещенным линиям сегментов, соответствующие им значения, в зависимости от того, какая сейчас линия активна: \r\n \r\n \r\n \r\nИ тут выясняется, что ресурсов снова не хватает! В процессе борьбы за ресурсы были предприняты следующие шаги: \r\n \r\nПеределал енкодер из bcd в семисегментный код. Обычный енкодер преобразует числа от 0 до 15 в шестнадцатиричный формат. В часах используется десятичная система для отображения, поэтому в енкодере были оставлены только цифры от 0 до 9, то есть вход — 4 бита и выход 10 вариантов семибитных слов. Но для десятков минут (от 0 до 5) достаточно 3 битного енкодера. То есть 6 вариантов. Похожая картина и для десятков часов. Только там вообще 3 цифры — от 0 до 2. И это уже 2 бита для входа енкодера и всего 3 варианта на выход. Все это наверняка должно было снизить сложность синтеза. Так и случилось! Количество использованных ячеек снизилось с 63 до 59. \r\n \r\nУменьшил разрядность регистров десятков минут и часов. Это снизит количество ячеек по количеству бит, плюс упростит схемы сумматоров. \r\n \r\n \r\nВ общей сложности, на момент начала оптимизаций было занято 63 из 64 ячеек, что ставило под вопрос возможность реализации не только возможных дополнительных функций, но и основных (таких, как установка времени). Теперь занято стало 56 макроячеек и 8 свободны. В процентном соотношении было 98%, стало 87%. Для такого малого объема это очень хорошо: около 10%! \r\n \r\n \r\nТеперь, когда с ресурсами стало легче, займемся установкой времени. Решено сделать 3 кнопки: \"часы\", \"минуты\", \"секунды\". При нажатии и удерживании кнопки \"часы\", их значение должно увеличиваться. Те же условия и для кнопки \"минуты\". Сделать это просто: мы просто разрешаем записать в регистр новое значение при каждом такте, если нажата кнопка (а не только при переполнении младшего разряда).  \r\n \r\nПри нажатии кнопки \"секунды\" — другая логика: секундный счетчик должен быть сброшен и счет секунд должен быть остановлен. \r\n \r\nДля того, чтобы установка времени была более комфортной, скорость увеличения значения пришлось увеличить, а для этого частоту итогового тактового генератора выбрать не 1 Гц, а 2 Гц (об этом я писал выше). Это так-же сэкономило одну ячейку на делителе частоты, которому потребовалось на один разряд меньше. \r\n \r\n \r\nКак видим, два раза в секунду у нас проверяется сигнал переполнения или признак нажатия кнопки. В обоих случаях, часы или минуты увеличиваются. А значению секунд присваивается ноль, пока удерживается кнопка \"секунды\". \r\n \r\n \r\nДля секундного индикатора нам уже не подходит какой-либо бит из регистра делителя частоты 32768. Потому что минимальная частота там — 2Гц, а мигать надо раз в секунду. Но два раза в секунду у нас увеличивается двоичный счетчик секунд. Поэтому просто возьем нулевой бит из этого счетчика: \r\n \r\n \r\n \r\nВысвобождение ячеек позволило добавить функционала. Одна из идей — сделать источник питания с резервной батареей. При наличии основного питания, часы могут работать в полноценном режиме, а в случае перехода на резервное питание — уходить в экономный режим: отключать индикацию и заниматься только подсчетом времени.  \r\n \r\nДля того, чтобы определить целесообразность этого функционала, нужно определить ток в штатном режиме и ток, который будет потреблять устройство в режиме энергосбережения. Можно ориентировочно посчитать, что при токе примерно в 10 мА на сегмент и не больше 12-ти одновременно работающих сегментов, мы получим только от индикации — 120 мА. Плюс сама ПЛИС потребляет энергию. Согласно документации — это должно быть порядка 10 мА. \r\n \r\nИзмерения показывают, что всё устройство в целом потребляет 125 мА. Это близко к расчетам. Добавляю в логику дополнительные условия и еще один сигнал на вход. Это сигнал перевода устройства в экономный режим. Управляющий сигнал будем брать с источника питания, с той части схемы, которая получает энергию из сети. Чтобы пользователь видел, что устройство работает, находясь в экономном режиме, добавим мигание секундным светодиодом. Частота будет та же, но заполнение не 50%, как в штатном режиме, а сделаем очень короткую вспышку. Для этого возьмем часть битов из счетчика-делителя частоты. На реализацию этой логики ушла всего одна ячейка. \r\n \r\n \r\nТок в режиме энергосбережения составил 18,5 мА. В принципе, это не так и мало. Однако, если найти примененную ПЛИС на сайте производителя, то она там классифицируется, как \"architectures to offer a SuperFAST™ CPLD solution with low power\". Супербыстрые — это до 400 МГц. У меня подозрения, что такой ток ради быстроты. Но в линейке есть более медленные и при этом экономные варианты: ispMACH 4064ZE, Operating Power Supply Current Vcc = 1.8V при этом ток  . \r\n \r\n \r\nДля создания генератора, изначально решил попробовать использовать  . Это было бы весьма оригинально и экономно. Однако, \"что-то пошло не так\" и такой вариант работать не стал. Причем даже на Cyclone, причем даже в случае создания железных буферов. Я думаю, конечно, можно заставить работать и такую схему, поиграв с параметрами обратной связи. В общем, нужно делать внешний генератор. Поискал схемы. Выбранная схема на транзисторах заработала, но параметры получаемого сигнала не позволили использовать ее в качестве генератора. В итоге решил собрать проверенную схему генератора — на ТТЛ микросхеме. Но на КР1533ЛА3 вот   не заработала. В итоге запустился генератор по мотивам вот этой схемы: \r\n \r\n \r\nОсталась одна проблема: микросхема КР1533ЛА3 должна работать при напряжении 5В +- 10%, то есть от 4.5 до 5.5, а ПЛИС работает от 3.3 вольт. Однако, работа микросхемы происходит по сути в аналоговом режиме. Рискнул, запустил от 3.3 вольт — работает! Поставил часы походить. \r\n \r\nЭксперимент по выбегу   частоты генератора показал, что часы \"бегут\". За три часа набирается уже секунд 20. При тестировании на начальном этапе, в качестве генератора использовался Cyclone с делением частоты генератора 50 МГц. При этом какого-то ощутимого отклонения в ходе часов не наблюдалось. А тут за 6 часов все \"убегает\" на 2-3 минуты! Подводить часы каждый день неприемлемо. Поэтому я вернулся к схеме  \r\n \r\n \r\nАнализ схем такого рода навел меня на мысль о том, что тип микросхемы имеет какое-то значение. Во всех таких схемах применяется КМОП, а в моем случае была микросхема ТТЛ. Чтобы проверить эту гипотезу, я зашел в магазин и приобрел несколько микросхем 561-й серии. Среди них оказалась микросхема К561ЛА9. Изучение документации показало, что работать эта микросхема может от напряжений в диапазоне 3..18 вольт, что снимает вопрос о правильности её питания. Схема запустилась сразу, только не на 32768 Гц, а на какой-то более высокой гармонике. Резистор 100К пришлось увеличить до 300К. Поставил часы походить. За два дня часы убежали вперед примерно на 30 секунд. Это тоже не мало, но уже лучше, чем +3 минуты за 6 часов. Теперь можно подобрать конденсаторы в схему генератора для более установки более точного хода. И продумать способ построения частотомера, способного измерить такое отклонение частоты. Иначе настройка точности хода может затянуться. С помощью ПЛИС Cyclone и генератора на 50 МГц я набросал частотомер со временем измерения 1 и 10 секунд, данные читал с помощью SignalTap. В генератор поставил конденсаторы по питанию, подобрал конденсатор и подстроечный конденсатор частоты хода. Частоту вогнал в 32768 плюс минус точность измерений. За сутки хода, видимого на глаз отклонения не обнаружено. На этом с генератором все. \r\n \r\nЧтение МРБ 1178 показало, что с часовыми кварцами можно достичь точности хода до 2 секунд в месяц. Считаю, что такое значение точности вполне достаточно для простых часов. \r\n \r\n \r\nСогласно расчетам и измерениям, устройство потребляет не мало: 120 мА. Питать устройство изначально планировалось от электросети 220 вольт. Поэтому высокие значения КПД от преобразователя 220 -> 3.3 вольта — не требовались. При таких токах, линейный стабилизатор приведет к увеличению потребления, но при этом даст отличную надежность, в силу своей простоты. Вообще, я согласен, что надо беречь энергию и даже подобрал подходящие компоненты. Но у нас простые часы и я готов оплатить перерасход электроэнергии. \r\n \r\n \r\nСигнал о том, что напряжение в сети есть, будем брать с выхода стабилизатора. Если его там нету — гасим индикаторы! \r\n \r\n \r\nКорпус выбрал маленький. Пришлось постараться, чтобы в него все поместилось. Некоторые стойки для монтажа накруткой пришлось обкусить. Блок питания придется использовать внешний. На корпус выведены кнопки установки часов, минут и сброса секунд. \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nВ результате проделанной работы я пришел к следующим положительным выводам: \r\n \r\n \r\n \r\n \r\nИсходные коды проекта на GitHub: \r\n \r\n \r\n \r\n \n\n      \n       \n    ", "hub": "Программирование микроконтроллеров / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2013 в 10:01", "article": "\n       \r\nСейчас появляется очень много материала про юнит и нагрузочное тестирования. Все поголовно пишут тесты, код создают исключительно через  , используют  . Однако, все тестирование очень тесно связано с тестовыми данными. А их нужно генерировать/писать. Проблема не стоит остро для юнит тестирования — накидал mock, погонял его и забыл. Но как быть с нагрузочным тестированием? Когда мне нужно не 1-2-5-10 объектов, а  ? \r\n \r\n Большинство (php) разработчиков, которых я встречал, сталкиваясь с задачей нагрузочного тестирования своего кода, создают несколько фикстур руками и насилуют их ( ). Полученный результат тестирования не является достоверным, но они об этом не думают. Более продвинутые пишут скрипты для генерации данных, закидывают в БД и после этого уже играются. Похвально, но таких значительно меньше, а сам способ мне не кажется идеальным — другой программист может не разобраться в говнокоде генерилки фикстур (ведь создатель писал это быстро и для утилитарных целей) и рано или поздно все либо пойдут по первому пути, либо начнут писать новую генерилку. \r\n \r\nЦенность правильного составления фикстур сейчас недооценена, многие просто на это забивают из-за трудоемкости такой работы (представим 15-25 связанных таблиц, писать скрипт генерации фикстур будет весьма, кхм, интересно). Я прекрасно понимаю почему разработчики так поступают, и, когда появилась такая же задача, то решил не биться головой об стену, а поискать инструментарий для нормальной генерации   данных. \r\n \r\nЯ был очень удивлен, но ничего вразумительного не было найдено, сложилось ощущение, что никого этот вопрос просто не интересует и мне всю жизнь придется писать кривые скрипты с кучей циклов. Тем не менее, подходящий инструмент был найден, мы успешно опробовали его в работе, и теперь я хочу представить его вам. \r\n \r\n \r\nБенератор (да, смешное название) служит для 2х целей: генерация данных и их анонимизация. Последнее выходит за рамки этой статьи, но тоже очень правильное и полезное дело (модификация дампа бд с продакшена, с целью порезать пользовательские личные данные и номера их кредиток). \r\nТулза использует написанный вами   сценарий для генерации   или экспорта прямо в базу. Работает вообщем-то везде и поддерживает следующие БД: \r\n \r\n \r\nСценарий представляет собою череду тэгов  , в которых вы описываете какие сущности вы будете создавать и каким способом. Звучит просто, но есть ньюансы. \r\n \r\nЭто обзорная статья, я не буду пытаться описать все возможности этого инструмента, не собираюсь переводить всю многостраничную документацию, моя задача — показать способ решения классических кейсов и, тем самым, заинтересовать тех кому это надо. \r\n \r\nСейчас я попробую пошагово описать процесс от скачивания до получения данных в постгресе. Предполагается, что у вас уже установлен постгрес :) \r\n \r\n \r\nРаспаковываем   и добавляем в конец  : \r\n \r\n \r\n \r\nВыполняем: \r\n \r\n \r\n \r\nСперва нам необходимо познакомиться с базовыми конструкциями сценария, то, как он строится и из чего состоит. Давайте сгенерируем 5 пользователей с несколькими полями и отдадим их в консоль. \r\n \r\nСоздаем произвольную папку, сохраняем в нее benerator.xml со следующим содержимым: \r\n \r\nЗапустив в этой папке   вы должны увидеть выдачу полученных объектов в консоль. А теперь давайте внимательно изучим   и разберемся как это произошло. \r\n \n\n      \n       \n    ", "hub": "Тестирование IT-систем / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2015 в 23:57", "article": "\n       \r\n \r\nЯ никогда не мог удержаться от покупки разных электронных штук, и однажды у меня стало на 10   МК больше. Я люблю ATtiny13 — дешево и сердито. Когда я их покупал, я твердо помнил, что у них   и сильно радовался их малой цене. \r\nОднако, когда я столкнул ATtiny13 с реальной задачей, оказалось что одной очень важной штуки в нем нету, а именно,   (разумеется, не считая GPIO).   Подумал я и пошел гуглить… И красивого готового решения под avr-gcc не нагуглил… О создании (надеюсь) такого решения, данная статья — добро пожаловать под кат. \r\n \r\nНа самом деле, нагуглилось примерно три варианта, но в   пишут на БЭЙСИКЕ (я вообще не знал что так можно), в   под CVAVR (привет моему первому мигающему светодиоду) и вообще там весь смысловой код на страшном ассемблере, а вот   вариант вроде бы подходит… Но какой-то очень странный код… Но заработал сполпинка. Но жииирный… \r\n \r\nНу да ладно, главное работает и вмещается, а там уже можно и почитать, и порефакторить… JumpStart состоялся — это главное. \r\n \r\n \r\n \r\nПосле вдумчивого чтения кода становится ясно, что его автор достоин глубокого уважения… Этот код похож на код человека, который программирует ОЧЕНЬ недавно, а задача то решена вполне мощная. Работает же. Очень хотелось для начинающих описать несколько тривиальных ошибок, но после третьей понял что сильно оффтоп, так что, увы… \r\n \r\n , код занимался и приёмом и отправкой данных, но реально, моя задача не требует приёма (тем более, в главном цикле). Мне достаточно просто отловить Pin Change Interrupt на любой ноге и выплюнуть результаты A->D преобразования. По этому, в целях похудения, было принято решение выпилить приём. Вот, что получилось после не очень долгого и не очень вдумчивого рефакторинга: \r\n \r\n \r\n \r\nНе хочется много думать, так что, я оставил всю смысловую часть как есть, выкинул лишнее и подправил вырвиглазные штуки вроде  . В этом коде мне сильно понравилась реализация интервалов! Весь геморрой с высчитыванием констант для baud rate сводится к одному числу BAUD_C, которое подбирается чуть ли не экспериентально и корректируется в зависимости от неточностей кварца (у автора она была равна 115 и на скриншоте вроде бы довольно точно работала. Возможно ли вообще такое хардовое уплывание?). Скорее всего, это не самое оптимальное, надежное и верное   решение, но мне оно кажется очень простым и красивым! \r\nЧтож, каков результат? \r\n \r\n  Но вообще, это еще не конец. Сейчас код умеет передавать только один символ, а надо передавать строки и значения регистров. А значит, время рыться в старых проектах! Эта задача уже не специфична для МК без UART и была решена неоднократно. \r\n \r\n \r\n \r\nОчень удобно передавать числа сразу с пояснениями, что это за числа, поэтому —  .  \r\nИ, с учетом примера использования: \r\n \r\n \r\n \r\nПолучается  \r\n \r\nЧуть больше, чем в оригинале. Но насколько полезнее!  \r\n \r\n \r\nХочу сразу предупредить, что цели сделать красивую и популярную статью не было. Писалась она очень быстро, перечитывалась очень мало и может содержать очень ошибки. Прошу не троллить — я всё исправлю. Главное, чтобы эта проблема перестала существовать и все, кто захочет построить свой датчик на ATtiny13 — имели готовое решение для интерфейса. \r\n \r\nP.S.: Сначала я смотрел в строну I2C и даже   очень многообещающий репозиторий, но не разобрался почему Atmel Studio говорит overflow. Было бы классно и этот интерфейс запилить на t13 — может даже худее получится… Но это уже не я. \r\n \r\nP.S.S.:  . Он шикарен!\n\n      \n       \n    ", "hub": "Программирование микроконтроллеров / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 09:53", "article": "\n       \r\n \r\nВ ядре Московской биржи имплементированы как функции непосредственно регистрации и проведения сделок, так и функции клиринга. Для примера, те же биржи Нью-Йорка и Лондона вынесли это в отдельный модуль, чтобы при падении одного из фронтов не останавливать все торги, а просто деградировать в плане возможных сервисов.  \r\n \r\nНо речь немного не об этом. В этом посте я расскажу о том, зачем вообще нужны системы клиринга и как проводится сделка между, например, Нидерландами и Москвой.  \r\n \r\nНачнём с того, что в ряде случаев биржи и банки, естественно, не меняются деньгами и бумагами сразу и мгновенно. Процедура очень похожа на кэширование на запись: чем делать тысячи одиночных Random IO, лучше записать сразу несколько блоков последовательно. Есть и максимальный допустимый объём долга за период — это объём кэша в нашем сравнении.  \r\n \r\nПлюс, конечно же, ни деньги, ни акции не попадают к конечным участникам сделки — они «физически» ложатся в депозитарии, а вы получаете «указатель» на них.  \r\n \r\n \r\nОни копят взаимные долги друг перед другом и закрывают их в момент завершения периода. Например, если речь идет о двух российских банках, то за день происходит множество операций между ними. К примеру, 40 клиентов банка А перевели в общей сложности 5 миллионов рублей в банк Б, а ещё 30 клиентов банка Б перевели за этот же день 4 миллиона рублей в банк А.  \r\n \r\n \r\n \r\nСоответственно, на конец операционного дня транзакций между банками будет не 70 штук на общую сумму 9 миллионов рублей, а всего одна на 1 миллион: \r\n \r\n \r\n \r\nЭто и делает клиринг. На самом деле, конечно, ситуация несколько сложнее, и участвует там не два банка, а все имеющиеся в клиринге. И возможны ситуации, когда банк А должен банку Б, банк Б должен банку В, банк В — банку Д, а банк Д — банку А. В итоге, как в известной задаче про 100 долларов, экономика начинает крутиться ещё до того, как по факту появляются деньги. А клиринг делает так, чтобы для каждого банка был минимум входящих и исходящих транзакций.  \r\n \r\n \r\nКак раз из-за такой архитектуры (упрощённо) взаимозачётов по сделкам ценными бумагами могут торговать только те, у кого есть лицензия на брокерскую деятельность. И именно поэтому вы, как физлицо, не можете взять и купить акций. Вы должны дать поручение своему брокеру (это может быть ваш же банк, например, ВТБ24, Альфа и Сбербанк имеют отличные инструменты для этого), а брокер уже купит нужные ценные бумаги.  \r\n \r\nВот так (опять же очень упрощённо) это выглядит: \r\n \r\n \r\n \r\nЕсли рассматривать брокеров в виде продакшн-серверов, вас в виде пользователя, биржу в роли роутера, а депозитарий ценных бумаг в виде системы хранения данных, будет понятно, почему нужен «финансовый кэш».  \r\n \r\n \r\nПредставьте себе, что вы решили купить акций Гугла у кого-то из Европы. Например, вы выставили заявку на покупку по определённой цене на бирже, а частный инвестор из Европы в этот момент решил продать несколько своих акций и как раз заключил сделку с вами.  \r\n \r\nНайти друг друга вам помогла биржа, которая сыграла с другими биржами в длинный роутинг и смогла пробросить между вами соединение.  \r\n \r\nПоскольку ценные бумаги можно рассматривать и как обычный физический товар наподобие картошки, дальше вы можете встретиться где-нибудь в лесу, заключить договор купли-продажи на двух языках, заверить переводы у нотариуса, проверить всё на соответствие законодательству двух стран и обменять бумаги на деньги. Деньги дальше пойдут по стандартному банковскому роутингу. Но это дорого, долго и совершенно никому не нужно, как правило.  \r\n \r\nПоэтому вы используете ту же самую биржу для заключения сделки. Ценные бумаги представляют собой, опять же очень грубо говоря, записи в базе данных с обозначением, что это, кто его выпустил и кому это принадлежит. В простом случае сделки внутри страны (даже внутри одного депозитария банка) достаточно просто поменять принадлежность акций и никуда их не двигать. \r\n \r\nНо разные страны и разные финансовые организации имеют разные депозитарии. Почему? Потому что именно депозитарий, по сути, является тем самым кэшем, который определяет возможный объём взаиморасчётов на конец периода. То есть если вы уже сделали сделок на 500 миллионов рублей в одну сторону, а вам должны 300 миллионов, но при этом ваш депозитарий ограничен всего лишь 500 миллионами, новые сделки по покупке вы совершать не сможете. Потому что с точки зрения финансовой системы прямо сейчас вы в нуле — и только после клиринга вы будете в плюс 200. Если будете. И надо либо ждать следующего периода, либо увеличивать кэш.  \r\n \r\nВ роли российского кэша выступает НРД — национальный расчётный депозитарий (НКО ЗАО НРД). Архитектурно он является частью Московской биржи и входит в её «кластер» как неотъемлемая часть.  \r\n \r\nОК, то есть есть роутеры-биржи и хранилища-депозитарии, так? Что тогда при расчёте с Германией? \r\n \r\n \r\n \r\n \r\n \r\n \r\nЕсть несколько таких систем, но мы для примера рассмотрим Euroclear (мы работаем со своими ETF для России именно через неё, поэтому я знаю мелкие детали лучше, чем по другим системам). Итак, Евроклир — бельгийский банк, на базе которого существует эта система — это, по факту,  .  \r\n \r\n \r\n \r\nС точки зрения НРД, мы рассчитываемся только с одной компанией — Евроклиром, а уже Евроклир как чёрный ящик разбирается, что и как роутить дальше. Каждая страна работает так же: есть депозитарий, куда Евроклир имеет, опять же упрощая, право на запись, и есть API, который позволяет отдавать ему поручения.  \r\n \r\nЕсли вы покупаете наши кластерные ETF у продавца в Нидерландах на бирже NYSE Euronext (на вторичном рынке), происходит довольно простая операция. С точки зрения Евроклира российский депозитарий дал задачу на перемещение ценной бумаги (ETF) к себе, а взамен обещал денег. Поскольку у нас режим DVP (поставка против платежа), Евроклир забирает ETF-бумаги из голландского депозитария и кладёт «в себя» (по факту ещё ничего не двигая, но работая с указателем). Затем он берёт деньги из нашего депозитария и также подтаскивает «в себя». То есть на этот момент не мы должны голландцам деньги, а голландцы нам акции, а мы должны деньги Евроклиру, а Евроклир нам акции. Аналогично — немцы должны Евроклиру акции, а Евроклир им денег.  \r\n \r\nРазмер долгов определяется размерами депозитариев. В DVP-режиме сделка не будет совершена, если в НРД нет достаточного количества свободных денег, а в депозитарии Голландии — достаточного числа нужных ценных бумаг. При этом НРД не поверит Евроклиру, если у него нет аналогичного ресурса. То есть каждый из участников гарантирует сделку своим «кэшем». Все операционные риски устраняет наш клиринг НКЦ (у него есть большие собственные средства, он делает оценку залога, оценку способности участников). НКЦ напрямую капитализируется ЦБ, чтобы минимизировать риски торговли через центрального контрагента. Естественно, НРД и НКЦ не верят вам, если свободный «кэш» вашего брокера меньше суммы сделки. \r\n \r\nПлюс внешний финансовый контроль, плюс другие оверхеды. \r\n \r\nЗатем в процессе клиринга все эти операции совершаются, и Евроклир быстро меняет акции на деньги, а затем не просто передаёт вам указатели на ресурсы, а «физически» меняет местами ресурсы в депозитариях. Вся эта процедура вместе с проверками занимает два стандартных операционных дня, и потому бумаги ETF покупаются в режиме Т+2. \r\n \r\n \r\nДа, одна бумага ETF представляет собой фактически кусок кластера, например, из огромного портфеля акций IT-компаний США или мировых IT-компаний. Но при этом все участники рынка работают с ETF как с одной ценной бумагой, а не пакетной сделкой, потому что виртуализация достигается ещё одним уровнем ниже. Фактически эмитент ETF — это юрлицо, которое купило акций IT-компаний по определённым правилам и выпустило свои акции. С точки зрения биржи, это одна компания, а с вашей точки зрения — индекс.  \r\n \r\n \r\nЧтобы транзакция прошла с точки зрения Евроклира, мы делаем DVP-расчёт через НРД с помощью НКЦ. НРД держит номинально большой счёт, а НКЦ делает расчеты для всех участников рынка.  \r\n \r\nПри необходимости более сложной сделки, например, с Америкой, возможна ситуация с появлением ещё одного ранга сети — это когда Евроклир прокидывает мост до аналогичной американской системы.  \r\n \r\nВот так всё хитро закручено.  \r\n \r\nЧто это значит в итоге простыми словами? \r\n \r\nЕстественно, всё не совсем так, гораздо сложнее и веселее (простите ограниченность этих сравнений). Но, думаю, в общих чертах задача интеграции ясна. \r\n \r\nТак вот, пять лет назад нам надо было сделать так, чтобы ETF начали листинговаться на Московской бирже, то есть могли быть «совместимы» с НКЦ, то есть могли бы входить в НРД. Этого нужно было добиться и юридическими, и IT-средствами. Поэтому в следующем посте я расскажу о том, как мы всё это подключали и автоматизировали документооборот.  \r\n \r\n \r\n \n\n      \n       \n    ", "hub": "Платежные системы / Интересные публикации / Хабрахабр", "title": "Как обеспечивается «совместимость» финансовых сделок на грубых IT-примерах"}
{"date": "18 февраля 2014 в 13:10", "article": "\n      Друзья! Всем доброго дня! \r\n \r\nМы хотим не только пиарить себя и свои услуги здесь, но и поделиться опытом и знаниями, полученными в ходе администрирования огромного количества проектов, и в конечном итоге сделать достойный и полезный блог. Для этого мы попросили наших инженеров поучаствовать в этом. А на будущее хотим понять: будет это интересно хабровчанам? И если да — то в какой форме?  \r\n \r\nПод катом в этот раз пойдет речь об  . \r\n \r\nМеню загруженного WinPE. \r\n \r\n \r\n \r\n \r\nКаждый   хороший системный администратор стремится автоматизировать как можно больше неинтересной ручной работы, дабы больше времени посвещать медитации и дзен.  \r\nПишутся скрипты, файлы ответов для различных визардов, настраиваются системы мониторинга и оповещения, системы управления конфигурациям… \r\nНичто не должно отвлекать на пути к просветлению. \r\n \r\nВ этой статье я опишу как подготовить образ среды предустановки Windows®   для загрузки с Linux   сервера.  \r\nУ нас он призван решать следующие задачи: \r\n \r\nНастройку серверной части PXE я опущу, т.к. тема достаточна хорошо описана в Internet(например  ). \r\nПо ходу повествования остановлюсь лишь на тех моментах, которые неообходимы в рамках данной статьи. \r\n \r\n \r\nДля подготовки образа WinPE потребуется компьютер с Windows 7 либо Windows Server 2008, а так же установленный пакет  \r\n \r\nКогда нужная ОС установлена и пакет инсталирован. \r\n \r\nМонтируем образ winpe.wim для внесения изменений, а так же подготовим загрузчик. \r\nВ открывшейся консоли последовательно вводим команды: \r\n \r\nЗапуск оболочки WinPE осуществляет файл    \r\nТак как меню у нас на русском языке, нужен редактор позволяющий менять кодировку текста(CP866 в нашем случае).  \r\nЯ использую Notepad++ \r\nПуть к файлу:  \r\nОтредактируем его для добавления меню, как на скриншоте выше. \r\n \r\nСохраняем изменения в  . \r\nОтмонтируем winpe.wim закомитив изменения и скопируем полученный образ к остальным файлам. \r\n \r\nТеперь займемся настройкой Данных Конфигурации Загрузки ( ) \r\n \r\nСоздадим новый скрипт createbcd.cmd. Он обеспечит генерацию нужной BCD. \r\nВновь открываем текстовый редактор и вставляем туда следующий код: \r\n \r\nЗапускаем скрипт. \r\n \r\nПриготовим файл ответов autounattend.xml для автоматической установки Windows Server 2008. \r\nНа первом диске сервера создается один раздел, занимающий все пространство диска. ОС ставится в этот раздел, устанавливается пароль для учетной записи Administrator, вводится ключ, но Windows не активируется.  \r\n \r\nФайл нужно скопировать в корень smb шары, которая монтируется скриптом  . \r\nПодготовительный этап в Windows близится к завершению, остается скопировать папку   в корень tftp сервера, а так же разместить в папке доступной для монтирования по протоколу sbm файлы с установочного диска/образа Windows Server 2008. \r\n \r\n \r\n \r\nКонфигурационные файлы tftp. \r\n \r\n \r\n \r\n \r\nФрагмент начала установки \r\n \r\n \r\nНа этом все. \r\nБлагодарю за внимание! \r\n \r\n \r\n1.  \r\n2.  \r\n3.  \r\n \r\nАвтор статьи и по совместительству инженер   —  \n\n      \t \n\n       \n    ", "hub": "Серверное администрирование / Интересные публикации / Хабрахабр", "title": "Установка Windows Server 2008 по сети с Linux PXE сервера. Кастомизация образа WinPE"}
{"date": "18 февраля 2013 в 10:32", "article": "\n       Некоторое время назад решил заняться разработкой программ для iOS. Хотелось написать что-нибудь для души и в качестве первого проекта я выбрал приложение для моего любимого сайта habrahabr.ru, так как на тот момент удобной программы для чтения постов сайта, удовлетворяющей моим требованиям, не было. Я представил себе какими свойствами должно обладать приложение, которым бы стал пользоваться ежедневно, и у меня получился следующий список:  \r\n \r\n \r\n \r\nСобственно речь и пойдет о том, как я решал поставленные задачи и что в итоге получилось. \r\n \r\n \r\n \r\n \r\nПрежде всего решил подготовиться теоретически: посмотрел  , прочитал книжку «Программируем для iPhone и iPad» Пайлонов, изучил имеющиеся в App Store магазине книжки по разработке для iOS. Сразу хочу сказать, что делал я все не совсем правильно, или даже совсем не правильно. Курсы очень понравились: докладчик обладает чувством юмора, явно знает и любит то, о чем рассказывает. Однако эти курсы лучше было бы слушать и при этом параллельно применять то, о чем рассказывают. В моем же случае, многое из того что было рассказано «выветрилось» к тому времени, когда дело дошло до практического использования полученных знаний. Книгу покупал под впечатлением от неплохого издания по шаблонам проектирования из той же серии, но она меня абсолютно разочаровала — слишком много воды и банальности. Книги по разработки для iOS, скачанные в App Store я явно читал не в том порядке. Несколько позже Apple выпустила отличное  , которое я советую всем начинающим iOS-программистам.  \r\n \r\nВ итоге на подготовительную часть было потрачено значительно больше времени, чем требовалось. \r\n \r\n \r\n \r\nОдним из самых важных и удачных решений, принятых в начале разработки было использование   для управления зависимостями. С использованием cocoapods подключение новой библиотеки требует совсем немного времени. Из-за 15-летнего опыта разработки коммерческих С++ программ у меня было большое искушение написать всю модель на «родном» языке, но я себя удержал, решив, что если уж изучать новую платформу, то делать это до конца. В итоге, удивляясь «топорности» местных контейнеров, выпил чашу до дна, написав абсолютно все на Objective C. Возможно, в следующих проектах я все-таки попробую использовать в модели boost ( библиотека есть в cocoapods ) — подозреваю, что процесс пойдет значительно быстрее.  \r\n \r\nОдним из главных требований при разработке была поддержка habracut и возможность уже в списке постов почитать начало статьи, посмотреть картинки и видео. Я решил эту задачу, используя стандартный класс UITableViewCell с переменной высотой. Каждая ячейка отображается при помощи Core Text, то есть в этом месте я полностью отказался от встроенного браузера UIWebView. Так выглядит результат для iPad в горизонтальной ориентации: \r\n \r\n \r\nв вертикальной ориентации: \r\n \r\n \r\nВ горизонтальной ориентации для iPhone: \r\n \r\n \r\nВидно, что TabBar для горизонтальной ориентации перемещен влево, что позволило сэкономить ценное пространство по вертикали, да и пальцами левой руки переключать вкладки при таком расположении удобнее.  \r\n \r\nДля iPad отдельно надо было решить проблему отображения списка хабов. От стандартного UISplitViewController пришлось отказаться — я так и не смог найти простое решение показывать/скрывать master view (список хабов) с применением анимации и воспользовался одной из многочисленных библиотек с поддержкой sliding views. В результате получилось примерно так: \r\n \r\n \r\nЭкран для чтения статьи хотелось оставить максимально свободным. Изначально показывается только ToolBar, но можно переключиться в полноэкранный режим (ToolBar и StatusBar будут скрыты). В списке управляющих элементов осталось только самое необходимое: \r\n \r\nВот как это выглядит для iPhone: \r\n \r\n \r\nверсия для iPad: \r\n \r\n \r\nвариант полноэкранного режима: \r\n \r\n \r\nДля удобства управления были добавлены различные жесты: \r\n \r\n \r\n \r\nДля синхронизации между устройствами естественно используется iCloud. Благодаря тому, что для списка прочитанного передаются только номера постов, а для списка избранного только начало каждого поста (без картинок), удалось добиться минимального использования траффика. Само приложение бесплатно, но в списке постов прикручена реклама, которую можно отключить через in-app покупку. Никаких функциональных ограничений для бесплатной версии нет.  \r\n \r\n \r\nВ зависимости от того, насколько будет всем интересно, планирую написать еще про технические моменты разработки. Так же существуют большие планы по улучшению приложения: \r\n \r\n \r\nОгромная просьба писать о пожеланиях, замечаниях, оставлять отзывы и хорошие рейтинги, что бы была мотивация улучшать приложение дальше. \r\n \r\nСамо приложение можно найти по   адресу.\n\n      \n       \n    ", "hub": "Я пиарюсь / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 14:05", "article": "\n       \r\nВо дворе мы играли в войнушку. Слабые и толстые были фашистами, остальные их побеждали. Двор и детство исчезли, а воевать хочется. \r\n  \r\nЯ превратил свой iPhone в автомат, а фашистов нарисовал в дополненной реальности.  \r\nВидеозахват рисует мир вокруг моего рабочего кресла, фашисты лезут из всех щелей, я держу круговую оборону.  \r\n \r\nСтоп! А как привязать врагов к окружающей действительности?  \r\n \r\nЯ сделал это очень просто.  \r\n \r\n \r\n \r\nПосмотрите на первую картинку видеозахвата.  \r\n \r\n \r\n \r\nЯ пробегаю по всей ширине (480 пикселей) изображения и суммирую RGB компоненты точек, лежащих с текущей точкой на одной вертикальной прямой. Получаем массив из 480 элементов.  \r\nНа верхней части рисунка изображена красно-белая (оле!) гистограмма найденной функции. \r\n \r\nТо же самое проделываем со следующим кадром из видеопотока. Посмотрите на рисунок 2. \r\n \r\n \r\n \r\nМоя задача — совместить красно-белые гистограммы первого и второго кадра. Сдвигая графики друг относительно друга, я очень быстро нахожу оптимальное совпадение. Разница в сдвиге и есть искомое смещение реальности в моем iPhone в горизонтальном направлении. \r\n \r\nТаким, образом виртуальный объект навсегда привязывается к реальному местоположению. В каждый момент времени мы знаем, насколько пикселов его сдвинуть в горизонтальной плоскости.  \r\nХорошо ли работает алгоритм? Очень неплохо. Главное, не дергать телефон во время игры — данный подход не любит смещения более 40 пикселов за такт. А такт у камеры 20 кадров в секунду. \r\nПри резких движениях надо, видимо, использовать уже тяжелую артиллерию — гироскоп и акселерометр. \r\n \r\n \r\nКаждое оптическое устройство имеет FOV (field of view). Будь то человеческий глаз или мыльница.  \r\n \r\nЧто в FOV попадает — прибор видит. Прочее — нет, хоть убей. На картинке человек видит дерево, а машину не видит. Это может быть опасно. \r\nFOV измеряется в градусах. У человека FOV около 120-ти градусов для каждого глаза. У зайца, например, 150. То есть двумя глазами заяц покрывает почти всю сферу видимости. Не видит заяц только ровно вперед 10 градусов и ровно назад 10 градусов. Косой… \r\nЛадно, а какой FOV у моего iPhone? \r\nЗаглянул, разумеется, в  . Бог мой, чтобы вычислить FOV iOS устройства, необходима специальная лаборатория с лазерными измерителями. \r\n \r\nУ меня нет такой лаборатории. А зачем? Я просто запустил вышеописанную программу. Сел на любимое вращающееся кресло. И повернулся на 360 градусов. Программа выдала размер сферы вокруг меня. \r\n \r\nПрограмма выдала размер сферы вокруг меня. \r\nВ пикселах — стабильно для iPod последнего поколения 3050-3060 пикселей. \r\n \r\nТаким образом FOV моего iPod равен 480*360/3060 = 56-57 градусов. \r\n \r\nА лазер намерял 55.7. \r\n \r\nНеплохая точность у меня получилась, согласитесь. \r\n \r\nСкрытая реклама моего приложения, использующего вышесказанный алгоритм, помещена в теги. Только это между нами, ок?\n\n      \n       \n    ", "hub": "Работа с видео / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 20:12", "article": "\n        В связи с узурпацией должности управдома нашего МКД, и необходимости «причесать» общественный беспорядок, потихоньку конструирую систему видеонаблюдения. \r\nРазумеется, финансирование минимальное, планы — грандиозные, поэтому собирается всё из подножного корма. \r\nПодробности чуть позже, а вот один из интересных багов, который заставил перебрать много чего. \r\n \r\nИтак, началось всё с малого — простенькая купольная камера, бралась самая паршивая по принципу \" \". После пошел процесс перебора софта (тема отдельного поста, будет, опять же, позже)… В конце зафиксировался на Macroscop в один канал. Стояла пару месяцев, есть-пить не просила, наркоманов гонять помогала. \r\n \r\nИ вот работает вроде всё, но как-то подлагивает периодически. Грешил на всё: на проц, на сеть, на софт, на погоду на марсе… Саппорт по логам говорит, что-де камера периодически отваливается. \r\n \r\n \r\n \r\nДа были закуплены еще камеры, качеством и ценою значительно выше. Поборовши лень в три прекрасных дня в разные недели было это всё смонтировано на 1м этаже, заведено в комп, подключено. И пошла вторая итерация перебора софта, так чтоб на камер побольше, ценою гуманней, интерфейс удобней… После второго раунда перебора, пока стоит AxxonNext. Да вот только лаги вышли на новый уровень: камеры отваливаются чуть ли не синхронно каждые 5-10 минут если включить TCP; а если держать UDP, то артефакты лезут как не знаю что. \r\n \r\nПожаловался я в жуйке на эту тему, а    возьми да и скажи, что это стабильный баг китайских камер, где прошивка создана их собственными силами: если клиент не успевает выгребать, то в поток лезет мусор. Так как продаван на ali прикормленный, было решено попытаться решить проблему капитально. Несколько раз разными словами пытался баг описать, но понял, что английский у нас обоих оставляет желать лучшего, поэтому надо просто «show code». \r\n \r\n \r\n \r\nИтак, сперва берём tcpdump, и ждём ситуацию обрыва. Ждать недолго, за 5 минут поймалось аж 3 штуки. Как понять что произошло? Поток в полтора миллиона пакетов… Для начала фильтруем, оставляя только одну камеру. Затем Ctrl+F => tcp.flags.syn==1 => находим начало реконнекта, откуда листаем вверх, чтобы понять что случилось… \r\n \r\n \r\n \r\nНаблюдаем, что коннект был закрыт со стороны компа… Вот только смущает что перед этим Win=11460, Win=10200, Win=8940 — то есть, похоже, клиент и правда не успевает. Стоп-стоп-стоп. Как это не успевает? AMD Phenom II X6 1100T? Не, странно. IO? Так запись на отдельный диск, и не упёрлось в полку. Да и тогда была бы зависимость от просмотра, например — а её нет… В общем, листаем еще выше чуть-чуть: \r\n \r\n \r\n \r\nТак буквально секунду назад вообще в ZeroWindow уходили. Точно не успевает. Но почему обрыв-то?! Листаем ниже, смотрим другой обрыв… \r\n \r\n \r\n \r\n \r\nСмотрим — совсем нехорошо себя повел TCP стек (камеры? вероятно...), после чего RTP поток сбился — в один прекрасный момент вместо DynamicRTP-Type-96 пошли RTSP Continuation. \r\n \r\nИтак, делаем вывод: всё что нужно для симуляции, это запросить RTP поток, немного вытянуть, а потом сделать sleep(), и смотреть сломается ли поток. \r\n \r\n \r\n \r\nКак поступить, когда надо быстро накидать тестовый кусок? Взять скриптовый язык, набор готовых библиотек, слепить всё это вместе, радоваться. Лезем в гугль. Python+ONVIF. Тухло. Ruby+ONVIF. О, есть  . ОК, берём. URL стрима поймали. Отлично, а если :protocol покрутить?.. UDP, HTTP, TCP… итог один — отлично, по onvif ловить URL научился, теперь его бы слить. \r\n \r\nCurl? Не жуёт. Ладно, Ruby + RTSP… Ура,   либа. Скушиваем ему урл, и облом. Пытается авторизоваться исключительно через Basic. Еще немного гугля. Облом. Тогда остаётся один метод — напильник.  , по пути матерясь на объёмы магии руби (кто мне скажет, как правильно сделать «честную» рекурсию, чтобы работал и return и yield? правильно, а как догадаться по описанию функции, что оно может потребоваться? правильно… проще избавиться от неё — и чище код, и шелковистее волосы). \r\n \r\nИ снова ликуем, rtsp_client работает. Открываем во втором окне tcpdump… Блин! Я же в ONVIF запрашивал :protocol=>«TCP». Что за черт, почему UDP? \r\n \r\nПаяльник в руки… Ха! UDP прибит гвоздями в lib/rtsp/client.rb@request_transport. Так, впаиваем туда теми же гвоздями /TCP. Запускаем — падает. Почему? Куда? Ага, он требует наличия client_port… Какой client_port, если это TCP? Хардкодим rtp_port если его нет в 554. Так, IP надо сервера — хардкодим. Опа, не может, говорит, при'bind()'иться на 554 порт не рутом. Логично. Так, а зачем? Ну-ка… Это в  … Смотрим на   в режиме :TCP и удивляемся — а зачем ему TCPServer для Receiver? Что-то не то. Явно, вот явно на TCP никто не отлаживал. \r\n \r\nПосле пары минут попыток понять логику, до Зоркого Глаза дошло, что стенки-то нет:  \r\n \r\n \r\nНу-ка ну-ка… что за interleaved? Google: rtp interleaved =>   => Find on page «interleaved» => Ага! Так RTP и RTSP валятся в этом же самом коннекте! \r\n \r\nВыкидываем rtsp либу, так как для моей задачи править либо откровенно не хочется — тут надо явно по-хорошему уже переделывать архитектуру. \r\n \r\n \r\n \r\nИтак, мы снова в нуле: у нас есть rtsp:// урл камеры, есть необходимость его слить и поиграться во время слива с коннектом… Стоп! Сначала. Есть урл. Есть камера. Есть задача воспроизвести на ней баг. Зачем мне ONVIF? Зачем RTSP либы? Надо просто запросить и качать ответ. \r\n \r\nСказано — сделано. DESCRIBE? А зачем он нам… Нам надо только сперва SETUP, откуда забрать Session: затем PLAY с ней. \r\n \r\nПервый же опыт показал, что ему даже авторизация тут не требуется. \r\nОтлично! Первый же блин выстрелил сразу:   прекрасно воспроизводил баг — после sleep()'а поток ломался на ура. \r\n \r\nНо чтобы поиграться с TCP Window надо бы иметь возможность задать TCP_WINDOW_CLAMP. А для этого надо сделать setsockopt ДО connect но после создания сокета. \r\nА как это в руби сделать? Эм… Заглядываем в гугль… Пусто. Заглядываем в   — init_inetsock_internal… фиг там! сперва создаём rsock_socket(), а затем сразу rsock_connect(). Блин. \r\n \r\nЛадно, я всё равно руби не очень люблю. За 2 минуты переписываем  , добавляем setsockopt. Добавляем анализ номера RTP пакета. \r\n \r\nИтог анализа: от размера начального окна меняется объём инфромации, сколько успевает камера передать нормального, прежде чем сломается. Впрочем, неважно, закоммитил итог, и написал багрепорт продавцу, дабы они передали девелоперам прошивки. \r\n \r\nТак бы и успокоился, но    спрашивал можно ли как-то жить с этим багом. И это потребовало дополнительных раскопок. \r\n \r\n \r\n1) Можно отслеживать сбой и пересоединяться самостоятельно \r\n2) Можно отслеживать собственный лаг (не знаю как, но можно) и присоединиться сразу, не дожидаясь поломки потока \r\n3) Можно попытаться восстановить синхронизацию с потоком. \r\n \r\nИтак, втыкаем вместо вызова raise — вызов reconnect, и анализируем потери по разнице номеров RTP пакетов. Если действуем по пункту 1, то потери составляют около 1500-1800 RTP пакетов (примерно 600 пакетов на секунду sleep()). \r\n \r\nВтыкаем reconnect сразу после sleep(). Итог в точности такой же. \r\n \r\nВтыкаем ресинхронизацию методом поиска \"$\\x00\" — работает отвратно. Втыкаем ресинхронизацию методом поиска \"$\\x00[LEN]{len bytes}$\\x00\" — работает стабильно, потеря составляет в полтора раза меньше, чем при reconnect. Но самое главное — TCP соединение при этом не падает, а значит, алгоритм адаптации TCP Window и буферов приема продолжают работать. Вследствие чего, спустя 1-2 сбоя в начале соединения, поток просто перестаёт ломаться — sleep() продолжают регулярно усыплять клиента, а поток не падает. \r\n \r\nПолученный тест-скрипт  , но прекрасно выполняет функцию proof-of-concept. \r\n \r\nИ теперь, наконец-то, мы пришли к вопросу, заданному в заголовке.  \r\n \r\n \r\n \r\nМоё личное мнение — это реально лучше, чем рвать соединение целиком: номера RTP пакетов в коннекте есть, так что без проблем потерянный объём можно замерить. \r\nЕсли канал тоньше, чем пропускная способность сети — то потери будут расти, таким образом, обнаружив потери больше чем на 5 сек — можно спокойно жаловаться на толщину канала (реконнектиться с меньшим качеством, попросить поменять кабель, взорвать АЭС, или любой другой способ реакции на эту проблему); если же проблема в том, что приёмник просто не успевает по какой-то причине выгребать — resync поведение = наше щасте. Получаем объединение плюсов одновременно UDP и TCP подходов: если совсем что-то плохо — кусочек потеряли; в остальных случаях — ретрансмиты автоматические и проблем не доставляют. \r\n \r\nНу и на закуску, поговорим о том, что за баг в прошивке… А баг прост: send(somedata, somelen) возвращает <0 в случае ошибки, число байт, если отработало. И любмая ошибка всех начинающих сетевых разработчиков: send(somedata, somelen) может вернуть что-то МЕНЬШЕ, чем somelen — в буфер не влезло. \r\n \r\nЕсли это не обрабатывать, хвост от somedata просто теряется — его и не отправили, и выбросили. \r\n \r\n \r\n \r\nПочинить так: надо запомнить недоотправленный кусок, и прекратить посылать что бы то ни было, пока send() этого остатка не отправит всё целиком. После этого надо начать посылать _следующие_ пакеты (выбросив те, которые были всё время, что буфер был занят). Тогда мы получим то самое поведение помеси TCP и UDP, но без необходимости клиенту заниматься магической ресинхронизацией с границами пакетов. \r\n \r\nНадеюсь, производители поправят этот баг, и через некоторое время, все китайские прошивки для китайских камер будут радовать корректной работой без единого разрыва&tm; \r\n \r\n \r\n \r\nСаппорт    обратил моё глупое невнимание на тот факт, что LIVE555 анонсируется 2011 года… Так что через binwalk+dd+mount заглянул в потроха прошивки. rtsp_streamer и правда, от 2011 года. \r\nЗаглянул в diff между двумя версиями live.2013.10.09.tar.gz и live.2014.02.19.tar.gz: \r\n \r\n \r\nА вот вырезка из changelog.txt: \r\n \n\n      \n       \n    ", "hub": "Работа с видео / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 01:36", "article": "\n      Сообщество Wikipedia   по вопросу использования видеороликов в формате MP4. С более чем двукратным превосходством решено отказаться от поддержки этого проприетарного формата: 309 голосов «против» и 145 голосов за полную поддержку MP4 (+4 голоса за частичную поддержку только для просмотра и +56 голосов за частичную поддержку только для закачек нового видео). \r\n \r\nПротивники MP4 объяснили свою точку зрения. Для того, чтобы создавать по-настоящему свободный продукт — Википедию, чтобы гарантировать её полную открытость и свободу в будущем, нужно использовать только свободные кодеки. Иначе в будущем может сложиться ситуация, когда для показа видео на Википедии необходимо будет получить лицензию на использование патента в той или иной форме. \r\n \r\nС этой точки зрения, любое программное обеспечение, которое используется в инфраструктуре проектов Wikimedia, должно соответствовать общественным нормам, касающимся интеллектуальной собственности, патентного статуса, лицензирования и проч. \r\n \r\nТекущие требования сообщества заключаются в том, что свободные/открытые стандарты должны всегда использоваться для кодирования и хранения видеофайлов на серверах, чтобы хранить данные, так что и контент и программное обеспечение могут распространяться без каких-либо ограничений. Фирменные видео контейнеры или кодеки, такие как MP4, не допускаются на проектах Wikimedia, потому что они запатентованы и их программное обеспечение не может быть свободно по содержанию лицензии (хотя содержимое MP4 само по себе и может быть свободно).\n\n      \n       \n    ", "hub": "Работа с видео / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 21:45", "article": "\n       \r\nДанная статья посвящена вопросу интеграции CRM-систем с серверами телефонии на базе Asterisk. \r\nВ статье   рассматриваются вопросы, связанные с настройкой сервера Asterisk или нюансами работы CRM-систем, рассматриваются лишь общие варианты организации взаимодействия со всеми их плюсами и минусами. \r\n \r\n \r\nКак компании «доходят» до разработки CRM «под себя» и собственной телефонии — вопрос скорее политический и бизнесовый, чем технический, поэтому на вопросы «зачем» ответов дать не смогу. Но факт остаётся фактом — в один прекрасный день нам понадобилось решение, способное обеспечить взаимодействие телефонной части с CRM. \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nПервое, что пришло в голову — реализация «звонилки» прямо в браузере (мы же помним, что взаимодействие с CRM осуществляется через веб-интерфейс). А тут как раз спецификация WebRTC подоспела с плагинами для популярных браузеров. \r\nИдея, несомненно, хорошая, но для бизнеса не подходящая в силу того, что при подвисании / закрытии браузера или обновлении страницы мы теряем коннект, звонок и клиента. \r\nПо тем же причинам не был выбран ни один из «классических» SIP веб-клиентов. \r\n \r\n \r\nИдея с отдельным SIP-телефоном обладает очевидным для бизнеса преимуществом — практически в любой непредвиденной ситуации мы сохраняем звонок. Возникают сложности с обеспечением взаимодействия с CRM, однако это решаемая задача. \r\n \r\n \r\n \r\nСкажите, как заставить сипфон на компьютере сделать исходящий звонок из браузера? Никак? Совершенно верно. \r\nИменно поэтому мы стали посылать команду астериску через AGI-интерфейс, по которой он организовывал исходящий вызов до абонента, а затем подключал к каналу оператора. Так для сипфона исходящий звонок стал входящим с командой автоответа от сервера телефонии. \r\n \r\nВсё стало хорошо, оператор, начинал работу с некой сущностью CRM-системы (для простоты назовём её «Звонок»), далее вручную или автоматически отправлялась команда инициации «исходящего» звонка через agi, а по завершению звонка asterisk присылал нам информацию о звонке (вариантов много, начиная от GET-запроса и заканчивая WebSocket). \r\n \r\nЭтап внедрения исходящей телефонии в России прошел не без шероховатостей, но довольно успешно, после чего было необходимо начинать работу со входящими звонками. \r\n \r\n \r\n \r\nТут уже пришлось завязаться на работу Asterisk непосредственно с браузером, по крайней мере до успешного принятия звонка, в результате чего на сервере телефонии появился еще и Web Socket сервер. \r\nТаким образом информация о поступлении входящего звонка к оператору поступала непосредственно в браузер, а остальные команды шли в Asterisk через сервер CRM и затем через AGI интерфейс. \r\n \r\nРешение организовать управление телефонией через сервер CRM было связано со следующими требованиями: \r\n \r\n \r\nВышеописанная схема достаточно хорошо работала в России, однако когда КЦ в Эстонии стал осуществлять звонки через CRM-систему мы обнаружили большое количество проблем со звонками, вызванных не корректной работой AGI. Пакеты от Эстонского Asterisk не доходили до нашего сервера CRM, в результате чего мы не получали никакой информации о выполнении команды, не знали дошла ли команда вообще. \r\n \r\nЧерез неделю стало очевидно, что ситуация не улучшится, повлиять на качество связи между Российским и Эстонским серверами мы не могли, в результате чего было принято волевое решение о полном переходе на управление телефонией через Web Socket. \r\n \r\nЕще через неделю мы получили работающую схему, где у каждого Asterisk был свой Web Socket сервер, с которым браузер оператора связывался в пределах одной локальной сети. Сервер CRM как был, так и остался в РФ. \r\n \r\nТаким образом связь интерфейса оператора с сервером телефонии происходит в пределах одной локальной сети без задержек и все команды управления телефонией и события отправляются и приходят обратно в браузер оператора. \r\nВзаимодействие CRM и телефонии непосредственно происходит через callback'и от Asterisk и опосредованно через асинхронные запросы из операторской панели. \r\n \r\n \r\n \r\n \r\nP.S. К сожалению, чукча не очень художник и не обладает достаточным количеством свободного времени, чтобы нарисовать общие схемы взаимодействия прямо сейчас, но обещает сделать это, если читателей заинтересуют затронутые в статье вопросы.\n       \n    ", "hub": "Разработка систем связи / Интересные публикации / Хабрахабр"}
{"date": "18 февраля 2014 в 10:21", "article": "\n       \r\n \r\nКак известно, SIP набирает всё большую и большую популярность и не для кого не секрет, что можно оптимизировать затраты на телефонию грамотно выбрав SIP-провайдера. \r\n \r\nНо, к сожалению, выше приведенное утверждение верно только для крупных городов России и СНГ. А как же поступать в случаях, где SIP недоступен? Использовать PRI? Довольно дорого, тем более для маленькой компании с небольшим количеством звонков, да и опять же не везде доступно. Использовать аналоговую телефонию по старинке? Безусловно, нет. Ответ кажется довольно очевидным: GSM-шлюз решит все ваши проблемы. \r\n \r\nНа первый взгляд, все довольно просто в реализации, однако при более детальном продумывании станет понятно, что шлюз сам по себе мало функционален и является лишь своего рода преобразователем GSM в SIP, а для реализации пусть и незаоблачных, но все-таки “хотелок” необходима АТС (например IVR, маршрутизация, очереди). Для минимизации процесса затрат допустим, что это наш любимый open-source Asterisk. Если речь идет уж о совсем глобальной экономии, то можно сразу сказать: “Ага, а сервер под Астериск? Он тоже не бесплатный!” (Хотя по-моему мнению у запасливого админа всегда найдется железка под Астер:)) \r\n \r\nА теперь представьте, что все выше написанное можно реализовать всего лишь средствами одного GSM-шлюза? Невероятно? Что ж, прошу под кат. \r\n \r\n \r\nПару месяцев назад   писал статью о новых GSM-шлюзах компании  , которая заканчивалась вполне оптимистичной и подстегивающей к действию фразой: “Коллеги, в нем полноценный Asterisk, я даже не удивлюсь, если на нем удастся настроить IVR и очереди, и использовать в качестве полноценной АТС.” \r\n \r\nА действительно? Возможно ли реализовать функции АТС средствами шлюза? \r\n \r\nКоллеги, сразу оговорюсь, что данное решение подходит для небольшой компании, скажем со штатом в 10 сотрудников. \r\n \r\nИтак, для начала оценим, какие функции АТС нужны небольшой компании? По моему мнению, это очереди, IVR, перевод звонка. \r\n \r\n \r\n \r\nПереходим во вкладку SIP → SIP Endpoints, нажимаем “Add New SIP Endpoint” \r\n \r\nName = 777 \r\nUser Name = 777 \r\nPassword = YourStrongPasssword123 \r\nRegistration = Endpoint registers with this gateway \r\nHostname or IP Address = dynamic \r\nTransport = UDP \r\nNAT Traversal = Yes \r\n \r\nНажимаем “Save” \r\n \r\n \r\n \r\nДалее по аналогии создаем необходимое количество SIP-пиров. \r\n \r\nТеперь в веб интерфейсе включим доступ по SSH. Для этого зайдем во вкладку System → Login Settings и поставим в SSH Login Settings “Enable ON” \r\n \r\n \r\nСтоит отметить, что порт для SSH в шлюзе 12345, изменить его нельзя, что видимо сделано для целей обеспечения безопасности. \r\n \r\nПо умолчанию пользователь admin не имеет root-овых прав, для того, чтобы получить root-а, необходимо изменить пользователя на super, пароль super. \r\n \r\n \r\nИтак, заходим на шлюз по SSH:  \r\nPassword: admin \r\n \r\nПосле того, как вы попали на шлюз, правим контекст конфигурационного файла extra-channels.conf: \r\n \r\n \r\n \r\n \r\n \r\nТеперь создадим новый контекст ivr. \r\n \r\n \r\n \r\n \r\n \r\nДиалплан в примере самый примитивный, предназначен для демонстрации, в продакшине, конечно, должна быть реализована защита от неправильно введенных цифр. \r\n \r\nСтоит отметить, что Openvox поддерживает звуки для IVR только в формате GSM. У меня были звуки в формате .wav, которые я перекодировал средствами системы: \r\n \r\n \r\n \r\nПо умолчания в Openvox не загружены модули gsm, надо загрузить их принудительно в CLI: \r\n \r\n \r\n \r\nДля того, чтобы не загружать gsm каждый раз после перезагрузки шлюза, рекомендую добавить две эти строчки в файл /etc/asterisk/modules.conf \r\n \r\n \r\n \r\nЗаходим в веб-интерфейс шлюза, вкладка Routing → Groups. Создаем новую группу щелкнув по кнопке New Group: \r\n \r\nGroup Name = YourGroupName \r\nType = SIP \r\nPolicy = выбираем подходящую вам стратегию обзвона \r\nMembers = ставим галочки в чекбоксах напротив тех номеров, которые мы хотим включить в группу: \r\n \r\n \r\nТеперь переходим во вкладку Routing → Call Routing Rules, нажимаем New Call Routing Rule: \r\n \r\nRouting name = YourRoutingName \r\nCall Comes in From = например, gsm-2 (то есть все звонки с sim 2 будут перенаправляться в нашу очередь) \r\nSend Call Through = из выпадающего списка выбираем только что созданную группу. \r\n \r\n \r\n \r\nЕще одной важной функцией является возможность осуществления перевода звонка. К нашей великой радости это уже предусмотрено разработчиками Openvox. Откроем feauteres.conf: \r\n \r\n \r\n \r\nИ видим там следующую строку: \r\n \r\n \r\n \r\nТо есть по нажатию #1 будет осуществлен трасфер звонка, в принципе можно выбрать любой префикс для перевода, я для удобства поменял на *1. \r\n \r\n \r\n \r\nИспользуя недокументированные возможности данного устройства, в результате мы получаем следующую калькуляцию затрат для типичного малого бизнеса:  \r\n \r\n1) 10 IP телефонов ~ 25 тыс. рублей. (не самые дешевые аппараты, но с поддержкой HD кодеков, гарнитуры, и вероятно даже PoE) \r\n \r\n2) OpenVox VS-GW1200-4G ~ 15 тыс. рублей \r\n \r\n3) Если вам необходимо подключить внешнюю аналоговую линию и, скажем, факс, то можно приобрести недорогой FXO / FXS шлюз, например Granstream HT-503 ~ 2.5 тыс. рублей. \r\n \r\n4) Как было сказано выше, шлюз умеет и регистрировать на себе конечные SIP-устройства, так и сам регистрироваться на АТС, поэтому с подключением SIP-провайдера не возникнет проблем \r\n \r\nИтого, полноценный VOIP в офисе, с анлим транками, очередями и блекджеком менее чем за 45 000 рублей! \r\n \r\nШлюз не имеет жестких дисков, и использует только качественные компоненты (в этом мы могли убедиться в   про OpenVox), поэтому является крайне надежным решением. \r\n \r\nБолее того, этому решению не требуются никакие доп. лицензии, подписки на тех. поддержку и прочее, поэтому общая стоимость владения вполне быстро просчитывается и можно забыть о всякого рода подводных камнях с лицензированием.\n\n      \n       \n    ", "hub": "Разработка систем связи / Интересные публикации / Хабрахабр"}
{"date": "18 февраля в 21:32", "article": "\n       \r\n \r\nЗдравствуйте, меня зовут Дмитрий. Я занимаюсь созданием компьютерных игр на Unreal Engine в качестве хобби. Для своего проекта я разрабатываю продцедурно генерируемый уровень. Мой алгоритм расставляет в определенно порядке точки в пространстве (которые я называю корни «roots»), после чего к этим точкам я прикрепляю меши. Но тут возникает проблема в том, что нужно с начала прикрепить меш потом откомпилировать проект и лиш после этого можно увидеть как она встала. Естественно постоянно бегать из окна редактора в окно VS очень долго. И я подумал что можно было-бы для этого использовать редактор blueprint, тем более мне попался на глаза плагин Dungeon architect, в котором расстановка объектов по уровню реализована через blueprint. Собственно здесь я расскажу о создании подобной системы скриншот из которой изображен на первом рисунке.  \r\n \r\n \r\nИтак с начала создадим свой тип файла (подробней можно посмотреть вот эту  ). В классе AssetAction переопределяем функцию OpenAssetEditor. \r\n \r\n \r\nТеперь если мы попытаемся открыть этот файл будет открыто не привычное окно, а то окно, которое мы определим в классе FCustAssetEditor. \r\n \r\nСамым важным для нас методом этого класса явлется InitCustAssetEditor. Сначала этот метод создает новый редактор о чем ниже, потом он, создает две новые пустые вкладки: \r\n \r\nОдна из этих вкладок будет вкладкой нашего блюпринт редактора, а вторая нужна для отображения свойств нодов. Собственно вкладки созданы нужно их чем-то заполнить. Заполняет вкладки содержимым метод RegisterTabSpawners \r\n \r\nПанель свойств нам подойдет стандартная, а вот bluprin редактор мы создадим свой. Создается он в методе CreateGraphEditorWidget. \r\n \r\nЗдесь с начала определяются действия и события на которые будет реагировать наш редактор, а потом собственно создается виджет редактора. Наиболее интересным параметром является .GraphToEdit(InGraph) он передает указатель на класс UEdGraphSchema_CustomEditor \r\n \r\nЭтот класс определяет такие вещи как пункты контекстного меню редактора, определяет как будут соединятся между собой ноды и т.д. Для нас самое главное это возможность создания собственных нод. Это делается в методе GetGraphContextActions. \r\n \r\n \r\nКак вы видете пока что я создал только четыре ноды итак по списку: \r\n1)Нода URootNode является отображением элемента корень на графе. URootNode также как и элементы типа корень имеют тип. \r\n2)Нода UBranchNode эта нода размещает на уровне статик меш (пока только меши, но можно легко создать ноды и для других элементов обстановки или персонажей) \r\n3)Нода URuleNode эта нода может быть либо открыта либо закрыта в зависимости от заданного условия. Условие естественно задаются в blueprint. \r\n4)Нода USwitcherNode эта нода имеет один вход и два выхода в зависимости от условия может открывать либо правый выход либо левый. \r\n \r\nПока только четыре ноды но если у вас есть идеи можете написать их в комментарии. Давайте посмотрим как они устроены. (Для экономии места я приведу здесь только базовый для них класс, исходники можно скачать по ссылке в конце статьи) \r\n \r\n \r\nЗдесь мы видим метод GetChildNodes в котором нода передает массив объектов присоединенных к её выходам. И метод CreateNodesMesh в котором нода создает меш или не создает а просто передает дальше значения AbsLocation и AbsRotation. Метод PostEditChangeProperty как вы наверно догадались выполняется когда кто-то меняет свойства ноды. \r\n \r\nНо как вы наверно заметили ноды на заглавном рисунке отличаются по внешнему виду от тех, которые мы привыкли видеть. Как же этого добиться. Для этого нужно создать для каждой ноды класс наследник SGraphNode. Как и в прошлый раз здесь я приведу только базовый класс. \r\n \r\n \r\n \r\nНаследование класса FNodePropertyObserver нужено исключительно для метода OnPropertyChanged. Самым важным методом является метод UpdateGraphNode именно в нем и создается виджет который мы видим на экране, Остальные методы вызываются из него для создания определенных частей этого виждета.  \r\n \r\nПрошу не путать класс SGraphNode с классом UEdGraphNode. SGraphNode определяет исключительно внешний вид ноды, в то время как класс UEdGraphNode определяет свойства самой ноды. \r\n \r\nНо даже сейчас если запустить проект ноды будут иметь прежний вид. Чтобы изменения внешнего вида вступили в силу, нужно их зарегистрировать. Где это сделать? Конечно же при старте модуля: \r\n \r\nХочу заметить что также здесь создается хранилище, для хранения иконок которые будут отображаться на нодах UBranchNode. Регестрация нодов происходит в методе CreateNode класса FGraphPanelNodeFactory_Custom. \r\n \r\n \r\nГенерация осуществляется в классе TestActor. \r\n \r\nЗдесь мы переберем в цикле все объекты root, каждый из них характерезуется координатой в пространстве и типом. Получив этот объект мы ищем в графе ноду URootNode c таким же типом. Найдя её передаем ей начальные координаты и запускаем метод CreateNodesMesh который пройдет по цепочки через весь граф. Делаем это пока все объекты root не будут обработаны. \r\n \r\nСобственно вот и все. Для дальнейшего ознакомления рекомендую смотреть исходники. \r\n \r\n   \r\n \r\nА я пока расскажу вам как же работает это хозяйство. Генерация осуществляется в объекте TestActor, с начала надо в ручную задать положения и типы объектов root (а что вы хотели проект учебный).  \r\n \r\nПосле этого выбираем в свойствах файл MyObject, в котором мы должны построить граф, определяющий какие меши будут созданы. \r\n \r\nИтак как-же задать правило для ноды rule и switcher. Для этого нажимаем плюсик в свойства чтобы создать новый блюпринт.  \r\n \r\nНо он оказывается пустым что-же делать дальше? Нужно нажать Override NodeBool. \r\n \r\nТеперь можно или открыть или закрыть ноду.  \r\n \r\nВсе аналогично и для switchera. У ноды Brunch есть такое же правило для задания координаты и поворота. Кроме того она имеет выход, это значит если к ней прикрепить другую Brunch то она в качестве привязки будет использовать координату предыдущей. \r\n \r\nОсталось только нажать кнопку Generate Meshes на панели свойств TestActor, и наслаждаться результатом.  \r\n \r\n \r\nНадеюсь вам понравилась эта статья. Она оказалась намного длинней чем раньше, боялся что не допишу до конца. \r\n \r\nP.S После того как я написал статью, я попробовал собрать игру и она не собралась. Чтобы игру можно было собрать надо в файле CustomNods.h внести следующие исправления: \r\n \r\nТо есть мы должны исключить все функции кроме GetChildNodes и CreateNodesMesh из класса ноды при помощи оператора #if WITH_EDITORONLY_DATA. В остальных нодах надо сделать тоже самое.  \r\n \r\nИ соответственно CustomNods.cpp: \r\n \r\n \r\n \r\nЕсли вы уже скачали файл проекта пожалуйста перекачайте его заново. \r\n \r\nP.P.S  \n\n      \n       \n    ", "hub": "Программирование / Интересные публикации / Хабрахабр"}
